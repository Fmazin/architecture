{
    "docs": [
        {
            "location": "/",
            "text": "Software architecture for Opening Reproducible Research\n\u00b6\n\n\n1. Introduction and Goals\n\u00b6\n\n\n1.1 Requirements Overview\n\u00b6\n\n\nThe system must provide a reliable way to create and examine (e.g. discover, inspect) reproducibility packages of computational research to support reproducible publications.\nThis architecture describes the relationship of the reproducibility service with other services from the context of scientific collaboration and publishing and how they can be combined to a new system for scholarly publications.\n\n\nAt its core is the concept of the \nExecutable Research Compendium\n (ERC, see \nspecification\n and \narticle\n), and a supporting reproducibility service, which is defined by a \nweb \nAPI\n specification\n and its \nreference implementation\n is published as open source software.\n\n\n1.2 Quality Goals\n\u00b6\n\n\n\n\nTransparency\n\n\nThe system must be transparent to allow scrutiny required by a rigorous scientific process.\n\n\nSeparation of concern\n\n\nThe system must integrate with existing services and focus on the core functionality.\nIt must not replicate existing functionality such as storage or persistent identification.\n\n\nFlexibility & modularity\n\n\nIn regard to the research project setting, the system components must be well separated, so functions can be developed independently, e.g. using different programming languages.\nThis allows different developers to contribute efficiently.\n\n\n\n\n1.3 Stakeholders\n\u00b6\n\n\n\n\n\n\n\n\nRole/Name\n\n\nGoal/point of contact\n\n\nRequired interaction\n\n\n\n\n\n\n\n\n\n\nAuthor (scientist)\n\n\npublish ERC as part of a scientific publication process\n\n\n-\n\n\n\n\n\n\nReviewer (scientist)\n\n\nexamine ERC during a review process\n\n\n-\n\n\n\n\n\n\nCo-author (scientist)\n\n\ncontribute to ERC during research (e.g. cloud based)\n\n\n-\n\n\n\n\n\n\nReader (scientist)\n\n\nview and interact with ERC on a journal website\n\n\n-\n\n\n\n\n\n\nPublisher\n\n\nincrease quality of publications in journals with ERC\n\n\n-\n\n\n\n\n\n\nCurator/preservationist\n\n\nensure research is complete and archivable using ERC\n\n\n-\n\n\n\n\n\n\nOperator\n\n\nprovide infrastructure to researchers at my university to collaborate and conduct high-quality research using ERC\n\n\n-\n\n\n\n\n\n\nDeveloper\n\n\nuse and extend the tools around ERC\n\n\n-\n\n\n\n\n\n\n\n\nSome of the stakeholders are accompanied by \nuser scenarios\n in prose.\n\n\n2. Architecture constraints\n\u00b6\n\n\nThe few constraints on this project are reflected in the final solution. This section shows them and if applicable, their motivation. (based on \nbiking2\n)\n\n\n2.1 Technical constraints\n\u00b6\n\n\n\n\n\n\n\n\n\u00a0\n\n\nConstraint\n\n\nBackground and/or motivation\n\n\n\n\n\n\n\n\n\n\nTECH.1\n\n\nOnly open licenses\n\n\nAll third party software or used data must be available under a suitable code license, i.e. either \nOSI-approved\n or \nODC license\n.\n\n\n\n\n\n\nTECH.2\n\n\nOS independent development and deployment\n\n\nServer applications must run in well defined \nDocker\n containers to allow installation on any host system and to not limit developers to a specific language or environment.\n\n\n\n\n\n\nTECH.3\n\n\nDo not store secure information\n\n\nThe team members experience and available resources do not allow for handling information with security concerns, so no critical data, such as user passwords but also data with privacy concerns, must be stored in the system.\n\n\n\n\n\n\n\n\n2.2 Organizational constraints\n\u00b6\n\n\n\n\n\n\n\n\n\u00a0\n\n\nConstraint\n\n\nBackground and/or motivation\n\n\n\n\n\n\n\n\n\n\nORG.1\n\n\nTeam and schedule\n\n\nhttp://o2r.info/about\n\n\n\n\n\n\nORG.2\n\n\nDo not interfere with existing well-established peer-review process\n\n\nThis software is \nnot\n going to change how scientific publishing works, nor should it. While intentioned to support public peer-reviews, open science etc., the software should be agnostic of these aspects.\n\n\n\n\n\n\nORG.3\n\n\nOnly open licenses\n\n\nAll created software must be available under an \nOSI-approved\n license, documentation and specification under a \nCC license\n.\n\n\n\n\n\n\nORG.4\n\n\nVersion control/management\n\n\nCode must be versioned using \ngit\n and published on \nGitHub\n.\n\n\n\n\n\n\nORG.5\n\n\nTransfer from group domain to persistent domain\n\n\nThe ERC bundles artifacts coming from a private or group domain for a transfer to a public and persistent domain (cf. \nCuration Domain Model\n), which imposes requirements on the availability and enrichment of metadata\n\n\n\n\n\n\n\n\n2.3 Conventions\n\u00b6\n\n\n\n\n\n\n\n\n\u00a0\n\n\nConstraint\n\n\nBackground and/or motivation\n\n\n\n\n\n\n\n\n\n\nCONV.1\n\n\nProvide architecture documentation\n\n\nBased on \narc42\n (template version 7.0).\n\n\n\n\n\n\nCONV.2\n\n\nReasonably follow coding conventions\n\n\nTypical project layout and coding conventions of the respective used language should be followed as far as possible. However, we explicitly accept the research project context and do \nnot\n provide full tests suites or documentation beyond what is needed by project team members.\n\n\n\n\n\n\nCONV.3\n\n\nDocumentation is English\n\n\nInternational research project, must be understandable by anyone interested.\n\n\n\n\n\n\nCONV.4\n\n\nUse subjectivization for server component names\n\n\nServer-side components are named using personalized verbs or professions: \nmuncher\n, \nloader\n, \ntransporter\n. All git repositories for software use an \no2r-\n prefix, in case of server-side components e.g. \no2r-shipper\n.\n\n\n\n\n\n\nCONV.5\n\n\nConfiguration using environment variables\n\n\nServer-side components must be configurable using all caps environment variables prefixed with the component name, e.g. \nSHIPPER_THE_SETTING\n, for required settings. Other settings should be put in a settings file suitable for the used language, e.g. \nconfig.js\n or \nconfig.yml\n.\n\n\n\n\n\n\n\n\n3. System scope and context\n\u00b6\n\n\n3.1 Business context\n\u00b6\n\n\n\n\n\n\n\n\n\n\nCommunication partner\n\n\nExchanged data\n\n\nTechnology/protocol\n\n\n\n\n\n\n\n\n\n\nReproducibility service\n, e.g. \no2r reference implementation\n\n\npublication platforms utilize creation and examination services for ERC; reproducibility service uses different \nsupporting services\n to retrieve software artifacts, store runtime environment images, execute workflows, and save complete ERC\n\n\nHTTP\n APIs\n\n\n\n\n\n\nPublishing platform\n, e.g. online journal website or review system\n\n\nusers access ERC status and metadata via search results and paper landing pages; review process integrates ERC details and supports manipulation;\n\n\nsystem's API using \nHTTP\n with \nJSON\n payload\n\n\n\n\n\n\nCollaboration platform\n\n\nprovide means to collaboratively work on data, code, or text; such platforms support both public and private (shared) digital workspaces\n\n\nHTTP\n\n\n\n\n\n\nID provider\n\n\nretrieve unique user IDs, user metadata, and authentication tokens; user must log in with the provider\n\n\nHTTP\n\n\n\n\n\n\nExecution infrastructure\n\n\nERC can be executed using a shared/distributed infrastructure\n\n\nHTTP\n\n\n\n\n\n\nData repository\n\n\nthe reproducibility service fetches (a) content for ERC creation, or (b) complete ERC, from different sources; it stores created ERC persistently at suitable repositories, which in turn may connect to long-term archives and preservation systems\n\n\nHTTP\n, \nFTP\n, \nWebDAV\n, \ngit\n\n\n\n\n\n\nRegistry (metadata)\n\n\nthe reproducibility service can deliver metadata on published ERC to registries/catalogues/search portals directly and mediately via data repositories; the service can also retrieve/harvest contextual metadata during ERC creation to reduce required user inputs; users discover ERC via registries\n\n\n(proprietary) \nHTTP\n APIs, persistent identifiers (\nDOI\n), \nOAI-PMH\n\n\n\n\n\n\nSoftware repository\n\n\nsoftware repository provide software artifacts during ERC creation and store executable runtime environments\n\n\nHTTP\n APIs\n\n\n\n\n\n\nArchives and digital preservation systems\n\n\nwhen information is transferred from the private (research group, single researcher) or group domain (collaborations) to a public and persistent domain (archives, repositories), then extended data and metadata management is needed but also different access and re-use is enabled; these concerns are only relevant in so far as \ndata repositories\n must be supported, but further aspects such as access rights in archives are only mediately relevant for the reproducibility service\n\n\nmetadata in \nJSON\n and \nXML\n provided via \nHTTP\n or as files\n\n\n\n\n\n\n\n\n3.2 Technical context\n\u00b6\n\n\nAll components use \nHTTP(S)\n over cable networks connections for communication (metadata documents, ERC, Linux containers, etc.).\n\n\n4. Solution strategy\n\u00b6\n\n\nThis section provides a short overview of architecture decisions and for some the reasoning behind them.\n\n\nWeb API\n\u00b6\n\n\nThe developed solution is set in an existing system of services, and first and foremost must integrate well with these systems, focussing on the specific missing features of building and running ERCs.\nThese features are provided via a \nwell-defined RESTful API\n.\n\n\nMicroservices\n\u00b6\n\n\nTo allow a dynamic development and support the large variety of skills, all server-side features are developed in independent \nmicroservices\n.\nThese microservices handle only specific functional parts of the API and allow independent development and deployment cycles.\nCore components are developed using server-side JavaScript based on Node.js while other components are implemented Python.\n\n\nWe accept this diversification \nincreases complexity\n of both development and testing environments and the deployment of said services.\n\n\nRequired documentation is minimal. The typical structure should follow common practices.\n\n\nStorage and intra-service communication\n\u00b6\n\n\nIn accordance with the system scope, there is no reliable storage solution implemented.\nThe microservices simply share a common pointer to a local file system path.\nStorage of ERC is only implemented to make the solution independent during development and for the needs of core functionality (temporal storage), but it is not a feature the solution will eventually provide.\n\n\nThe unifying component of the architecture is the \ndatabase\n.\nIt is known to all microservices.\n\n\nSome microservices communicate via an eventing mechanism for real-time updates, such as the search database and the component providing live updates to the user via WebSockets-\nThe eventing is based on the operation log of the database (which is normally used to synchronise database nodes).\nThis is a clear \nmisuse of an internal feature\n, but a lot simpler than maintaining a full-blown eventing solution.\n\n\nDemonstration, user data & authentication\n\u00b6\n\n\nTo be able to demonstrate the system, a \nbrowser-based client application\n is developed.\nIt uses the RESTful API to control the system.\n\nOAuth 2.0\n is used for authentication and minimal information, which is already public, is stored for each user.\nThis information is shared between all services which require authentication via the database.\n\n\nThe client application manages the control flow\n of all user interactions.\n\n\nTools\n\u00b6\n\n\nIf standalone tools are developed, they provide a command-line interface (CLI).\nThe CLI allows integration into microservices when needed and to package tools including their dependencies as containers and distributing them using a container registry.\nThese \n2nd level containers\n are started by the microservices and can run either next to the microservices or in an independent container cluster, providing scalability.\nIt must only be ensured they are correctly configured in each microservice.\nThe only required documentation is the installation into a container and usage of the CLI.\n\n\n5. Building block view\n\u00b6\n\n\n5.1 Refinement Level 1\n\u00b6\n\n\n5.1.1 Blackbox Publication Platforms\n\u00b6\n\n\nPublications platforms are the online interaction points of users with scientific works.\nUsers create publications, e.g. submitting to a scientific journal, publishing on a pre-print server, publishing on a self-hosted website, or collaborating in online repositories.\nUsers examine publications, e.g. browsing, searching, reading, downloading, or reviewing.\n\n\n5.1.2 Blackbox ID Provider\n\u00b6\n\n\nIdentification information of distributed systems is crucial, and for security reasons as well as for limiting manual reproduction of metadata, a central service can provide all of\n\n\n\n\nunique \nidentification of users\n and \nmetadata on users\n,\n\n\nauthentication\n of users, and\n\n\nmetadata on a user's \nworks\n, e.g. publications or ERC.\n\n\n\n\nPersistent identifiers for artifacts in the reproducibility service itself are \nnot required\n, as these are provided by data storage and registries.\nHowever, services such as \nePIC\n could allow to retrieve persistent IDs.\n\n\n5.1.3 Blackbox Execution Infrastructure\n\u00b6\n\n\nThe execution infrastructure provides CPU time and temporary result storage space for execution of ERC, both \"as is\" and with manipulation, i.e. changed parameters.\n\n\n5.1.4 Blackbox Data Repositories\n\u00b6\n\n\nData repositories are all services storing data but not software.\nMore specifically, they may store software \"as data\", but not with software-specific features such as code versioning or installation binaries for different computer architectures.\nData repositories may be self-hosted or public/free, domain-specific or generic.\nThey typically provide persistent identifiers or handles, e.g. a \nDOI\n or \nURN\n.\nThey are used both for loading created ERC and for storing the ERC created by the reproducibility service.\n\n\n5.1.5 Blackbox Registries\n\u00b6\n\n\nRegistries are metadata indexes or catalogues.\n\n\nThey are recipients of metadata exports by the reproducibility service to share information about ERC, e.g. add a new ERC to an author's profile.\nThis requires the reproducibility services to translate the internal metadata model into the recipients data model and encoding. \n\n\nThey are sources of metadata during ERC creation when the information in the fetched content is used to query registries for additional information which can be offered to the user.\n\n\n5.1.6 Blackbox Software Repositories\n\u00b6\n\n\nSoftware repositories are a source and a sink for software at different abstraction levels.\nThey are a source for software artifacts or packages, such as system packages in install a library or language-specific extension packages.\nThey are a sink for executable images of software, which comprise a number of software artifacts, for a specific ERC instance.\n\n\n5.2 Refinement Level 2\n\u00b6\n\n\n5.2.1 Whitebox Publication Platforms\n\u00b6\n\n\nPublication platforms can be roughly divided into two groups.\nThey can be either specific journals hosted independently, such as \nJStatSoft\n or \nJOSS\n, or a larger platform provided by a publisher to multiple journals, such as \nScienceDirect\n, \nMDPI\n, \nSpringerLink\n, or \nPLOS\n.\nTo some extend, pre-print servers, for example \nOSF\n or \narXiv.org\n, can also fall into the latter category.\n\n\nIntegration with the reproducibility service can happen via plug-ins to generic software, e.g. \nOJS\n, or by bespoke extensions.\nIntegrations are based on the service's public API.\n\n\n5.2.2 Whitebox ID Provider\n\u00b6\n\n\nThe reproducibility service uses \nORCID\n to authenticate users and retrieve user metadata.\nThe reproducibility service does not use the ORCID authorisation to edit ORCID user data or retrieve non-public data from ORCID, thus this process is (\npseudo-authentication using OAuth\n).\nInternally, the user's public \nORCID\n is the main identifier.\nUser have different levels, which allow different actions, such as \"registered user\" or \"administrator\".\nThese levels are stored in the reproducibility service.\n\n\n5.2.3 Whitebox Execution Infrastructure\n\u00b6\n\n\nSuch an infrastructure could be either self-hosted, e.g. \nDocker Swarm\n-based, or use a cloud service provide, such as \nAmazon EC2\n, \nDocker Cloud\n, or even use continuous integration services such as \nTravis CI\n or \nGitlab CI\n.\n\n\n5.2.4 Whitebox Data Repositories\n\u00b6\n\n\n\n\nThe reproducibility service \ndoes not persistently store anything\n.\nIt only keeps copies of files during creation and inspection.\nSo where are ERCs saved and where is their data coming from?\n\n\nCollaboration platforms\n, e.g. \nownCloud/Sciebo\n, \nGitHub\n, \nShareLatex\n, \nOSF\n, allow users to create, store, and share their research (code, text, data, et cetera).\nBesides being an interaction platform for users, they can also be seen simply as a data repository.\nThe reproducibility service fetches contents for building an ERC from them based on public links, e.g. a public GitHub repository or shared Sciebo folder.\nIt is possible to link ERC creation to an project/repository under development on a collaboration platform as to trigger an ERC (re-)creation or execution when changes are made.\n\n\nProtocols: \nWebDAV\n, \nownCloud\n, \nHTTP\n (including \nwebhooks\n), \ngit\n\n\nDomain data repositories\n, e.g. \nPANGAEA\n or \nGFZ Data Services\n, can be accessed by the reproducibility service during creation and execution of ERC to download data.\nAllowing access to data repositories reduces data duplication but requires control over/trust in the respective repository.\n\n\nProtocol: \nHTTP\n APIs\n\n\nGeneric \nRepositories\n, e.g. \nZenodo\n, \nMendeley Data\n, \nFigshare\n, \nOSF\n, provide (a) access to complete ERC stored in repositories for inspection and execution by the reproducibility service, and (b) storage of created ERC. repositories.\n\n\nProtocols: (authenticated) \nHTTP\n APIs\n\n\nArchives\n and digital preservation solutions can provide long-term preservation of ERC.\nPreservation either lies in the responsibility of the repository, which might save the hosted content to an archive or an archive harvests a repository, or is a task for one of the involved platform providers, e.g. using an installation of \nArchivematica\n.\n\n\nProtocol: \nHTTP\n carrying bitstreams and metadata\n\n\n\n\nData Curation Continuum\n\n\nThe Data Curation Continuum (cf. \ndiagram by Andre Treloar\n), describes how data moves from the private domain of a researcher to the public domain of data repositories over the course of conducting research. It describes the properties of data and important aspects of the transitions. In a publishing process based on the reproducibility service, the full migration process is run through. \n\n\n\n\n5.2.5 Whitebox Registries\n\u00b6\n\n\nResearch data registries and websites, for example (\nCRIS\n, \nDataCite\n, \nGoogle Scholar\n, \nScopus\n, \nAltmetric\n, to name just a few, collect metadata on publications and provide services with this data.\nServices comprise discovery but also derivation of citation data and creating networks of researchers and publications.\n\n\nThe listed examples include open platforms, commercial solutions, and institution-specific platforms.\nSome of the registries offer a public, well-defined API to retrieve structured metadata and to create new records.\n\n\nProtocol: \nHTTP\n APIs\n\n\n5.2.6 Whitebox Software Repositories\n\u00b6\n\n\n\n\n5.2.6.1 Blackbox Package repositories\n\u00b6\n\n\nPackage repositories are used during ERC creation to download and install software artifacts for specific operating systems, e.g. \nDebian APT\n or \nUbuntu Launchpad\n, for specific programming languages or environments, e.g. \nCRAN\n, or from source, e.g. \nGitHub\n.\n\n\n5.2.6.2 Blackbox Container registries\n\u00b6\n\n\nContainer registries such as \nDocker Hub\n, \nQuay\n, self-hosted \nDocker Registry 2.0\n or \nAmazon ERC\n, store executable images of runtime environments.\nThey can be used to distribute the runtime environments across the execution infrastructure and provide an intermediate ephemeral storage for the reproducibility service.\n\n\n5.2.7 Whitebox Reproducibility Service\n\u00b6\n\n\n\n\n5.2.7.1 Blackbox Webserver\n\u00b6\n\n\nA webserver handles all incoming calls to the API (\n/api/v1/\n) via \nHTTPS\n (\nHTTP\n is redirected) and distributes them to the respective microservice.\nA working \nnginx\n configuration is available \nin the test setup\n.\n\n\n5.2.7.2 Blackbox UI\n\u00b6\n\n\nThe UI is a web application based on \nAngular JS\n, see \no2r-platform\n.\nIt connects to all microservices via their API and is served using the same webserver as the API.\n\n\n5.2.7.3 Blackbox Microservices\n\u00b6\n\n\nThe reproducibility service uses a \nmicroservice architecture\n to separate functionality defined by the \nweb API specification\n into manageable units.\n\n\nThis allows scalability (selected microservices can be deployed as much as needed) and technology independence for each use case and developer.\nThe microservices all access one main database and a shared file storage.\n\n\n5.2.7.4 Blackbox Tools\n\u00b6\n\n\nSome functionality is developed as standalone tools and used as such in the microservices instead of re-implementing features.\nThese tools are integrated via their command line interface (CLI) and executed as \n2nd level containers\n by microservices.\n\n\n5.2.7.5 Blackbox Databases\n\u00b6\n\n\nThe \nmain document database\n is the unifying element of the microservice architecture.\nAll information shared between microservices or transactions between microservices are made via the database, including session state handling for authentication.\n\n\nA \nsearch database\n is used for full-text search and advanced queries.\n\n\nThe database's operation log, normally used for synchronization between database nodes, is also used for \n\n\n\n\nevent-driven communication between microservices, and\n\n\nsynchronization between main document database and search index.\n\n\n\n\n\n\nNote\n\n\nThis eventing \"hack\" is expected to be replaced by a proper eventing layer for productive deployments.\n\n\n\n\n5.2.7.6 Blackbox Ephemeral file storage\n\u00b6\n\n\nAfter loading from external sources and during creation of ERC, the files are stored in a file storage shared between the microservices.\nThe file structure is known to each microservice and read/write operations happen as needed.\n\n\n5.3 Refinement Level 3\n\u00b6\n\n\n5.3.1 Whitebox microservices\n\u00b6\n\n\nEach microservice is encapsulated as a \nDocker\n container running at its own port on an internal network and only serving its respective API path.\nInternal communication between the webserver and the microservices is unencrypted, i.e. \nHTTP\n.\n\n\nTesting\n: the \nreference implementation\n provides instructions on running a local instance ofr the microservices and the demonstration UI.\n\n\nDevelopment\n: the \no2r-platform\n GitHub project contains \ndocker-compose\n configurations to run all microservices, see repository file \ndocker-compose.yml\n and the project's \nREADME.md\n for instructions.\n\n\nThe following table describes the microservices, their endpoints, and their features.\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmuncher\n\n\n/api/v1/compendium\n and \n/api/v1/job\n\n\nJavaScript (Node.js)\n\n\ncore component for \nCRUD\n of compendia and jobs (ERC execution)\n\n\n\n\n\n\nloader\n\n\n/api/v1/compendium\n (\nHTTP POST\n only)\n\n\nJavaScript (Node.js)\n\n\nload workspaces from repositories and collaboration platforms\n\n\n\n\n\n\nfinder\n\n\n/api/v1/search\n\n\nJavaScript (Node.js)\n\n\ndiscovery and search, synchronizes the database with a search database (Elasticsearch) and exposes read-only search endpoints\n\n\n\n\n\n\ntransporter\n\n\n~ /data/\n and \n~* \\.(zip|tar|tar.gz)\n\n\nJavaScript (Node.js)\n\n\ndownloads of compendia in zip or (gzipped) tar formats\n\n\n\n\n\n\ninformer\n\n\n~* \\.io\n\n\nJavaScript (Node.js)\n\n\nsocket.io\n-based WebSockets for live updates to the UI based on database event log, e.g. job progress\n\n\n\n\n\n\ninspecter\n\n\n/api/v1/inspection\n\n\nR (\nplumber\n)\n\n\nallow inspection of non-text-based file formats, e.g. \n.Rdata\n\n\n\n\n\n\nsubstituter\n\n\n/api/v1/substitution\n\n\nJavaScript (Node.js)\n\n\ncreate new ERCs based on existing ones by substituting files\n\n\n\n\n\n\nmanipulater\n\n\nunder development\n\n\n--\n\n\nprovide back-end containers for interactive ERCs\n\n\n\n\n\n\n\n\nERC exporting\n\u00b6\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nshipper\n\n\n/api/v1/shipment\n\n\nPython\n\n\nship ERCs, including packaging, and their metadata to third party repositories and archives\n\n\n\n\n\n\n\n\nAuthentication\n\u00b6\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbouncer\n\n\n/api/v1/auth\n, \n/api/v1/user/\n\n\nJavaScript (Node.js)\n\n\nauthentication service and user management (whoami, level changing)\n\n\n\n\n\n\n\n\n5.3.2 Whitebox database\n\u00b6\n\n\nTwo databases are used.\n\n\nMongoDB\n document database\n with enabled \nreplica-set oplog\n for eventing.\n\n\nCollections:\n\n\n\n\nusers\n\n\nsessions\n\n\ncompendia\n\n\njobs\n\n\nshipments\n\n\n\n\nThe MongoDB API is used by connecting microservices via suitable client packages, which are available for all required languages.\n\n\nElasticsearch\n search index\n, kept in sync with the main document database by the microservice \nfinder\n.\nThe ids are mapped to support update and delete operations.\n\n\nThe two main resources of the API are kept in separate indices due to \ntheir different structure/mappings\n:\n\n\n\n\ncompendia\n with type \ncompendia\n\n\njobs\n with type \njobs\n\n\n\n\nThe search index is accessed by clients through the search endpoint provided by \nfinder\n.\n\n\n5.3.3 Whitebox tools\n\u00b6\n\n\n\n\n\n\n\n\nproject\n\n\nlanguage\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nmeta\n\n\nPython\n\n\nscripts for extraction, translation and validation of metadata; for details see \nmetadata documentation\n\n\n\n\n\n\ncontainerit\n\n\nR\n\n\ngeneration of Dockerfiles based on R sessions and scripts\n\n\n\n\n\n\n\n\nEach tool's code repository includes one or more \nDockerfiles\n, which are automatically build and published on Docker Hub.\nThe microservices use the tool's Docker images to execute the tools instead of installing all their dependencies into the microservices.\nThe advantages are a controlled environment for the tool usage, independent development cycles and updating of the tools, smaller independent images for the microservices, and scalability.\n\n\nMeta\n\u00b6\n\n\nMeta provides a CLI for each step of the metadata processing required in the reproducibility service as shown by the following diagram.\nAfter each step the created metadata is saved as a file per model to a directory in the compendium.\nA detailed view of the meta tool usage in the creation process is provided in the runtime view \nERC Creation\n.\n\n\n\n\nContainerit\n\u00b6\n\n\nThe containerit tool extracts required dependencies from ERC main documents and uses the information and external configuration to create a Dockerfile, which executes the full computational workflow when the container is started.\nIts main strategy is to analyse the session at the end of executing the full workflow.\n\n\n5.3.4 Whitebox ephemeral file storage\n\u00b6\n\n\nA host directory is mounted into every container to the location \n/tmp/o2r\n.\n\n\n6. Runtime view\n\u00b6\n\n\nThe runtime view describes the interaction between the static building blocks.\nIt cannot cover all potential cases and focusses on the following main scenarios.\n\n\n\n\n\n\n\n\nScenario\n\n\nPurpose and overview\n\n\n\n\n\n\n\n\n\n\nERC Creation\n\n\nThe most important workflow for an author is creating an ERC from his workspace of data, code and documentation. The author can provide these resources as a direct upload, but a more comfortable process is loading the files from a collaboration platform. Three microservices are core to this scenario: \nloader\n, \nmuncher\n, and \nshipper\n.\n\n\n\n\n\n\nERC Inspection\n\n\nThe most important workflow for a reviewer or reader is executing the analysis encapsulated in an ERC. The execution comprises creation of configuration files (if missing) from metadata, compiling the a display file using the actual analysis, and saving the used runtime environment. The core microservice for this scenario is \nmuncher\n.\n\n\n\n\n\n\n\n\n6.1 ERC Creation\n\u00b6\n\n\n\n\nFirst, the user initiates a \ncreation\n of a new ERC based on a workspace containing at least a viewable file (e.g. an HTML document or a plot) based on the code and instructions provided in a either a script or \nliterate programming document\n), and any other data.\nThe \nloader\n fetches the files, runs some checks, starts metadata extraction, starts metadata brokering from the raw metadata to o2r metadata, and saves the compendium, as a non-public candidate which only the uploading user can see, to the database.\nAll metadata processing is based on the tool \nmeta\n.\n\n\nThen the user opens the candidate compendium, checks and completes the metadata.\n\nmuncher\n triggers a metadata validation and brokering to several output formats (also using \nmeta\n), and loads the brokered metadata from the files to save a copy in the database for better \nsearchability\n.\n\n\nNext, the user must start a \njob\n to add the ERC configuration and runtime environment to the workspace, which are core elements of an ERC.\nThe ERC configuration is a file generated from the user-provided metadata (see \nERC specification\n).\nThe runtime environment consists of two parts: (a) the runtime manifest, which is created by executing the workflow once in a container based on the tool \ncontainerit\n; and (b) the runtime image, which is built from the runtime manifest.\nA user may provide the ERC configuration file and the runtime manifest with the workspace for fine-grained control; the generation steps are skipped then.\n\n\nFinally the user starts a shipment of the compendium to a data repository.\nThe \nshipper\n manages this two step process.\nThe separate \"create\" and \"publish\" steps allow checking the shipped files, and avoid unintentional shipments, because a published shipment creates a unique public resource, which potentially cannot be unpublished.\n\n\n\n\nIn the code\n\n\nThe \nloader\n has two core controllers for direct \nupload\n and \nload\n from a collaboration platform.\nTheir core chain of functions are realised as \nJavaScript Promises\n, see the code for \nloader\n and \nuploader\n respectively.\nThe respective steps are shared between these two cases where possible, i.e. starting with the step \nstripSingleBasedir\n.\n\n\n\n\n6.2 ERC Inspection\n\u00b6\n\n\n\n\nThe user initiates an \ninspection\n of an existing ERC by providing a reference such as \nDOI\n or URL.\n\nloader\n retrieves the compendium files, saves them locally and loads the contained metadata.\nThen the user can start a new \njob\n for the compendium.\n\nmuncher\n checks the request, creates a new job in the database and returns the job ID.\nThe user's client can use the ID to connect to the live logs provided by \ninformer\n.\nAll following steps by muncher regularly update the database, whose change events \ninformer\n uses to continuously update client via WebSockets.\n\n\nThe job starts with creating a copy of the compendium's files for the job.\nA \ncopy-on-write filesystem\n is advantageous for this step.\nThen the archived runtime image is loaded from the file in the compendium into a runtime repository.\nThis repository may be remote (either public or private, e.g. based on \nDocker Registry\n, \nECR\n or \nGitLab\n) or simply the local image storage.\nThen all files except the runtime image archive are packed so they can be send to a container runtime.\nThe container runtime can be local (e.g. the Docker daemon), or a container orchestration such as \nKubernetes\n.\nIt provides log updates as a stream to \nmuncher\n, which updates the database, whose changes trigger updates of the user interface via \ninformer\n.\nWhen the container is finished, \nmuncher\n compares the created outputs with the ones provided in the compendium and provides the result to the user.\n\n\n\n\nIn the code\n\n\nThe \nmuncher\n has two core resources: a \ncompendium\n represents an ERC, a \njob\n represents a \n\"run\"\n of an ERC, i.e. the building, running, and saving of the runtime environment including execution of the contained workflow.\nThe core function for this is the \nExecutor\n, which chains a number of steps using \nJavaScript Promises\n, see the \ncode\n.\nThe check uses the tool \nerc-checker\n.\n\n\n\n\n7. Deployment View\n\u00b6\n\n\n7.1 Test server \nhttps://o2r.uni-muenster.de\n\u00b6\n\n\n\n\n\n\nMotivation\n\n\n\n\nThe o2r infrastructure is driven by the research community's need for user friendly and transparent but also scalable and reliable solutions to increase computational reproducibility in the scientific publication process. To retrieve feedback from the community (public demo) and to increase software quality (controlled non-development environment), the current development state is regularly published on a test server.\n\n\n\n\nQuality and/or Performance Features\n\n\n\n\nThe server is managed completely with \nAnsible\n to ensure a well-document setup. The base operating system is CentOS Linux 7. The machine has 4 cores, 8 GB RAM, and a local storage ~100 GB, and runs on a VM host. The one machine in this deployment runs the full o2r reproducibility service, i.e. all microservices and a webserver to serve the user interfaces. It also runs the databases and ancillary services, such as a web traffic statistics service.\n\n\n\n\nMapping of Building Blocks to Infrastructure\n\n\n\n\nAll building blocks run in their own Docker container using an image provided via and build on \nDocker Hub\n using a \nDockerfile\n included in \neach microservice's code repository\n. The server is managed by the o2r team; external building blocks are managed by the respective organisation/provider.\n\n\n\n\n\n\n7.2 Production (sketch)\n\u00b6\n\n\n\n\n\n\nNote\n\n\nThis deployment view is a sketch for a potential productive deployment and intends to point out features of the chosen architecture and expected challenges or solutions.\n\n\n\n\n\n\nMotivation\n\n\n\n\nA productive system must be reliable and scalable providing a single reproducibility service API endpoint. It must also adopt the distribution and deployments to the hosted software and based on containers it naturally uses one of the powerful orchestration engines, such as \nDocker Swarm\n or \nKubernetes\n.\n\n\n\n\nQuality and/or Performance Features\n\n\n\n\nThe services are redundantly provided via separated clusters of nodes for (a) running the reproducibility service's microservices and ancillary services, (b) running the document and search databases, (c) running ERC executions. Separating the clusters allows common security protocols, e.g. the tool and execution cluster should not be able to contact arbitrary websites. The software in the data cluster can run in containers or bare metal. The clusters for app and compendia have access to a common shared file storage, a potential bottleneck. Performance of microservices can be easily scaled by adding nodes to the respective clusters.\n\n\n\n\nMapping of Building Blocks to Infrastructure\n\n\n\n\nThe o2r reproducibility services is managed by the o2r team similar to the test server. The other big building blocks, like publishing platforms or data repositories, are managed by the respective organisations.\n\n\n\n\n\n\n\n\nCredits\n\u00b6\n\n\nThis specification and guides are developed by the members of the project Opening Reproducible Research (\nOffene Reproduzierbare Forschung\n) funded by the German Research Foundation (Deutsche Forschungsgemeinschaft (DFG) - Projektnummer \n274927273\n) under grant numbers PE 1632/10-1, KR 3930/3-1, and TR 864/6-1).\n\n\n\n\n\n\nOpening Reproducible Research (o2r, \nhttp://o2r.info/about\n) is a DFG-funded research project by Institute for Geoinformatics (\nifgi\n) and University and Regional Library (\nULB\n), University of M\u00fcnster, Germany. Building on recent advances in mainstream IT, o2r envisions a new architecture for storing, executing and interacting with the original analysis environment alongside the corresponding research data and manuscript. This architecture evolves around so called Executable Research Compendia (ERC) as the container for both research, review, and archival.\n\n\n\n\n\n\nLicense\n\u00b6\n\n\n\n\nThe o2r architecture specification is licensed under \nCreative Commons CC0 1.0 Universal License\n, see file \nLICENSE\n.\nTo the extent possible under law, the people who associated CC0 with this work have waived all copyright and related or neighboring rights to this work.\nThis work is published from: Germany.\n\n\n\n\nAbout arc42\n\u00b6\n\n\narc42, the Template for documentation of software and system architecture.\n\n\nBy Dr. Gernot Starke, Dr. Peter Hruschka and contributors.\n\n\nTemplate Revision: 7.0 EN (based on asciidoc), January 2017\n\n\n\u00a9 We acknowledge that this document uses material from the arc 42 architecture template, \nhttp://www.arc42.de\n. Created by Dr. Peter Hruschka & Dr. Gernot Starke\n\n\nBuild @@VERSION@@ @ @@TIMESTAMP@@",
            "title": "Architecture"
        },
        {
            "location": "/#software-architecture-for-opening-reproducible-research",
            "text": "",
            "title": "Software architecture for Opening Reproducible Research"
        },
        {
            "location": "/#1-introduction-and-goals",
            "text": "",
            "title": "1. Introduction and Goals"
        },
        {
            "location": "/#11-requirements-overview",
            "text": "The system must provide a reliable way to create and examine (e.g. discover, inspect) reproducibility packages of computational research to support reproducible publications.\nThis architecture describes the relationship of the reproducibility service with other services from the context of scientific collaboration and publishing and how they can be combined to a new system for scholarly publications.  At its core is the concept of the  Executable Research Compendium  (ERC, see  specification  and  article ), and a supporting reproducibility service, which is defined by a  web  API  specification  and its  reference implementation  is published as open source software.",
            "title": "1.1 Requirements Overview"
        },
        {
            "location": "/#12-quality-goals",
            "text": "Transparency  The system must be transparent to allow scrutiny required by a rigorous scientific process.  Separation of concern  The system must integrate with existing services and focus on the core functionality.\nIt must not replicate existing functionality such as storage or persistent identification.  Flexibility & modularity  In regard to the research project setting, the system components must be well separated, so functions can be developed independently, e.g. using different programming languages.\nThis allows different developers to contribute efficiently.",
            "title": "1.2 Quality Goals"
        },
        {
            "location": "/#13-stakeholders",
            "text": "Role/Name  Goal/point of contact  Required interaction      Author (scientist)  publish ERC as part of a scientific publication process  -    Reviewer (scientist)  examine ERC during a review process  -    Co-author (scientist)  contribute to ERC during research (e.g. cloud based)  -    Reader (scientist)  view and interact with ERC on a journal website  -    Publisher  increase quality of publications in journals with ERC  -    Curator/preservationist  ensure research is complete and archivable using ERC  -    Operator  provide infrastructure to researchers at my university to collaborate and conduct high-quality research using ERC  -    Developer  use and extend the tools around ERC  -     Some of the stakeholders are accompanied by  user scenarios  in prose.",
            "title": "1.3 Stakeholders"
        },
        {
            "location": "/#2-architecture-constraints",
            "text": "The few constraints on this project are reflected in the final solution. This section shows them and if applicable, their motivation. (based on  biking2 )",
            "title": "2. Architecture constraints"
        },
        {
            "location": "/#21-technical-constraints",
            "text": "Constraint  Background and/or motivation      TECH.1  Only open licenses  All third party software or used data must be available under a suitable code license, i.e. either  OSI-approved  or  ODC license .    TECH.2  OS independent development and deployment  Server applications must run in well defined  Docker  containers to allow installation on any host system and to not limit developers to a specific language or environment.    TECH.3  Do not store secure information  The team members experience and available resources do not allow for handling information with security concerns, so no critical data, such as user passwords but also data with privacy concerns, must be stored in the system.",
            "title": "2.1 Technical constraints"
        },
        {
            "location": "/#22-organizational-constraints",
            "text": "Constraint  Background and/or motivation      ORG.1  Team and schedule  http://o2r.info/about    ORG.2  Do not interfere with existing well-established peer-review process  This software is  not  going to change how scientific publishing works, nor should it. While intentioned to support public peer-reviews, open science etc., the software should be agnostic of these aspects.    ORG.3  Only open licenses  All created software must be available under an  OSI-approved  license, documentation and specification under a  CC license .    ORG.4  Version control/management  Code must be versioned using  git  and published on  GitHub .    ORG.5  Transfer from group domain to persistent domain  The ERC bundles artifacts coming from a private or group domain for a transfer to a public and persistent domain (cf.  Curation Domain Model ), which imposes requirements on the availability and enrichment of metadata",
            "title": "2.2 Organizational constraints"
        },
        {
            "location": "/#23-conventions",
            "text": "Constraint  Background and/or motivation      CONV.1  Provide architecture documentation  Based on  arc42  (template version 7.0).    CONV.2  Reasonably follow coding conventions  Typical project layout and coding conventions of the respective used language should be followed as far as possible. However, we explicitly accept the research project context and do  not  provide full tests suites or documentation beyond what is needed by project team members.    CONV.3  Documentation is English  International research project, must be understandable by anyone interested.    CONV.4  Use subjectivization for server component names  Server-side components are named using personalized verbs or professions:  muncher ,  loader ,  transporter . All git repositories for software use an  o2r-  prefix, in case of server-side components e.g.  o2r-shipper .    CONV.5  Configuration using environment variables  Server-side components must be configurable using all caps environment variables prefixed with the component name, e.g.  SHIPPER_THE_SETTING , for required settings. Other settings should be put in a settings file suitable for the used language, e.g.  config.js  or  config.yml .",
            "title": "2.3 Conventions"
        },
        {
            "location": "/#3-system-scope-and-context",
            "text": "",
            "title": "3. System scope and context"
        },
        {
            "location": "/#31-business-context",
            "text": "Communication partner  Exchanged data  Technology/protocol      Reproducibility service , e.g.  o2r reference implementation  publication platforms utilize creation and examination services for ERC; reproducibility service uses different  supporting services  to retrieve software artifacts, store runtime environment images, execute workflows, and save complete ERC  HTTP  APIs    Publishing platform , e.g. online journal website or review system  users access ERC status and metadata via search results and paper landing pages; review process integrates ERC details and supports manipulation;  system's API using  HTTP  with  JSON  payload    Collaboration platform  provide means to collaboratively work on data, code, or text; such platforms support both public and private (shared) digital workspaces  HTTP    ID provider  retrieve unique user IDs, user metadata, and authentication tokens; user must log in with the provider  HTTP    Execution infrastructure  ERC can be executed using a shared/distributed infrastructure  HTTP    Data repository  the reproducibility service fetches (a) content for ERC creation, or (b) complete ERC, from different sources; it stores created ERC persistently at suitable repositories, which in turn may connect to long-term archives and preservation systems  HTTP ,  FTP ,  WebDAV ,  git    Registry (metadata)  the reproducibility service can deliver metadata on published ERC to registries/catalogues/search portals directly and mediately via data repositories; the service can also retrieve/harvest contextual metadata during ERC creation to reduce required user inputs; users discover ERC via registries  (proprietary)  HTTP  APIs, persistent identifiers ( DOI ),  OAI-PMH    Software repository  software repository provide software artifacts during ERC creation and store executable runtime environments  HTTP  APIs    Archives and digital preservation systems  when information is transferred from the private (research group, single researcher) or group domain (collaborations) to a public and persistent domain (archives, repositories), then extended data and metadata management is needed but also different access and re-use is enabled; these concerns are only relevant in so far as  data repositories  must be supported, but further aspects such as access rights in archives are only mediately relevant for the reproducibility service  metadata in  JSON  and  XML  provided via  HTTP  or as files",
            "title": "3.1 Business context"
        },
        {
            "location": "/#32-technical-context",
            "text": "All components use  HTTP(S)  over cable networks connections for communication (metadata documents, ERC, Linux containers, etc.).",
            "title": "3.2 Technical context"
        },
        {
            "location": "/#4-solution-strategy",
            "text": "This section provides a short overview of architecture decisions and for some the reasoning behind them.",
            "title": "4. Solution strategy"
        },
        {
            "location": "/#web-api",
            "text": "The developed solution is set in an existing system of services, and first and foremost must integrate well with these systems, focussing on the specific missing features of building and running ERCs.\nThese features are provided via a  well-defined RESTful API .",
            "title": "Web API"
        },
        {
            "location": "/#microservices",
            "text": "To allow a dynamic development and support the large variety of skills, all server-side features are developed in independent  microservices .\nThese microservices handle only specific functional parts of the API and allow independent development and deployment cycles.\nCore components are developed using server-side JavaScript based on Node.js while other components are implemented Python.  We accept this diversification  increases complexity  of both development and testing environments and the deployment of said services.  Required documentation is minimal. The typical structure should follow common practices.",
            "title": "Microservices"
        },
        {
            "location": "/#storage-and-intra-service-communication",
            "text": "In accordance with the system scope, there is no reliable storage solution implemented.\nThe microservices simply share a common pointer to a local file system path.\nStorage of ERC is only implemented to make the solution independent during development and for the needs of core functionality (temporal storage), but it is not a feature the solution will eventually provide.  The unifying component of the architecture is the  database .\nIt is known to all microservices.  Some microservices communicate via an eventing mechanism for real-time updates, such as the search database and the component providing live updates to the user via WebSockets-\nThe eventing is based on the operation log of the database (which is normally used to synchronise database nodes).\nThis is a clear  misuse of an internal feature , but a lot simpler than maintaining a full-blown eventing solution.",
            "title": "Storage and intra-service communication"
        },
        {
            "location": "/#demonstration-user-data-authentication",
            "text": "To be able to demonstrate the system, a  browser-based client application  is developed.\nIt uses the RESTful API to control the system. OAuth 2.0  is used for authentication and minimal information, which is already public, is stored for each user.\nThis information is shared between all services which require authentication via the database.  The client application manages the control flow  of all user interactions.",
            "title": "Demonstration, user data &amp; authentication"
        },
        {
            "location": "/#tools",
            "text": "If standalone tools are developed, they provide a command-line interface (CLI).\nThe CLI allows integration into microservices when needed and to package tools including their dependencies as containers and distributing them using a container registry.\nThese  2nd level containers  are started by the microservices and can run either next to the microservices or in an independent container cluster, providing scalability.\nIt must only be ensured they are correctly configured in each microservice.\nThe only required documentation is the installation into a container and usage of the CLI.",
            "title": "Tools"
        },
        {
            "location": "/#5-building-block-view",
            "text": "",
            "title": "5. Building block view"
        },
        {
            "location": "/#51-refinement-level-1",
            "text": "",
            "title": "5.1 Refinement Level 1"
        },
        {
            "location": "/#511-blackbox-publication-platforms",
            "text": "Publications platforms are the online interaction points of users with scientific works.\nUsers create publications, e.g. submitting to a scientific journal, publishing on a pre-print server, publishing on a self-hosted website, or collaborating in online repositories.\nUsers examine publications, e.g. browsing, searching, reading, downloading, or reviewing.",
            "title": "5.1.1 Blackbox Publication Platforms"
        },
        {
            "location": "/#512-blackbox-id-provider",
            "text": "Identification information of distributed systems is crucial, and for security reasons as well as for limiting manual reproduction of metadata, a central service can provide all of   unique  identification of users  and  metadata on users ,  authentication  of users, and  metadata on a user's  works , e.g. publications or ERC.   Persistent identifiers for artifacts in the reproducibility service itself are  not required , as these are provided by data storage and registries.\nHowever, services such as  ePIC  could allow to retrieve persistent IDs.",
            "title": "5.1.2 Blackbox ID Provider"
        },
        {
            "location": "/#513-blackbox-execution-infrastructure",
            "text": "The execution infrastructure provides CPU time and temporary result storage space for execution of ERC, both \"as is\" and with manipulation, i.e. changed parameters.",
            "title": "5.1.3 Blackbox Execution Infrastructure"
        },
        {
            "location": "/#514-blackbox-data-repositories",
            "text": "Data repositories are all services storing data but not software.\nMore specifically, they may store software \"as data\", but not with software-specific features such as code versioning or installation binaries for different computer architectures.\nData repositories may be self-hosted or public/free, domain-specific or generic.\nThey typically provide persistent identifiers or handles, e.g. a  DOI  or  URN .\nThey are used both for loading created ERC and for storing the ERC created by the reproducibility service.",
            "title": "5.1.4 Blackbox Data Repositories"
        },
        {
            "location": "/#515-blackbox-registries",
            "text": "Registries are metadata indexes or catalogues.  They are recipients of metadata exports by the reproducibility service to share information about ERC, e.g. add a new ERC to an author's profile.\nThis requires the reproducibility services to translate the internal metadata model into the recipients data model and encoding.   They are sources of metadata during ERC creation when the information in the fetched content is used to query registries for additional information which can be offered to the user.",
            "title": "5.1.5 Blackbox Registries"
        },
        {
            "location": "/#516-blackbox-software-repositories",
            "text": "Software repositories are a source and a sink for software at different abstraction levels.\nThey are a source for software artifacts or packages, such as system packages in install a library or language-specific extension packages.\nThey are a sink for executable images of software, which comprise a number of software artifacts, for a specific ERC instance.",
            "title": "5.1.6 Blackbox Software Repositories"
        },
        {
            "location": "/#52-refinement-level-2",
            "text": "",
            "title": "5.2 Refinement Level 2"
        },
        {
            "location": "/#521-whitebox-publication-platforms",
            "text": "Publication platforms can be roughly divided into two groups.\nThey can be either specific journals hosted independently, such as  JStatSoft  or  JOSS , or a larger platform provided by a publisher to multiple journals, such as  ScienceDirect ,  MDPI ,  SpringerLink , or  PLOS .\nTo some extend, pre-print servers, for example  OSF  or  arXiv.org , can also fall into the latter category.  Integration with the reproducibility service can happen via plug-ins to generic software, e.g.  OJS , or by bespoke extensions.\nIntegrations are based on the service's public API.",
            "title": "5.2.1 Whitebox Publication Platforms"
        },
        {
            "location": "/#522-whitebox-id-provider",
            "text": "The reproducibility service uses  ORCID  to authenticate users and retrieve user metadata.\nThe reproducibility service does not use the ORCID authorisation to edit ORCID user data or retrieve non-public data from ORCID, thus this process is ( pseudo-authentication using OAuth ).\nInternally, the user's public  ORCID  is the main identifier.\nUser have different levels, which allow different actions, such as \"registered user\" or \"administrator\".\nThese levels are stored in the reproducibility service.",
            "title": "5.2.2 Whitebox ID Provider"
        },
        {
            "location": "/#523-whitebox-execution-infrastructure",
            "text": "Such an infrastructure could be either self-hosted, e.g.  Docker Swarm -based, or use a cloud service provide, such as  Amazon EC2 ,  Docker Cloud , or even use continuous integration services such as  Travis CI  or  Gitlab CI .",
            "title": "5.2.3 Whitebox Execution Infrastructure"
        },
        {
            "location": "/#524-whitebox-data-repositories",
            "text": "The reproducibility service  does not persistently store anything .\nIt only keeps copies of files during creation and inspection.\nSo where are ERCs saved and where is their data coming from?  Collaboration platforms , e.g.  ownCloud/Sciebo ,  GitHub ,  ShareLatex ,  OSF , allow users to create, store, and share their research (code, text, data, et cetera).\nBesides being an interaction platform for users, they can also be seen simply as a data repository.\nThe reproducibility service fetches contents for building an ERC from them based on public links, e.g. a public GitHub repository or shared Sciebo folder.\nIt is possible to link ERC creation to an project/repository under development on a collaboration platform as to trigger an ERC (re-)creation or execution when changes are made.  Protocols:  WebDAV ,  ownCloud ,  HTTP  (including  webhooks ),  git  Domain data repositories , e.g.  PANGAEA  or  GFZ Data Services , can be accessed by the reproducibility service during creation and execution of ERC to download data.\nAllowing access to data repositories reduces data duplication but requires control over/trust in the respective repository.  Protocol:  HTTP  APIs  Generic  Repositories , e.g.  Zenodo ,  Mendeley Data ,  Figshare ,  OSF , provide (a) access to complete ERC stored in repositories for inspection and execution by the reproducibility service, and (b) storage of created ERC. repositories.  Protocols: (authenticated)  HTTP  APIs  Archives  and digital preservation solutions can provide long-term preservation of ERC.\nPreservation either lies in the responsibility of the repository, which might save the hosted content to an archive or an archive harvests a repository, or is a task for one of the involved platform providers, e.g. using an installation of  Archivematica .  Protocol:  HTTP  carrying bitstreams and metadata   Data Curation Continuum  The Data Curation Continuum (cf.  diagram by Andre Treloar ), describes how data moves from the private domain of a researcher to the public domain of data repositories over the course of conducting research. It describes the properties of data and important aspects of the transitions. In a publishing process based on the reproducibility service, the full migration process is run through.",
            "title": "5.2.4 Whitebox Data Repositories"
        },
        {
            "location": "/#525-whitebox-registries",
            "text": "Research data registries and websites, for example ( CRIS ,  DataCite ,  Google Scholar ,  Scopus ,  Altmetric , to name just a few, collect metadata on publications and provide services with this data.\nServices comprise discovery but also derivation of citation data and creating networks of researchers and publications.  The listed examples include open platforms, commercial solutions, and institution-specific platforms.\nSome of the registries offer a public, well-defined API to retrieve structured metadata and to create new records.  Protocol:  HTTP  APIs",
            "title": "5.2.5 Whitebox Registries"
        },
        {
            "location": "/#526-whitebox-software-repositories",
            "text": "",
            "title": "5.2.6 Whitebox Software Repositories"
        },
        {
            "location": "/#5261-blackbox-package-repositories",
            "text": "Package repositories are used during ERC creation to download and install software artifacts for specific operating systems, e.g.  Debian APT  or  Ubuntu Launchpad , for specific programming languages or environments, e.g.  CRAN , or from source, e.g.  GitHub .",
            "title": "5.2.6.1 Blackbox Package repositories"
        },
        {
            "location": "/#5262-blackbox-container-registries",
            "text": "Container registries such as  Docker Hub ,  Quay , self-hosted  Docker Registry 2.0  or  Amazon ERC , store executable images of runtime environments.\nThey can be used to distribute the runtime environments across the execution infrastructure and provide an intermediate ephemeral storage for the reproducibility service.",
            "title": "5.2.6.2 Blackbox Container registries"
        },
        {
            "location": "/#527-whitebox-reproducibility-service",
            "text": "",
            "title": "5.2.7 Whitebox Reproducibility Service"
        },
        {
            "location": "/#5271-blackbox-webserver",
            "text": "A webserver handles all incoming calls to the API ( /api/v1/ ) via  HTTPS  ( HTTP  is redirected) and distributes them to the respective microservice.\nA working  nginx  configuration is available  in the test setup .",
            "title": "5.2.7.1 Blackbox Webserver"
        },
        {
            "location": "/#5272-blackbox-ui",
            "text": "The UI is a web application based on  Angular JS , see  o2r-platform .\nIt connects to all microservices via their API and is served using the same webserver as the API.",
            "title": "5.2.7.2 Blackbox UI"
        },
        {
            "location": "/#5273-blackbox-microservices",
            "text": "The reproducibility service uses a  microservice architecture  to separate functionality defined by the  web API specification  into manageable units.  This allows scalability (selected microservices can be deployed as much as needed) and technology independence for each use case and developer.\nThe microservices all access one main database and a shared file storage.",
            "title": "5.2.7.3 Blackbox Microservices"
        },
        {
            "location": "/#5274-blackbox-tools",
            "text": "Some functionality is developed as standalone tools and used as such in the microservices instead of re-implementing features.\nThese tools are integrated via their command line interface (CLI) and executed as  2nd level containers  by microservices.",
            "title": "5.2.7.4 Blackbox Tools"
        },
        {
            "location": "/#5275-blackbox-databases",
            "text": "The  main document database  is the unifying element of the microservice architecture.\nAll information shared between microservices or transactions between microservices are made via the database, including session state handling for authentication.  A  search database  is used for full-text search and advanced queries.  The database's operation log, normally used for synchronization between database nodes, is also used for    event-driven communication between microservices, and  synchronization between main document database and search index.    Note  This eventing \"hack\" is expected to be replaced by a proper eventing layer for productive deployments.",
            "title": "5.2.7.5 Blackbox Databases"
        },
        {
            "location": "/#5276-blackbox-ephemeral-file-storage",
            "text": "After loading from external sources and during creation of ERC, the files are stored in a file storage shared between the microservices.\nThe file structure is known to each microservice and read/write operations happen as needed.",
            "title": "5.2.7.6 Blackbox Ephemeral file storage"
        },
        {
            "location": "/#53-refinement-level-3",
            "text": "",
            "title": "5.3 Refinement Level 3"
        },
        {
            "location": "/#531-whitebox-microservices",
            "text": "Each microservice is encapsulated as a  Docker  container running at its own port on an internal network and only serving its respective API path.\nInternal communication between the webserver and the microservices is unencrypted, i.e.  HTTP .  Testing : the  reference implementation  provides instructions on running a local instance ofr the microservices and the demonstration UI.  Development : the  o2r-platform  GitHub project contains  docker-compose  configurations to run all microservices, see repository file  docker-compose.yml  and the project's  README.md  for instructions.  The following table describes the microservices, their endpoints, and their features.     Project  API path  Language  Description      muncher  /api/v1/compendium  and  /api/v1/job  JavaScript (Node.js)  core component for  CRUD  of compendia and jobs (ERC execution)    loader  /api/v1/compendium  ( HTTP POST  only)  JavaScript (Node.js)  load workspaces from repositories and collaboration platforms    finder  /api/v1/search  JavaScript (Node.js)  discovery and search, synchronizes the database with a search database (Elasticsearch) and exposes read-only search endpoints    transporter  ~ /data/  and  ~* \\.(zip|tar|tar.gz)  JavaScript (Node.js)  downloads of compendia in zip or (gzipped) tar formats    informer  ~* \\.io  JavaScript (Node.js)  socket.io -based WebSockets for live updates to the UI based on database event log, e.g. job progress    inspecter  /api/v1/inspection  R ( plumber )  allow inspection of non-text-based file formats, e.g.  .Rdata    substituter  /api/v1/substitution  JavaScript (Node.js)  create new ERCs based on existing ones by substituting files    manipulater  under development  --  provide back-end containers for interactive ERCs",
            "title": "5.3.1 Whitebox microservices"
        },
        {
            "location": "/#erc-exporting",
            "text": "Project  API path  Language  Description      shipper  /api/v1/shipment  Python  ship ERCs, including packaging, and their metadata to third party repositories and archives",
            "title": "ERC exporting"
        },
        {
            "location": "/#authentication",
            "text": "Project  API path  Language  Description      bouncer  /api/v1/auth ,  /api/v1/user/  JavaScript (Node.js)  authentication service and user management (whoami, level changing)",
            "title": "Authentication"
        },
        {
            "location": "/#532-whitebox-database",
            "text": "Two databases are used.  MongoDB  document database  with enabled  replica-set oplog  for eventing.  Collections:   users  sessions  compendia  jobs  shipments   The MongoDB API is used by connecting microservices via suitable client packages, which are available for all required languages.  Elasticsearch  search index , kept in sync with the main document database by the microservice  finder .\nThe ids are mapped to support update and delete operations.  The two main resources of the API are kept in separate indices due to  their different structure/mappings :   compendia  with type  compendia  jobs  with type  jobs   The search index is accessed by clients through the search endpoint provided by  finder .",
            "title": "5.3.2 Whitebox database"
        },
        {
            "location": "/#533-whitebox-tools",
            "text": "project  language  description      meta  Python  scripts for extraction, translation and validation of metadata; for details see  metadata documentation    containerit  R  generation of Dockerfiles based on R sessions and scripts     Each tool's code repository includes one or more  Dockerfiles , which are automatically build and published on Docker Hub.\nThe microservices use the tool's Docker images to execute the tools instead of installing all their dependencies into the microservices.\nThe advantages are a controlled environment for the tool usage, independent development cycles and updating of the tools, smaller independent images for the microservices, and scalability.",
            "title": "5.3.3 Whitebox tools"
        },
        {
            "location": "/#meta",
            "text": "Meta provides a CLI for each step of the metadata processing required in the reproducibility service as shown by the following diagram.\nAfter each step the created metadata is saved as a file per model to a directory in the compendium.\nA detailed view of the meta tool usage in the creation process is provided in the runtime view  ERC Creation .",
            "title": "Meta"
        },
        {
            "location": "/#containerit",
            "text": "The containerit tool extracts required dependencies from ERC main documents and uses the information and external configuration to create a Dockerfile, which executes the full computational workflow when the container is started.\nIts main strategy is to analyse the session at the end of executing the full workflow.",
            "title": "Containerit"
        },
        {
            "location": "/#534-whitebox-ephemeral-file-storage",
            "text": "A host directory is mounted into every container to the location  /tmp/o2r .",
            "title": "5.3.4 Whitebox ephemeral file storage"
        },
        {
            "location": "/#6-runtime-view",
            "text": "The runtime view describes the interaction between the static building blocks.\nIt cannot cover all potential cases and focusses on the following main scenarios.     Scenario  Purpose and overview      ERC Creation  The most important workflow for an author is creating an ERC from his workspace of data, code and documentation. The author can provide these resources as a direct upload, but a more comfortable process is loading the files from a collaboration platform. Three microservices are core to this scenario:  loader ,  muncher , and  shipper .    ERC Inspection  The most important workflow for a reviewer or reader is executing the analysis encapsulated in an ERC. The execution comprises creation of configuration files (if missing) from metadata, compiling the a display file using the actual analysis, and saving the used runtime environment. The core microservice for this scenario is  muncher .",
            "title": "6. Runtime view"
        },
        {
            "location": "/#61-erc-creation",
            "text": "First, the user initiates a  creation  of a new ERC based on a workspace containing at least a viewable file (e.g. an HTML document or a plot) based on the code and instructions provided in a either a script or  literate programming document ), and any other data.\nThe  loader  fetches the files, runs some checks, starts metadata extraction, starts metadata brokering from the raw metadata to o2r metadata, and saves the compendium, as a non-public candidate which only the uploading user can see, to the database.\nAll metadata processing is based on the tool  meta .  Then the user opens the candidate compendium, checks and completes the metadata. muncher  triggers a metadata validation and brokering to several output formats (also using  meta ), and loads the brokered metadata from the files to save a copy in the database for better  searchability .  Next, the user must start a  job  to add the ERC configuration and runtime environment to the workspace, which are core elements of an ERC.\nThe ERC configuration is a file generated from the user-provided metadata (see  ERC specification ).\nThe runtime environment consists of two parts: (a) the runtime manifest, which is created by executing the workflow once in a container based on the tool  containerit ; and (b) the runtime image, which is built from the runtime manifest.\nA user may provide the ERC configuration file and the runtime manifest with the workspace for fine-grained control; the generation steps are skipped then.  Finally the user starts a shipment of the compendium to a data repository.\nThe  shipper  manages this two step process.\nThe separate \"create\" and \"publish\" steps allow checking the shipped files, and avoid unintentional shipments, because a published shipment creates a unique public resource, which potentially cannot be unpublished.   In the code  The  loader  has two core controllers for direct  upload  and  load  from a collaboration platform.\nTheir core chain of functions are realised as  JavaScript Promises , see the code for  loader  and  uploader  respectively.\nThe respective steps are shared between these two cases where possible, i.e. starting with the step  stripSingleBasedir .",
            "title": "6.1 ERC Creation"
        },
        {
            "location": "/#62-erc-inspection",
            "text": "The user initiates an  inspection  of an existing ERC by providing a reference such as  DOI  or URL. loader  retrieves the compendium files, saves them locally and loads the contained metadata.\nThen the user can start a new  job  for the compendium. muncher  checks the request, creates a new job in the database and returns the job ID.\nThe user's client can use the ID to connect to the live logs provided by  informer .\nAll following steps by muncher regularly update the database, whose change events  informer  uses to continuously update client via WebSockets.  The job starts with creating a copy of the compendium's files for the job.\nA  copy-on-write filesystem  is advantageous for this step.\nThen the archived runtime image is loaded from the file in the compendium into a runtime repository.\nThis repository may be remote (either public or private, e.g. based on  Docker Registry ,  ECR  or  GitLab ) or simply the local image storage.\nThen all files except the runtime image archive are packed so they can be send to a container runtime.\nThe container runtime can be local (e.g. the Docker daemon), or a container orchestration such as  Kubernetes .\nIt provides log updates as a stream to  muncher , which updates the database, whose changes trigger updates of the user interface via  informer .\nWhen the container is finished,  muncher  compares the created outputs with the ones provided in the compendium and provides the result to the user.   In the code  The  muncher  has two core resources: a  compendium  represents an ERC, a  job  represents a  \"run\"  of an ERC, i.e. the building, running, and saving of the runtime environment including execution of the contained workflow.\nThe core function for this is the  Executor , which chains a number of steps using  JavaScript Promises , see the  code .\nThe check uses the tool  erc-checker .",
            "title": "6.2 ERC Inspection"
        },
        {
            "location": "/#7-deployment-view",
            "text": "",
            "title": "7. Deployment View"
        },
        {
            "location": "/#71-test-server-httpso2runi-muensterde",
            "text": "Motivation   The o2r infrastructure is driven by the research community's need for user friendly and transparent but also scalable and reliable solutions to increase computational reproducibility in the scientific publication process. To retrieve feedback from the community (public demo) and to increase software quality (controlled non-development environment), the current development state is regularly published on a test server.   Quality and/or Performance Features   The server is managed completely with  Ansible  to ensure a well-document setup. The base operating system is CentOS Linux 7. The machine has 4 cores, 8 GB RAM, and a local storage ~100 GB, and runs on a VM host. The one machine in this deployment runs the full o2r reproducibility service, i.e. all microservices and a webserver to serve the user interfaces. It also runs the databases and ancillary services, such as a web traffic statistics service.   Mapping of Building Blocks to Infrastructure   All building blocks run in their own Docker container using an image provided via and build on  Docker Hub  using a  Dockerfile  included in  each microservice's code repository . The server is managed by the o2r team; external building blocks are managed by the respective organisation/provider.",
            "title": "7.1 Test server https://o2r.uni-muenster.de"
        },
        {
            "location": "/#72-production-sketch",
            "text": "Note  This deployment view is a sketch for a potential productive deployment and intends to point out features of the chosen architecture and expected challenges or solutions.    Motivation   A productive system must be reliable and scalable providing a single reproducibility service API endpoint. It must also adopt the distribution and deployments to the hosted software and based on containers it naturally uses one of the powerful orchestration engines, such as  Docker Swarm  or  Kubernetes .   Quality and/or Performance Features   The services are redundantly provided via separated clusters of nodes for (a) running the reproducibility service's microservices and ancillary services, (b) running the document and search databases, (c) running ERC executions. Separating the clusters allows common security protocols, e.g. the tool and execution cluster should not be able to contact arbitrary websites. The software in the data cluster can run in containers or bare metal. The clusters for app and compendia have access to a common shared file storage, a potential bottleneck. Performance of microservices can be easily scaled by adding nodes to the respective clusters.   Mapping of Building Blocks to Infrastructure   The o2r reproducibility services is managed by the o2r team similar to the test server. The other big building blocks, like publishing platforms or data repositories, are managed by the respective organisations.",
            "title": "7.2 Production (sketch)"
        },
        {
            "location": "/#credits",
            "text": "This specification and guides are developed by the members of the project Opening Reproducible Research ( Offene Reproduzierbare Forschung ) funded by the German Research Foundation (Deutsche Forschungsgemeinschaft (DFG) - Projektnummer  274927273 ) under grant numbers PE 1632/10-1, KR 3930/3-1, and TR 864/6-1).    Opening Reproducible Research (o2r,  http://o2r.info/about ) is a DFG-funded research project by Institute for Geoinformatics ( ifgi ) and University and Regional Library ( ULB ), University of M\u00fcnster, Germany. Building on recent advances in mainstream IT, o2r envisions a new architecture for storing, executing and interacting with the original analysis environment alongside the corresponding research data and manuscript. This architecture evolves around so called Executable Research Compendia (ERC) as the container for both research, review, and archival.",
            "title": "Credits"
        },
        {
            "location": "/#license",
            "text": "The o2r architecture specification is licensed under  Creative Commons CC0 1.0 Universal License , see file  LICENSE .\nTo the extent possible under law, the people who associated CC0 with this work have waived all copyright and related or neighboring rights to this work.\nThis work is published from: Germany.",
            "title": "License"
        },
        {
            "location": "/#about-arc42",
            "text": "arc42, the Template for documentation of software and system architecture.  By Dr. Gernot Starke, Dr. Peter Hruschka and contributors.  Template Revision: 7.0 EN (based on asciidoc), January 2017  \u00a9 We acknowledge that this document uses material from the arc 42 architecture template,  http://www.arc42.de . Created by Dr. Peter Hruschka & Dr. Gernot Starke  Build @@VERSION@@ @ @@TIMESTAMP@@ < /div",
            "title": "About arc42"
        },
        {
            "location": "/user-scenarios/",
            "text": "User scenarios\n\u00b6\n\n\nAndrea the author and reader\n\u00b6\n\n\nAndrea turned 29 this year.\nShe is always up for a joke and a pot of coffee but is also quite impatient.\nEspecially if she has to wait for others or if she hasn\u2019t had any progress for a while.\nHowever, currently Andrea does his Ph.D in the field of geosciences.\nTwo years ago she decided to go for a cumulative dissertation, meaning she publishes scientific papers throughout his graduation and summarizes them at the end.\nShe already published his first paper a few months ago which is good, actually.\nOne of the reviewers was interested in the data and the source-code in order to reproduce the results.\n\n\nAfter a few hours of searching (remember she is not one of the most patient), she finally finds some files which include the dataset and also the source-code in R (a statistics program).\nJust a short try if it still working...\nweird, the results are different.\nJust a short look into the paper...\nThe configuration is different than the one described in the method section.\nWell, just few manipulations and - still not working.\n\n\nAlthough she submitted the paper just a few months ago, she can\u2019t remember the exact configuration for the results in the paper.\nFortunately, submitting data and code was not mandatory.\nBut Andrea knows she made a mistake.\n\n\nMore and more journals expect their authors to submit data and source-code which underlie the research findings.\nFor this reason, she wants to change her working behavior and to keep data and code files better under control.\nShe remembers her last research work which was quite unstructured, maybe already messy.\nCode and data was distributed over several folders and even computers.\nShe had so search for them for quite a while.\nMoreover, some components do not work anymore.\n\n\nThis time, she wants to do it better and searches for a great tool assisting her workflow.\nShe just heard about a new website supporting reproducible research.\nIt allows to upload all necessary files and to create a so called \"container\" which is \"executable\" - whatever that means.\nIt even verifies the results in the paper making it possible to detect errors immediately.\nOf course it also contains common features like sharing the publication with other authors.\nOn top of that, Andrea can also benefit from other publications.\nAs the website automatically generates a number of meta information, new search capabilities arise.\nIt is not only possible to search for other publications by using keywords, but also by using spatial and temporal properties and constraints.\nIt is even possible to constrain the search to hypotheses and research questions having certain vocabulary, thus simplifying search for related work.\nAndrea is quite impressed! She easily finds related papers around her own work.\nShe gets a good overview about existing research questions making it easier to identify a research gap he can focus on.\nAndrea doesn\u2019t even have to implement all the code lines for his statistical analysis from scratch, but can build upon existing.\nWhile reading some of the related papers in the browser, he realizes a couple of user interface widgets besides the incorporated figures.\nHe doesn\u2019t know them from traditional, static papers which are typically published as .pdf-files.\nAndrea recognizes that the widgets allow to interact with the diagrams to which the widgets belong.\nThey allow to change, for example, thresholds, input variables and constants.\nShe is thus able to check the assumptions and conclusions underlying the paper.\nShe is a bit overwhelmed by the number of new features, such as exchanging the dataset or the source-code underlying the paper.\nAndrea is quite happy about his new tool.\nIt provides support for structured work, finding related publications, algorithms and datasets, identifying a research gap, and even tools for interacting with traditional, static papers.\nSo, let\u2019s go for the second paper.\n\n\nArthur the administrator\n\u00b6\n\n\nArthur works as a system administrator in a large university library in Germany.\nHe's quite happy with his job.\nAfter working as freelance software developer for over 20 years, he now enjoys the challenge to make all the different servers and applications under his care work like a charm 24/7 while having a stable paycheck and reasonable working hours.\nHe is particularly proud that, since he took over the job, he successfully migrated all services to a private cloud infrastructure and enabled https-only traffic on all, event the internal, APIs and websites.\nSince then, there has been minimal overtime for him and close to 0 minutes downtime for the services...\nand a raise!\n\n\nArthur is interested in this new reproducibility service which the head of the library is interested in, but he is sceptical about all new systems.\nThere are going to be bugs, unforseen problems, and a lot of testing \"\"in production\"\", which he does not like.\nBut he knows scientists have been in touch with the library before about archiving data and software, so if this is a high priority for his customers, as he sees them, there is no way around it.\n\n\nAt second look though, he realizes the project seems to have all the basics straight for a stable and scalable deployment: All components are published under open source licenses, and the project maintainers provide different ready-to-use Docker images.\nArthur worries about security, so there is no better way to make sure things work well than source code access.\nThe project is written in a language he has not used before himself, but he can actually build the project himself from source with the provided instructions.\nHe also understands that, instead of reinventing the wheel, the developers seem to be competent enough to build upon established libraries.\n\n\nThe Dockerfiles are great to play around with, but also easy to integrate in his own server management solution.\nHe also likes the HTTP APIs and the setup and configuration, which seem to be very well documented.\nThis should make it easy to integrate the new solution with some custom tools he developed, but also with some legacy infrastructure he has not yet been able to get rid off.\nHe does worry a bit about the scheduling solution, since he is not very keen on Docker containers being started automatically on his servers.\nGood thing the project contributors seem to operate a public chat, and professional support is also available at reasonable prices.\n\n\nAfter some testing, he feels good to tell his colleagues: looks good to me, let's try this out!\n\n\nOlivia the operator\n\u00b6\n\n\n\"Olivia is the chancellor of a mid-size state owned university in the US.\nShe is proud to have been elected to this position a few years ago, and works very hard each day to improve both the university's reputation and the working and learning conditions of her employees and students.\nShe had to make some unpleasant first hand experiences with aspects of todays academic life, some of which sadly became almost normal: budget cuts, violence on campus, and plagiarism scandals.\nDuring all of these upsets, she is happy she never wavered on the importance of personal integrity and credibility of each and every one of the scientists and researchers working on her campus.\n\n\nTo gain some ground in the competition with other universities, Olivia puts her best assistant on the job of finding the newest trends in academia.\nSoon enough she presents to her the idea of making all research conducted at the university reproducible.\nOlivia is first surprised by the fact, being an arts major herself, as she thought that is already the case.\nShe starts reading the material provided to hear and realizes science, and especially something called computational science, is very much different from the practical work she has encountered during her years as a researcher.\nIt also becomes clear it won't work to just put out a statement forcing every lab to spend enormous efforts on changing established research practices, or to re-do what has been done 5, 10 or 20 years ago.\nThe huge variety of labs and workflows and all the different kinds of people...\ngetting out the stick simply won't work.\nBut maybe the carrot will?\n\n\nShe discovers a novel website.\nIt promises to solve all the problems of reproducibility.\nThe people behind it seem competent enough to her, but again she asks her assistant to consult with experts from the university library and computer science departments to see what they think.\nA lengthy discussion starts, and there seems to be no consensus after months of meetings and evaluations.\nThe assistant doesn't know what to report back to Olivia.\nEventually, Olivia is tired of waiting and joins a few of the meetings of the expert group as an observer.\nShe realizes nothing comes for free...\nshe encourages the expert group to create a list of requirements on establishing a reproducibility website for the university.\nShe quickly understands they might get the proper time and money to do it, because the lecturers and staff in the group realize they won't just get more work to do!\n\n\nOlivia makes the new website a matter for the boss.\nShe successfully acquires the funds to start and maintain both the technical services and to hire support staff to maintain it.\nBeyond that, the supports staff is even equipped to provided consultancy services to all researchers at the university.\nThese services quickly become popular across all disciplines working with data and code, and after just a few months, more and more fully reproducible papers appear on the public reproducibility website.\nOlivia is very glad to see the changes she introduced did not have an impact on the scientific output of the university - the monthly statistics tell her that much.\nIs the quality or quantity of the output going to increase? It's too soon to tell, but Olivia is sure it will.\nJust last week, the head of the programme reported to her that now ten papers are available on the website for which researchers from different university departments collaborated, who never collaborated before - they discovered the overlap through the new system! More than 20 undergraduate courses teaching scientific methods incorporated material from the website into their course schedule, and 50% of the graduate theses from the computer science department are now using the university reproducibility tools.\nThose are good enough signs for Olivia.\nShe decides to pitch an idea to the university board: let's include reproducibility of publications as an evaluation factor for the budget allocations next year.\nYou got to use the stick from time to time to make people appreciate the carrot.\n\n\nCarl the curator\n\u00b6\n\n\nCarl works as a digital assets curator at a university library in Germany.\nHe has been working as a librarian for about ten years and experienced the digital transformation of the field, which is why he specialized in the area of digital curation and archiving.\nHe is qualified to manage and organize several collections of digital objects at a given time and recently selected objects for an exhibition of gold standard open access publications in the software category of his institutions catalog front page.\n\n\nCarl\u2019s expertise encompasses the management of accessibility levels as well as the preservation of file integrity and meta data curation.\nSince he discovered a growing interest in the preservation of software, he realized reproducibility of research findings, including code and data increases the value and visibility of his university\u2019s portfolio.\nAs a result, Carl is working closely with the library\u2019s team for Research Data Management, in order to facilitate integration of reproducible computational environments into the digital objects' life cycle.\nThis work matches their current policies.\n\n\nAs he strongly believes publicly funded research data are public goods, Carl values his profession as a vital point of intersection between researchers, librarians und the general public.\nTherefore, when planning a selection of digital assets or curating the library\u2019s catalogs, Carl enjoys the interoperability provided by international metadata standards and linked open data vocabularies.\n\n\nPolly the publisher\n\u00b6\n\n\nPolly is the head of a large publishing firm for scientific journals.\nShe grew up being part of a publisher family, the third of four kids.\nWhile her older brothers wrestled with the family legacy, she has always been close to her late grandfather, who started the publishing business as a young man.\nSo it came as no surprise she studied arts and library science and after a few well planned career steps around the globe, she joined the family business as assistant of her father and became CEO after a few years, a decision she rarely regrets.\n\n\nThough there is one thing making her job challenging every day: technology's high development speed.\nFor a large publishing business, it is hard to keep up with new and modern technology.\nShe has to serve both old (in more than one way) customers and employers, who have had a long relationship and a work environment and processes which have developed and settled in over many years.\nOn the other hand, she sees new ideas by entrepreneurs and startups almost every week, some crazy and some rightfully called revolutionary, who experiment with new ways to publish science without the baggage of a reputation and hundreds of journals and an order of magnitude more employees.\n\n\nSo what should Polly do? Scramble up some money to acquire a few startups and replace the existing review and authoring solution? Fire all staff members who are too slow adopting the new technologies? Close journals with an excellent reputation because editors and reviewers are not tech-savvy? \n\n\nObviously, none of these were an option.\nChange had to come gradually and inclusively, not in a disruptive fashion.\nPolly turned to her CTO Charlotte.\nShe joined the company recently and played out to be a very good hire, as she was able to revive the in-house development team with a positive attitude and a few key hires.\nCharlotte is aware of the challenges and agrees to compile an action plan from her perspective.\nA few weeks later, she presents the options to Polly and the other board members.\nShe suggests to adopt an open service for interactive publications, which is an integrated solution for hosting and archiving data as well as code, all of which are often part of publications these days.\nIt is open source, but of course it does not come for free.\nCharlotte suggests a combined approach of experiments by her own staff and external consulting by the original developers of the software.\nAnd she quickly mitigates all concerns raised by the other CxOs: the website is customizable, so it will not look like the competitors versions, it is extensible, so their few \"cool features\" which have been developed over the last years will be easy to integrate, and it is compatible with the existing data repository (so no need to replace that beast of a software).\nThis new website would be an option presented to all editors to adopt for their journals.\nEducation of the company's staff would precede this offer to make sure the intended message is spread: don't be left behind, challenge reviewers and authors to improve the quality of the journals and subsequently raise the bar for high quality open science.\n\n\nRichard the reviewer\n\u00b6\n\n\nRichard is a successful researcher.\nAfter getting tenure a few years back, he embraces the chance to support students and collaborate with other scientists instead of hunting for the next easy publication to get his name on.\nA big part of his time is taken up by his membership in the editorial boards of two journals and his engagement with several more journals as a reviewer.\n\n\nRichard is \"senior\" in some ways, and he as well as his colleagues know his value lies in experience, not in hunting the latest hot new things.\nTherefore Richard never came around to catch up practically with the latest technologies, and while he has a good understanding of computer science and used to be a very capable programmer, this new stuff the kids are doing is beyond his means.\n\n\nAs the next paper review request lands in his inbox, he skims the abstract and soon thinks \"I will never be able to thoroughly evaluate this work, the code must be too complex to run on my machine\".\nBut the content is so interesting! What a shame.\nHe almost replies with a negative answer and then sees a new link at the bottom of the notification.\nThe publisher must have added a new feature.\nThe link's title is \"Click here to examine and manipulate code and data\".\n\n\nRichard clicks the link.\nHe is taken to a website looking partially similar to the old review system he is used to.\nOne the one side there is the well-known article view where he can read, add highlights and make comments.\nBut on the other side, there is a new menu he enthusiastically explores.\nIt allows him to edit parameters and re-run analysis of the paper!\nWithout even downloading any data or code.\nHe immediately sees the benefits: What a relief for his work, and what a chance to dig deeper into the article and conduct a thorough review.\n\n\nAfter some brief inspections of the article figures and manipulation of some parameters, Richard feels confident he can actually do the review properly.\nHe let's the editor know about his decision and wants to dive right back into the article, but then stops himself.\nFirst, he writes an email to his fellow editors about this new review system for evaluating code and data - they need it for their journal, too.\n\n\nRachel the reader\n\u00b6\n\n\nRachel is a second year graduate student in geoinformatics.\nShe's eager to learn and has left all struggles with the technical side of research, and has become a trusted programmer in her group and is seen as an expert in more than one programming language.\n\n\nWhen she starts one of her final courses in advanced geoinformatics, the lecturer sends out a long list of reading material.\n\nHow is she supposed to get through all of it?\n\nNever faltering, she starts reading all the documents...\n\n\nAfter the third article, she is annoyed and underwhelmed by the fancy descriptions and high-level diagrams.\nAlthough they all make sense, she feels like there is more to see and understand than is presented in the article.\n\n\nShe shares her thoughts with her teacher Teresa during the next seminar.\nTeresa can relate to Rachel's frustration and quickly points her to items 8 and 9 on the reading list.\n\"These are different\", she says.\n\n\nRachel gets back to reading.\nThe next articles start out the same as the others, but she soon realizes something is different.\nThe website takes a bit longer to load, and the graphics do not seem like they are compressed images at all.\nShe needs some time to explore the relatively complex navigation, but then is excited to discover she can read and even download all the code and data which was used to generate the figures.\nEven more, she can interact with the present methods and play around with the algorithms.\nFinally she can immediately test her own understanding, challenge her criticism, and resolve misunderstandings.\n\n\nShe plays around with the articles on the website for a little while and spends a lot longer on trying to understand the bits and pieces.\nEventually she sees a close relation of one aspect of the analysis with the research project she though about doing for her thesis.\nRachel is enthusiastic and directly downloads the whole article with its code to her own laptop to try the code out with her own dataset.",
            "title": "User scenarios"
        },
        {
            "location": "/user-scenarios/#user-scenarios",
            "text": "",
            "title": "User scenarios"
        },
        {
            "location": "/user-scenarios/#andrea-the-author-and-reader",
            "text": "Andrea turned 29 this year.\nShe is always up for a joke and a pot of coffee but is also quite impatient.\nEspecially if she has to wait for others or if she hasn\u2019t had any progress for a while.\nHowever, currently Andrea does his Ph.D in the field of geosciences.\nTwo years ago she decided to go for a cumulative dissertation, meaning she publishes scientific papers throughout his graduation and summarizes them at the end.\nShe already published his first paper a few months ago which is good, actually.\nOne of the reviewers was interested in the data and the source-code in order to reproduce the results.  After a few hours of searching (remember she is not one of the most patient), she finally finds some files which include the dataset and also the source-code in R (a statistics program).\nJust a short try if it still working...\nweird, the results are different.\nJust a short look into the paper...\nThe configuration is different than the one described in the method section.\nWell, just few manipulations and - still not working.  Although she submitted the paper just a few months ago, she can\u2019t remember the exact configuration for the results in the paper.\nFortunately, submitting data and code was not mandatory.\nBut Andrea knows she made a mistake.  More and more journals expect their authors to submit data and source-code which underlie the research findings.\nFor this reason, she wants to change her working behavior and to keep data and code files better under control.\nShe remembers her last research work which was quite unstructured, maybe already messy.\nCode and data was distributed over several folders and even computers.\nShe had so search for them for quite a while.\nMoreover, some components do not work anymore.  This time, she wants to do it better and searches for a great tool assisting her workflow.\nShe just heard about a new website supporting reproducible research.\nIt allows to upload all necessary files and to create a so called \"container\" which is \"executable\" - whatever that means.\nIt even verifies the results in the paper making it possible to detect errors immediately.\nOf course it also contains common features like sharing the publication with other authors.\nOn top of that, Andrea can also benefit from other publications.\nAs the website automatically generates a number of meta information, new search capabilities arise.\nIt is not only possible to search for other publications by using keywords, but also by using spatial and temporal properties and constraints.\nIt is even possible to constrain the search to hypotheses and research questions having certain vocabulary, thus simplifying search for related work.\nAndrea is quite impressed! She easily finds related papers around her own work.\nShe gets a good overview about existing research questions making it easier to identify a research gap he can focus on.\nAndrea doesn\u2019t even have to implement all the code lines for his statistical analysis from scratch, but can build upon existing.\nWhile reading some of the related papers in the browser, he realizes a couple of user interface widgets besides the incorporated figures.\nHe doesn\u2019t know them from traditional, static papers which are typically published as .pdf-files.\nAndrea recognizes that the widgets allow to interact with the diagrams to which the widgets belong.\nThey allow to change, for example, thresholds, input variables and constants.\nShe is thus able to check the assumptions and conclusions underlying the paper.\nShe is a bit overwhelmed by the number of new features, such as exchanging the dataset or the source-code underlying the paper.\nAndrea is quite happy about his new tool.\nIt provides support for structured work, finding related publications, algorithms and datasets, identifying a research gap, and even tools for interacting with traditional, static papers.\nSo, let\u2019s go for the second paper.",
            "title": "Andrea the author and reader"
        },
        {
            "location": "/user-scenarios/#arthur-the-administrator",
            "text": "Arthur works as a system administrator in a large university library in Germany.\nHe's quite happy with his job.\nAfter working as freelance software developer for over 20 years, he now enjoys the challenge to make all the different servers and applications under his care work like a charm 24/7 while having a stable paycheck and reasonable working hours.\nHe is particularly proud that, since he took over the job, he successfully migrated all services to a private cloud infrastructure and enabled https-only traffic on all, event the internal, APIs and websites.\nSince then, there has been minimal overtime for him and close to 0 minutes downtime for the services...\nand a raise!  Arthur is interested in this new reproducibility service which the head of the library is interested in, but he is sceptical about all new systems.\nThere are going to be bugs, unforseen problems, and a lot of testing \"\"in production\"\", which he does not like.\nBut he knows scientists have been in touch with the library before about archiving data and software, so if this is a high priority for his customers, as he sees them, there is no way around it.  At second look though, he realizes the project seems to have all the basics straight for a stable and scalable deployment: All components are published under open source licenses, and the project maintainers provide different ready-to-use Docker images.\nArthur worries about security, so there is no better way to make sure things work well than source code access.\nThe project is written in a language he has not used before himself, but he can actually build the project himself from source with the provided instructions.\nHe also understands that, instead of reinventing the wheel, the developers seem to be competent enough to build upon established libraries.  The Dockerfiles are great to play around with, but also easy to integrate in his own server management solution.\nHe also likes the HTTP APIs and the setup and configuration, which seem to be very well documented.\nThis should make it easy to integrate the new solution with some custom tools he developed, but also with some legacy infrastructure he has not yet been able to get rid off.\nHe does worry a bit about the scheduling solution, since he is not very keen on Docker containers being started automatically on his servers.\nGood thing the project contributors seem to operate a public chat, and professional support is also available at reasonable prices.  After some testing, he feels good to tell his colleagues: looks good to me, let's try this out!",
            "title": "Arthur the administrator"
        },
        {
            "location": "/user-scenarios/#olivia-the-operator",
            "text": "\"Olivia is the chancellor of a mid-size state owned university in the US.\nShe is proud to have been elected to this position a few years ago, and works very hard each day to improve both the university's reputation and the working and learning conditions of her employees and students.\nShe had to make some unpleasant first hand experiences with aspects of todays academic life, some of which sadly became almost normal: budget cuts, violence on campus, and plagiarism scandals.\nDuring all of these upsets, she is happy she never wavered on the importance of personal integrity and credibility of each and every one of the scientists and researchers working on her campus.  To gain some ground in the competition with other universities, Olivia puts her best assistant on the job of finding the newest trends in academia.\nSoon enough she presents to her the idea of making all research conducted at the university reproducible.\nOlivia is first surprised by the fact, being an arts major herself, as she thought that is already the case.\nShe starts reading the material provided to hear and realizes science, and especially something called computational science, is very much different from the practical work she has encountered during her years as a researcher.\nIt also becomes clear it won't work to just put out a statement forcing every lab to spend enormous efforts on changing established research practices, or to re-do what has been done 5, 10 or 20 years ago.\nThe huge variety of labs and workflows and all the different kinds of people...\ngetting out the stick simply won't work.\nBut maybe the carrot will?  She discovers a novel website.\nIt promises to solve all the problems of reproducibility.\nThe people behind it seem competent enough to her, but again she asks her assistant to consult with experts from the university library and computer science departments to see what they think.\nA lengthy discussion starts, and there seems to be no consensus after months of meetings and evaluations.\nThe assistant doesn't know what to report back to Olivia.\nEventually, Olivia is tired of waiting and joins a few of the meetings of the expert group as an observer.\nShe realizes nothing comes for free...\nshe encourages the expert group to create a list of requirements on establishing a reproducibility website for the university.\nShe quickly understands they might get the proper time and money to do it, because the lecturers and staff in the group realize they won't just get more work to do!  Olivia makes the new website a matter for the boss.\nShe successfully acquires the funds to start and maintain both the technical services and to hire support staff to maintain it.\nBeyond that, the supports staff is even equipped to provided consultancy services to all researchers at the university.\nThese services quickly become popular across all disciplines working with data and code, and after just a few months, more and more fully reproducible papers appear on the public reproducibility website.\nOlivia is very glad to see the changes she introduced did not have an impact on the scientific output of the university - the monthly statistics tell her that much.\nIs the quality or quantity of the output going to increase? It's too soon to tell, but Olivia is sure it will.\nJust last week, the head of the programme reported to her that now ten papers are available on the website for which researchers from different university departments collaborated, who never collaborated before - they discovered the overlap through the new system! More than 20 undergraduate courses teaching scientific methods incorporated material from the website into their course schedule, and 50% of the graduate theses from the computer science department are now using the university reproducibility tools.\nThose are good enough signs for Olivia.\nShe decides to pitch an idea to the university board: let's include reproducibility of publications as an evaluation factor for the budget allocations next year.\nYou got to use the stick from time to time to make people appreciate the carrot.",
            "title": "Olivia the operator"
        },
        {
            "location": "/user-scenarios/#carl-the-curator",
            "text": "Carl works as a digital assets curator at a university library in Germany.\nHe has been working as a librarian for about ten years and experienced the digital transformation of the field, which is why he specialized in the area of digital curation and archiving.\nHe is qualified to manage and organize several collections of digital objects at a given time and recently selected objects for an exhibition of gold standard open access publications in the software category of his institutions catalog front page.  Carl\u2019s expertise encompasses the management of accessibility levels as well as the preservation of file integrity and meta data curation.\nSince he discovered a growing interest in the preservation of software, he realized reproducibility of research findings, including code and data increases the value and visibility of his university\u2019s portfolio.\nAs a result, Carl is working closely with the library\u2019s team for Research Data Management, in order to facilitate integration of reproducible computational environments into the digital objects' life cycle.\nThis work matches their current policies.  As he strongly believes publicly funded research data are public goods, Carl values his profession as a vital point of intersection between researchers, librarians und the general public.\nTherefore, when planning a selection of digital assets or curating the library\u2019s catalogs, Carl enjoys the interoperability provided by international metadata standards and linked open data vocabularies.",
            "title": "Carl the curator"
        },
        {
            "location": "/user-scenarios/#polly-the-publisher",
            "text": "Polly is the head of a large publishing firm for scientific journals.\nShe grew up being part of a publisher family, the third of four kids.\nWhile her older brothers wrestled with the family legacy, she has always been close to her late grandfather, who started the publishing business as a young man.\nSo it came as no surprise she studied arts and library science and after a few well planned career steps around the globe, she joined the family business as assistant of her father and became CEO after a few years, a decision she rarely regrets.  Though there is one thing making her job challenging every day: technology's high development speed.\nFor a large publishing business, it is hard to keep up with new and modern technology.\nShe has to serve both old (in more than one way) customers and employers, who have had a long relationship and a work environment and processes which have developed and settled in over many years.\nOn the other hand, she sees new ideas by entrepreneurs and startups almost every week, some crazy and some rightfully called revolutionary, who experiment with new ways to publish science without the baggage of a reputation and hundreds of journals and an order of magnitude more employees.  So what should Polly do? Scramble up some money to acquire a few startups and replace the existing review and authoring solution? Fire all staff members who are too slow adopting the new technologies? Close journals with an excellent reputation because editors and reviewers are not tech-savvy?   Obviously, none of these were an option.\nChange had to come gradually and inclusively, not in a disruptive fashion.\nPolly turned to her CTO Charlotte.\nShe joined the company recently and played out to be a very good hire, as she was able to revive the in-house development team with a positive attitude and a few key hires.\nCharlotte is aware of the challenges and agrees to compile an action plan from her perspective.\nA few weeks later, she presents the options to Polly and the other board members.\nShe suggests to adopt an open service for interactive publications, which is an integrated solution for hosting and archiving data as well as code, all of which are often part of publications these days.\nIt is open source, but of course it does not come for free.\nCharlotte suggests a combined approach of experiments by her own staff and external consulting by the original developers of the software.\nAnd she quickly mitigates all concerns raised by the other CxOs: the website is customizable, so it will not look like the competitors versions, it is extensible, so their few \"cool features\" which have been developed over the last years will be easy to integrate, and it is compatible with the existing data repository (so no need to replace that beast of a software).\nThis new website would be an option presented to all editors to adopt for their journals.\nEducation of the company's staff would precede this offer to make sure the intended message is spread: don't be left behind, challenge reviewers and authors to improve the quality of the journals and subsequently raise the bar for high quality open science.",
            "title": "Polly the publisher"
        },
        {
            "location": "/user-scenarios/#richard-the-reviewer",
            "text": "Richard is a successful researcher.\nAfter getting tenure a few years back, he embraces the chance to support students and collaborate with other scientists instead of hunting for the next easy publication to get his name on.\nA big part of his time is taken up by his membership in the editorial boards of two journals and his engagement with several more journals as a reviewer.  Richard is \"senior\" in some ways, and he as well as his colleagues know his value lies in experience, not in hunting the latest hot new things.\nTherefore Richard never came around to catch up practically with the latest technologies, and while he has a good understanding of computer science and used to be a very capable programmer, this new stuff the kids are doing is beyond his means.  As the next paper review request lands in his inbox, he skims the abstract and soon thinks \"I will never be able to thoroughly evaluate this work, the code must be too complex to run on my machine\".\nBut the content is so interesting! What a shame.\nHe almost replies with a negative answer and then sees a new link at the bottom of the notification.\nThe publisher must have added a new feature.\nThe link's title is \"Click here to examine and manipulate code and data\".  Richard clicks the link.\nHe is taken to a website looking partially similar to the old review system he is used to.\nOne the one side there is the well-known article view where he can read, add highlights and make comments.\nBut on the other side, there is a new menu he enthusiastically explores.\nIt allows him to edit parameters and re-run analysis of the paper!\nWithout even downloading any data or code.\nHe immediately sees the benefits: What a relief for his work, and what a chance to dig deeper into the article and conduct a thorough review.  After some brief inspections of the article figures and manipulation of some parameters, Richard feels confident he can actually do the review properly.\nHe let's the editor know about his decision and wants to dive right back into the article, but then stops himself.\nFirst, he writes an email to his fellow editors about this new review system for evaluating code and data - they need it for their journal, too.",
            "title": "Richard the reviewer"
        },
        {
            "location": "/user-scenarios/#rachel-the-reader",
            "text": "Rachel is a second year graduate student in geoinformatics.\nShe's eager to learn and has left all struggles with the technical side of research, and has become a trusted programmer in her group and is seen as an expert in more than one programming language.  When she starts one of her final courses in advanced geoinformatics, the lecturer sends out a long list of reading material. How is she supposed to get through all of it? \nNever faltering, she starts reading all the documents...  After the third article, she is annoyed and underwhelmed by the fancy descriptions and high-level diagrams.\nAlthough they all make sense, she feels like there is more to see and understand than is presented in the article.  She shares her thoughts with her teacher Teresa during the next seminar.\nTeresa can relate to Rachel's frustration and quickly points her to items 8 and 9 on the reading list.\n\"These are different\", she says.  Rachel gets back to reading.\nThe next articles start out the same as the others, but she soon realizes something is different.\nThe website takes a bit longer to load, and the graphics do not seem like they are compressed images at all.\nShe needs some time to explore the relatively complex navigation, but then is excited to discover she can read and even download all the code and data which was used to generate the figures.\nEven more, she can interact with the present methods and play around with the algorithms.\nFinally she can immediately test her own understanding, challenge her criticism, and resolve misunderstandings.  She plays around with the articles on the website for a little while and spends a lot longer on trying to understand the bits and pieces.\nEventually she sees a close relation of one aspect of the analysis with the research project she though about doing for her thesis.\nRachel is enthusiastic and directly downloads the whole article with its code to her own laptop to try the code out with her own dataset.",
            "title": "Rachel the reader"
        },
        {
            "location": "/glossary/",
            "text": "Glossary\n\u00b6\n\n\nCRUD\n\u00b6\n\n\nBasic operations on a digital artefact are create, read, update, and delete, often abbreviated to \"\nCRUD\n\".\n\n\nDigital Object Identifier\n\u00b6\n\n\nSee \nDOI\n.\n\n\nDOI\n\u00b6\n\n\n\n\nIn computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects [..]\n\n\nA DOI aims to be \"resolvable\", usually to some form of access to the information object to which the DOI refers.\n\nvia \nWikipedia\n, see also \nhttps://doi.org\n\n\n\n\nERC\n\u00b6\n\n\nExecutable Research Compendium, see this \nscientific article\n for concepts and the \nspecification\n for technical documentation.\n\n\nExecutable Research Compendium\n\u00b6\n\n\nSee \nERC\n\n\nJavaScript Promises\n\u00b6\n\n\n\n\nA Promise is an object representing the eventual completion or failure of an asynchronous operation. [...] Essentially, a promise is a returned object to which you attach callbacks, instead of passing callbacks into a function.\n\nvia \nMDN web docs\n\n\n\n\nLiterate Programming\n\u00b6\n\n\n\n\nLiterate programming is a programming paradigm [..] in which a program is given as an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which a compilable source code can be generated.\n\nvia \nWikipedia",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#glossary",
            "text": "",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#crud",
            "text": "Basic operations on a digital artefact are create, read, update, and delete, often abbreviated to \" CRUD \".",
            "title": "CRUD"
        },
        {
            "location": "/glossary/#digital-object-identifier",
            "text": "See  DOI .",
            "title": "Digital Object Identifier"
        },
        {
            "location": "/glossary/#doi",
            "text": "In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects [..]  A DOI aims to be \"resolvable\", usually to some form of access to the information object to which the DOI refers. \nvia  Wikipedia , see also  https://doi.org",
            "title": "DOI"
        },
        {
            "location": "/glossary/#erc",
            "text": "Executable Research Compendium, see this  scientific article  for concepts and the  specification  for technical documentation.",
            "title": "ERC"
        },
        {
            "location": "/glossary/#executable-research-compendium",
            "text": "See  ERC",
            "title": "Executable Research Compendium"
        },
        {
            "location": "/glossary/#javascript-promises",
            "text": "A Promise is an object representing the eventual completion or failure of an asynchronous operation. [...] Essentially, a promise is a returned object to which you attach callbacks, instead of passing callbacks into a function. \nvia  MDN web docs",
            "title": "JavaScript Promises"
        },
        {
            "location": "/glossary/#literate-programming",
            "text": "Literate programming is a programming paradigm [..] in which a program is given as an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which a compilable source code can be generated. \nvia  Wikipedia",
            "title": "Literate Programming"
        },
        {
            "location": "/metadata/",
            "text": "Metadata workflows\n\u00b6\n\n\nThis document describes the internal processes handling metadata for ERC.\nFor information on the metadata model for ERC as part of the o2r web API please see \nthe API specification\n.\n\n\nThe remainder of this document describes \nwho\n handles metadata \nwhen\n and \nhow\n within the \no2r architecture\n.\n\n\nFiles vs. database\n\u00b6\n\n\nIn all workflows files are created within ERC in a specific subdirectory \n.erc\n holding different kinds, formats, or versions of metadata.\nFor ease of access via web API, the information is also stored within the database.\n\n\nThe files in the compendium are always the normative source of information.\n\n\nThe term \nbrokering\n means the translation from schema-less to schema-specific metadata, as well as inter-schema mappings.\nThe brokering output is then stored in respective files and mirrored to the database by the reproducibility service.\n\n\nMetadata extraction and brokering during creation\n\u00b6\n\n\nmuncher\n is the main \nCRUD\n component for compedia.\nIt controls the creation workflow.\n\n\nThe creation from the metadata perspective is as follows:\n\n\n\n\ninit\n stores the files for a new ERC in a directory.\n\n\nextract\n uses \nmetaextract.py\n (\ndocs\n) to analyse the incoming ERC and creates new files with \nraw\n metadata for each of the scanned files. Currently the following types of files will be considered: \n.r, .rmd, netcdf, \"bagit.txt\"\n. Future releases of the extractor will be likely to consider \n.tex, .json (geojson), .jp2, .tiff\n and more.\nThis raw metadata itself is \nschema-less\n and non-semantic.\nThe processed files are in conceptual competition for the best representative of the working directory's meta information, i.e. there will be only one main output, ideally represented by the most complete set of metadata.\nBy default the competing bits of information will also be preserved in \n.erc/metadata_raw_<filename>.json\n where \nfilename\n is an identifier based on the original source file.\n\n\noutput file: \n.erc/metadata_raw.json\n\n\ndatabase field: \n<compendium>.metadata.raw\n\n\nbroker\n uses \nmetabroker.py\n (\ndocs\n) to translate the \nraw\n metadata in \njson\n to \no2r\n metadata in \njson\n as being compliant to the o2r json-schema.\n \u00a0- output file: \n.erc/metadata_o2r_X.json\n (where \nX\n is the version number as set in the \no2r-map.json\n mapping file, e.g. \n1\n)\n \u00a0- database field: \n<compendium>.metadata.o2r\n\n\n(\nharvest\n TBD; will connect to third party database endpoint via OAI-PMH to gather additional information for the enrichment of the o2r metadata collected via extraction)\n\n\nsave\n stores the new ERC to the database including the aforementioned metadata fields.\n\n\nuser check\n provides an interactive form to the uploading user to control and edit the suggested metadata.\nSuggestions are based on \no2r\n metadata.\nThe check workflow is handled in the web client project.\n\n\nupdate\n updates the metadata in both database and file with the user's edits.\nThis step creates \nvalid o2r\n metadata.\nThe metadata update includes \nall brokering\n to the configured metadata formats, meaning the brokered metadata is always up-to-date and based on the same source, the \no2r\n metadata.\n\n\n\n\nBy design there is no metadata brokering during shipments or job executions.\nBecause it is likely that not all information can be brokered automatically, the metadata required by shipping destinations are mandatory in the o2r metadata model to reduce the user involvement to a minimum, i.e. when updating the metadata.\nIn the same vein, all \nvalidation\n takes place during metadata updates, because that is the only time a user can react to validation errors.\n\n\nMetadata for shipments\n\u00b6\n\n\nThe \nshipper\n uses the metadata stored in the ERC directory \n.erc\n to start a shipment of data or metadata to third-party repositories.\nIt does not do any updating, brokering, or validation.\n\n\nMetadata mappings\n\u00b6\n\n\n\n\n\n\n\n\ndestination\n\n\nmodel\n\n\nformat(s)\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb2share\n | using o2r schema for the o2r community depositions on \nb2share\n  | \njson\n | ...\n\ncodemeta\n | \ncodemeta 2.0-rc\n | \njson ld\n | ...\n\nzenodo\n | \nDeposition metadata\n | \njson\n | for storing full ERC in the Zenodo data repository; Zenodo also publishes metadata on \nDataCite",
            "title": "Metadata"
        },
        {
            "location": "/metadata/#metadata-workflows",
            "text": "This document describes the internal processes handling metadata for ERC.\nFor information on the metadata model for ERC as part of the o2r web API please see  the API specification .  The remainder of this document describes  who  handles metadata  when  and  how  within the  o2r architecture .",
            "title": "Metadata workflows"
        },
        {
            "location": "/metadata/#files-vs-database",
            "text": "In all workflows files are created within ERC in a specific subdirectory  .erc  holding different kinds, formats, or versions of metadata.\nFor ease of access via web API, the information is also stored within the database.  The files in the compendium are always the normative source of information.  The term  brokering  means the translation from schema-less to schema-specific metadata, as well as inter-schema mappings.\nThe brokering output is then stored in respective files and mirrored to the database by the reproducibility service.",
            "title": "Files vs. database"
        },
        {
            "location": "/metadata/#metadata-extraction-and-brokering-during-creation",
            "text": "muncher  is the main  CRUD  component for compedia.\nIt controls the creation workflow.  The creation from the metadata perspective is as follows:   init  stores the files for a new ERC in a directory.  extract  uses  metaextract.py  ( docs ) to analyse the incoming ERC and creates new files with  raw  metadata for each of the scanned files. Currently the following types of files will be considered:  .r, .rmd, netcdf, \"bagit.txt\" . Future releases of the extractor will be likely to consider  .tex, .json (geojson), .jp2, .tiff  and more.\nThis raw metadata itself is  schema-less  and non-semantic.\nThe processed files are in conceptual competition for the best representative of the working directory's meta information, i.e. there will be only one main output, ideally represented by the most complete set of metadata.\nBy default the competing bits of information will also be preserved in  .erc/metadata_raw_<filename>.json  where  filename  is an identifier based on the original source file.  output file:  .erc/metadata_raw.json  database field:  <compendium>.metadata.raw  broker  uses  metabroker.py  ( docs ) to translate the  raw  metadata in  json  to  o2r  metadata in  json  as being compliant to the o2r json-schema.\n \u00a0- output file:  .erc/metadata_o2r_X.json  (where  X  is the version number as set in the  o2r-map.json  mapping file, e.g.  1 )\n \u00a0- database field:  <compendium>.metadata.o2r  ( harvest  TBD; will connect to third party database endpoint via OAI-PMH to gather additional information for the enrichment of the o2r metadata collected via extraction)  save  stores the new ERC to the database including the aforementioned metadata fields.  user check  provides an interactive form to the uploading user to control and edit the suggested metadata.\nSuggestions are based on  o2r  metadata.\nThe check workflow is handled in the web client project.  update  updates the metadata in both database and file with the user's edits.\nThis step creates  valid o2r  metadata.\nThe metadata update includes  all brokering  to the configured metadata formats, meaning the brokered metadata is always up-to-date and based on the same source, the  o2r  metadata.   By design there is no metadata brokering during shipments or job executions.\nBecause it is likely that not all information can be brokered automatically, the metadata required by shipping destinations are mandatory in the o2r metadata model to reduce the user involvement to a minimum, i.e. when updating the metadata.\nIn the same vein, all  validation  takes place during metadata updates, because that is the only time a user can react to validation errors.",
            "title": "Metadata extraction and brokering during creation"
        },
        {
            "location": "/metadata/#metadata-for-shipments",
            "text": "The  shipper  uses the metadata stored in the ERC directory  .erc  to start a shipment of data or metadata to third-party repositories.\nIt does not do any updating, brokering, or validation.",
            "title": "Metadata for shipments"
        },
        {
            "location": "/metadata/#metadata-mappings",
            "text": "destination  model  format(s)  description             b2share  | using o2r schema for the o2r community depositions on  b2share   |  json  | ... codemeta  |  codemeta 2.0-rc  |  json ld  | ... zenodo  |  Deposition metadata  |  json  | for storing full ERC in the Zenodo data repository; Zenodo also publishes metadata on  DataCite",
            "title": "Metadata mappings"
        }
    ]
}