{
    "docs": [
        {
            "location": "/",
            "text": "Software architecture for Opening Reproducible Research\n\n\n1. Introduction and Goals\n\n\n1.1 Requirements Overview\n\n\nThe system must provide a reliable way to create and examine (e.g. discover, inspect) reproducibility packages of computational research to support reproducible publications.\nThis architecture describes the relationship of the reproducibility service with other services from the context of scientific collaboration and publishing and how they can be combined to a new system for scholarly publications.\n\n\nAt its core is the concept of the \nExecutable Research Compendium\n (ERC, see \nspecification\n and \narticle\n), and a supporting reproducibility service, which is defined by a \nweb \nAPI\n specification\n and its \nreference implementation\n is published as open source software.\n\n\n1.2 Quality Goals\n\n\n\n\nTransparency\n\n\nThe system must be transparent to allow scrutiny required by a rigorous scientific process.\n\n\nSeparation of concern\n\n\nThe system must integrate with existing services and focus on the core functionality.\nIt must not replicate existing functionality such as storage or persistent identification.\n\n\nFlexibility & modularity\n\n\nIn regard to the research project setting, the system components must be well separated, so that functions can be developed independently, e.g. using different programming languages.\nThis allows different developers to contribute efficiently.\n\n\n\n\n1.3 Stakeholders\n\n\n\n\n\n\n\n\nRole/Name\n\n\nGoal/point of contact\n\n\nRequired interaction\n\n\n\n\n\n\n\n\n\n\nAuthor (scientist)\n\n\npublish ERC as part of a scientific publication process\n\n\n-\n\n\n\n\n\n\nReviewer (scientist)\n\n\nexamine ERC during a review process\n\n\n-\n\n\n\n\n\n\nCo-author (scientist)\n\n\ncontribute to ERC during research (e.g. cloud based)\n\n\n-\n\n\n\n\n\n\nReader (scientist)\n\n\nview and interact with ERC on a journal website\n\n\n-\n\n\n\n\n\n\nPublisher\n\n\nincrease quality of publications in journals with ERC\n\n\n-\n\n\n\n\n\n\nCurator/preservationist\n\n\nensure research is complete and archivable using ERC\n\n\n-\n\n\n\n\n\n\nOperator\n\n\nprovide infrastructure to researchers at my university to collaborate and conduct high-quality research using ERC\n\n\n-\n\n\n\n\n\n\nDeveloper\n\n\nuse and extend the tools around ERC\n\n\n-\n\n\n\n\n\n\n\n\nSome of the stakeholders are accompanied by \nuser scenarios\n in prose.\n\n\n2. Architecture constraints\n\n\nThe few constraints on this project are reflected in the final solution. This section shows them and if applicable, their motivation. (based on \nbiking2\n)\n\n\n2.1 Technical constraints\n\n\n\n\n\n\n\n\n\u00a0\n\n\nConstraint\n\n\nBackground and/or motivation\n\n\n\n\n\n\n\n\n\n\nTECH.1\n\n\nOnly open licenses\n\n\nAll third party software or used data must be available under a suitable code license, i.e. either \nOSI-approved\n or \nODC license\n.\n\n\n\n\n\n\nTECH.2\n\n\nOS independent development and deployment\n\n\nServer applications must run in well defined \nDocker\n containers to allow installation on any host system and to not limit developers to a specific language or environment.\n\n\n\n\n\n\nTECH.3\n\n\nDo not store secure information\n\n\nThe team members experience and available resources do not allow for handling information with security concerns, so no critical data, such as user passwords but also data with privacy concerns, must be stored in the system.\n\n\n\n\n\n\n\n\n2.2 Organizational constraints\n\n\n\n\n\n\n\n\n\u00a0\n\n\nConstraint\n\n\nBackground and/or motivation\n\n\n\n\n\n\n\n\n\n\nORG.1\n\n\nTeam and schedule\n\n\nhttp://o2r.info/about\n\n\n\n\n\n\nORG.2\n\n\nDo not interfere with existing well-established peer-review process\n\n\nThis software is \nnot\n going to change how scientific publishing works, nor should it. While intentioned to support public peer-reviews, open science etc., the software should be agnostic of these aspects.\n\n\n\n\n\n\nORG.3\n\n\nOnly open licenses\n\n\nAll created software must be available under an \nOSI-approved\n license, documentation and specification under a \nCC license\n.\n\n\n\n\n\n\nORG.4\n\n\nVersion control/management\n\n\nCode must be versioned using \ngit\n and published on \nGitHub\n.\n\n\n\n\n\n\nORG.5\n\n\nTransfer from group domain to persistent domain\n\n\nThe ERC bundles artifacts coming from a private or group domain for a transfer to a public and persistent domain (cf. \nCuration Domain Model\n), which imposes requirements on the availability and enrichment of metadata\n\n\n\n\n\n\n\n\n2.3 Conventions\n\n\n\n\n\n\n\n\n\u00a0\n\n\nConstraint\n\n\nBackground and/or motivation\n\n\n\n\n\n\n\n\n\n\nCONV.1\n\n\nProvide architecture documentation\n\n\nBased on \narc42\n (template version 7.0).\n\n\n\n\n\n\nCONV.2\n\n\nReasonably follow coding conventions\n\n\nTypical project layout and coding conventions of the respective used language should be followed as far as possible. However, we explicitly accept the research project context and do \nnot\n provide full tests suites or documentation beyond what is needed by project team members.\n\n\n\n\n\n\nCONV.3\n\n\nDocumentation is English\n\n\nInternational research project, must be understandable by anyone interested.\n\n\n\n\n\n\nCONV.4\n\n\nUse subjectivization for server component names\n\n\nServer-side components are named using personalized verbs or professions: \nmuncher\n, \nloader\n, \ntransporter\n. All git repositories for software use an \no2r-\n prefix, in case of server-side components e.g. \no2r-shipper\n.\n\n\n\n\n\n\nCONV.5\n\n\nConfiguration using environment variables\n\n\nServer-side components must be configurable using all caps environment variables prefixed with the component name, e.g. \nSHIPPER_THE_SETTING\n, for required settings. Other settings should be put in a settings file suitable for the used language, e.g. \nconfig.js\n or \nconfig.yml\n.\n\n\n\n\n\n\n\n\n3. System scope and context\n\n\n3.1 Business context\n\n\n\n\n\n\n\n\n\n\nCommunication partner\n\n\nExchanged data\n\n\nTechnology/protocol\n\n\n\n\n\n\n\n\n\n\nReproducibility service\n, e.g. \no2r\n\n\npublication platforms utilize creation and examination services for ERC, reproducibility service uses repositories to retrieve software artifacts, store runtime environment images, and save complete ERC\n\n\nHTTP\n APIs\n\n\n\n\n\n\nPublication platform\n, e.g. online journal website and review system\n\n\nusers access ERC status and metadata via search results and paper landing pages; review process integrates ERC details and supports manipulation;\n\n\nsystem's API using \nHTTP\n with \nJSON\n payload\n\n\n\n\n\n\nID provider\n\n\nretrieve unique user IDs, user metadata, and authentication tokens; user must log in with the provider\n\n\nHTTP\n\n\n\n\n\n\nExecution infrastructure\n\n\nERC can be executed using a shared/distributed infrastructure\n\n\nHTTP\n\n\n\n\n\n\nData repository\n\n\nthe reproducibility service fetches (a) content for ERC creation, or (b) complete ERC, from different sources and stores crated ERC persistently at suitable repositories\n\n\nHTTP\n, \nFTP\n, \nWebDAV\n, \ngit\n\n\n\n\n\n\nRegistry\n\n\nthe reproducibility service can deliver metadata on published ERC to registries/catalogues/search portals, but also retrieve/harvest contextual metadata during ERC creation; users discover ERC via registries\n\n\n(proprietary) \nHTTP\n APIs, persistent identifiers (\nDOI\n), \nOAI-PMH\n\n\n\n\n\n\nSoftware repository\n\n\nsoftware repository provide software artifacts during ERC creation and store executable runtime environments\n\n\nHTTP\n APIs\n\n\n\n\n\n\nArchives and digital preservation systems\n\n\nwhen information is transferred from the private (research group, single researcher) or group domain (collaborations) to a public and persistent domain (archives, repositories), then extended data and metadata management is needed but also different access and re-use is enabled; these concerns are only relevant in so far as \ndata repositories\n must be supported, but further aspects such as access rights in archives are only mediately relevant for the reproducibility service\n\n\nmetadata in \nJSON\n and \nXML\n provided via \nHTTP\n or as files\n\n\n\n\n\n\n\n\n3.2 Technical context\n\n\nAll components use \nHTTP\n over cable networks connections for communication of all exchanged data (metadata documents, ERC, Linux containers, etc.).\n\n\n4. Solution strategy\n\n\nThis section provides a short overview of architecture decisions and for some the reasoning behind them.\n\n\nWeb API\n\n\nThe developed solution is set in an existing system of services, and first and foremost must integrate well with these systems, focussing on the specific missing features of building and running ERCs.\nThese features are provided via a \nwell-defined RESTful API\n.\n\n\nMicroservices\n\n\nTo allow a dynamic development and support the large variety of skills, all server-side features are developed in independent \nmicroservices\n.\nThese microservices handle only specific functional parts of the API and allow independent development and deployment cycles.\nCore components are developed using server-side JavaScript based on Node.js while other components are implemented Python.\n\n\nWe accept that this diversification \nincreases complexity\n of both development and testing environments and the deployment of said services.\n\n\nRequired documentation is minimal. The typical structure should follow common practices.\n\n\nStorage and intra-service communication\n\n\nIn accordance with the system scope, there is no reliable storage solution implemented.\nThe microservices simply share a common pointer to a local file system path.\nStorage of ERC is only implemented to make the solution independent during development and for the needs of core functionality (temporal storage), but it is not a feature the solution will eventually provide.\n\n\nThe unifying component of the architecture is the \ndatabase\n.\nIt is known to all microservices.\n\n\nSome microservices communicate via an eventing mechanism for real-time updates, such as the search database and the component providing live updates to the user via WebSockets-\nThe eventing is based on the operation log of the database (which is normally used to synchronise database nodes).\nThis is a clear \nmisuse of an internal feature\n, but a lot simpler than maintaining a full-blown eventing solution.\n\n\nDemonstration, user data & authentication\n\n\nTo be able to demonstrate the system, a \nbrowser-based client application\n is developed.\nIt uses the RESTful API to control the system.\n\nOAuth 2.0\n is used for authentication and minimal information, which is already public, is stored for each user.\nThis information is shared between all services that require authentication via the database.\n\n\nThe client application manages the control flow\n of all user interactions.\n\n\nTools\n\n\nIf standalone tools are developed, they should provide a command-line interface (CLI) that allows integration into microservices when needed.\nThanks to the container architecture and the controlled, we don't need to worry about documentation for or distribution/packaging of these tools.\nIt must only be ensured they are correctly installed using the microservice's Dockerfile.\nThe only required documentation is for the installation into a container and usage of the CLI.\n\n\n5. Building block view\n\n\n5.1 Refinement Level 1\n\n\n5.1.1 Blackbox Publication Platforms\n\n\nPublications platforms are the online interaction points of users with scientific works.\nUsers create publications, e.g. submitting to a scientific journal, publishing on a pre-print server or archive, or collaborating in online repositories.\nUsers examine publications, e.g. browsing, searching, reading, downloading, or reviewing.\n\n\n5.1.2 Blackbox ID Provider\n\n\nIdentification information of distributed systems is crucial, and for security reasons as well as for limiting manual reproduction of metadata, a central service can provide all of\n\n\n\n\nunique \nidentification of users\n and \nmetadata on users\n,\n\n\nauthentication\n of users, and\n\n\nmetadata on a user's \nworks\n, e.g. publications or ERC.\n\n\n\n\nPersistent identifiers for artifacts in the reproducibility service itself are \nnot required\n, as these are provided by data storage and registries.\nHowever, services such as \nePIC\n could allow to retrieve persistent IDs.\n\n\n5.1.3 Blackbox Execution Infrastructure\n\n\nThe execution infrastructure provides CPU time and temporary result storage space for execution of ERC, both \"as is\" and with manipulation, i.e. changed parameters.\n\n\n5.1.4 Blackbox Data Repositories\n\n\nData repositories are all services storing data but not software.\nMore specifically, they may store software \"as data\", but not with software-specific features such as code versioning or installation binaries for different computer architectures.\nData repositories may be self-hosted or public/free, domain-specific or generic.\nThey typically provide persistent identifiers or handles, e.g. a \nDOI\n or \nURN\n.\nThey are used both for loading created ERC and for storing the ERC created by the reproducibility service.\n\n\n5.1.5 Blackbox Registries\n\n\nRegistries are metadata indexes or catalogues.\n\n\nThey are recipients of metadata exports by the reproducibility service to share information about ERC, e.g. add a new ERC to an author's profile.\nThis requires the reproducibility services to translate the internal metadata model into the recipients data model and encoding. \n\n\nThey are sources of metadata during ERC creation when the information in the fetched content is used to query registries for additional information which can be offered to the user.\n\n\n5.1.6 Blackbox Software Repositories\n\n\nSoftware repositories are a source and a sink for software at different abstraction levels.\nThey are a source for software artifacts or packages, such as system packages in install a library or language-specific extension packages.\nThey are a sink for executable images of software, which comprise a number of software artifacts, for a specific ERC instance.\n\n\n5.2 Refinement Level 2\n\n\n5.2.1 Whitebox Publication Platforms\n\n\nPublication platforms can be roughly divided into two groups.\nThey can be either specific journals hosted independently, such as \nJStatSoft\n or \nJOSS\n, or a larger platform provided by a publisher to multiple journals, such as \nScienceDirect\n, \nMDPI\n, \nSpringerLink\n, or \nPLOS\n.\nTo some extend, pre-print servers, for example \nOSF\n or \narXiv.org\n, can also fall into the latter category.\n\n\nIntegration with the reproducibility service can happen via plug-ins to generic software, e.g. \nOJS\n, or by bespoke extensions.\nIntegrations are based on the service's public API.\n\n\n5.2.2 Whitebox ID Provider\n\n\nThe reproducibility service uses \nORCID\n to authenticate users and retrieve user and works metadata.\nInternally, the user's public \nORCID\n is the main identifier.\n\n\n5.2.3 Whitebox Execution Infrastructure\n\n\nSuch an infrastructure could be either self-hosted, e.g. \nDocker Swarm\n-based, or use a cloud service provide, such as \nAmazon EC2\n, \nDocker Cloud\n, or even use continuous integration services such as \nTravis CI\n or \nGitlab CI\n.\n\n\n5.2.4 Whitebox Data Repositories\n\n\n\n\nCollaboration platforms\n, e.g. \nownCloud/Sciebo\n, \nGitHub\n, \nShareLatex\n, \nOSF\n, allow users to create, store, and share their research (code, text, data, et cetera).\nThe reproducibility service fetches contents for building an ERC from them based on public links, e.g. a public GitHub repository or shared Sciebo folder.\nIt is possible that ERC creation is linked to a project/repository on a collaboration platform and updates trigger an ERC (re-)creation or execution.\n\n\nProtocols: \nWebDAV\n, \nownCloud\n, \nHTTP\n (including \nwebhooks\n), \ngit\n\n\nDomain data repositories\n, e.g. \nPANGAEA\n or \nGFZ Data Services\n, can be accessed by the reproducibility service during creation and execution of ERC to download data.\nAllowing access to data repositories reduces data duplication but requires control over/trust in the respective repository.\n\n\nProtocol: \nHTTP\n APIs\n\n\nGeneric \nRepositories\n, e.g. \nZenodo\n, \nMendeley Data\n, \nFigshare\n, \nOSF\n, provide (a) access to complete ERC stored in repositories for inspection and execution by the reproducibility service, and (b) storage of created ERC. repositories.\n\n\nThe reproducibility service \ndoes not persistently store anything\n.\n\n\nProtocols: (authenticated) \nHTTP\n APIs\n\n\nArchives\n, e.g. using an installation of \nArchivematica\n, might provide long-term preservation of ERC. Preservation lies in the responsibility of the repository, which might save the hosted content to an archive, or an archive harvests a repository.\n\n\nProtocol: \nHTTP\n carrying bitstreams and metadata\n\n\n5.2.5 Whitebox Registries\n\n\nResearch data registries and websites, for example (\nCRIS\n, \nDataCite\n, \nGoogle Scholar\n, \nScopus\n, \nAltmetric\n, to name just a few, collect metadata on publications and provide services with this data.\nServices comprise discovery but also derivation of citation data and creating networks of researchers and publications.\n\n\nThe listed examples include open platforms, commercial solutions, and institution-specific platforms.\nSome of the registries offer a public, well-defined API to retrieve structured metadata and to create new records.\n\n\nProtocol: \nHTTP\n APIs\n\n\n5.2.6 Whitebox Software Repositories\n\n\n\n\n5.2.6.1 Blackbox Package repositories\n\n\nPackage repositories are used during ERC creation to download and install software artifacts for specific operating systems, e.g. \nDebian APT\n or \nUbuntu Launchpad\n, for specific programming languages or environments, e.g. \nCRAN\n, or from source, e.g. \nGitHub\n.\n\n\n5.2.6.2 Blackbox Container registries\n\n\nContainer registries such as \nDocker Hub\n, \nQuay\n, self-hosted \nDocker Registry 2.0\n or \nAmazon ERC\n, store executable images of runtime environments.\nThey can be used to distribute the runtime environments across the execution infrastructure and provide an intermediate ephemeral storage for the reproducibility service.\n\n\n5.2.7 Whitebox Reproducibility Service\n\n\n\n\n5.2.7.1 Blackbox Webserver\n\n\nA webserver handles all incoming calls to the API and distributes them to the respective microservice.\nA working \nnginx\n configuration is available \nin the test setup\n.\n\n\n5.2.7.2 Blackbox UI\n\n\nThe UI is a web application based on \nAngular JS\n, see \no2r-platform\n.\nIt connects to an execution microservice (\u00b5service) for real-time WebSocket-based notifications.\n\n\n5.2.7.3 Blackbox microservices\n\n\nThe reproducibility service uses a \nmicroservice architecture\n to separate functionality defined by the \nweb API specification\n into manageable units.\n\n\nThis allows scalability (selected \u00b5services can be deployed as much as needed) and technology independence for each use case and developer.\nThe \u00b5services all access one main database and a shared file storage.\n\n\n5.2.7.4 Blackbox Tools\n\n\nSome functionality is developed as standalone tools and used as such in the \u00b5services instead of re-implementing features.\nThese tools are integrated via their command line interface (CLI).\n\n\n5.2.7.5 Blackbox Database\n\n\nThe main database is the unifying element of the microservice architecture.\nAll information shared between \u00b5services or transactions between microservices are made via the database, including session state handling (= authentication).\n\n\nA search database/index is used for full-text search and advanced search queries.\n\n\nThe database's operation log, normally used for synchronization between database nodes, is also used for \n\n\n\n\nevent-driven communication between microservices, and\n\n\nsynchronization between main document database and search index.\n\n\n\n\nThis \"eventing hack\" is expected to be replaced by a proper eventing layer for productive deployments.\n\n\n\n5.2.7.6 Blackbox Ephemeral file storage\n\n\nAfter loading from external sources and during creation of ERC, the files are stored in a file storage shared between the \u00b5services.\nThe file structure is known to each microservice and read/write operations happen as needed.\n\n\n5.3 Refinement Level 3\n\n\n5.3.1 Whitebox \u00b5services\n\n\nEach microservice is encapsulated as a \nDocker\n container running at its own port on an internal network and only serving its respective API path.\nFor testing or developing the \no2r-platform\n GitHub project contains \ndocker-compose\n configurations to run all microservices, see the repository's directory \n/test\n and check the projects \nREADME.md\n for instructions.\n\n\nERC creation and examination\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmuncher\n\n\n/api/v1/compendium\n and \n/api/v1/job\n\n\nJavaScript (Node.js)\n\n\ncore component for CRUD of compendia and jobs (ERC execution)\n\n\n\n\n\n\nloader\n\n\n/api/v1/compendium\n (\nHTTP POST\n only)\n\n\nJavaScript (Node.js)\n\n\nload workspaces from repositories and collaboration platforms\n\n\n\n\n\n\nfinder\n\n\n/api/v1/search\n\n\nJavaScript (Node.js)\n\n\ndiscovery and search, synchronizes the database with a search database (Elasticsearch) and exposes read-only search endpoints\n\n\n\n\n\n\ntransporter\n\n\n~ /data/\n and \n~* \\.(zip|tar|tar.gz)\n\n\nJavaScript (Node.js)\n\n\ndownloads of compendia in zip or (gzipped) tar formats\n\n\n\n\n\n\ninformer\n\n\n~* \\.io\n\n\nJavaScript (Node.js)\n\n\nsocket.io\n-based WebSockets for live updates to the UI based on database event log, e.g. job progress\n\n\n\n\n\n\nsubstituter\n\n\n/api/v1/substitution\n\n\nJavaScript (Node.js)\n\n\ncreate new ERCs based on existing ones by substituting files\n\n\n\n\n\n\nmanipulater\n\n\nunder development\n\n\n--\n\n\nprovide back-end containers for interactive ERCs\n\n\n\n\n\n\ninspecter\n\n\nunder development\n\n\n--\n\n\nallow inspection of non-text-based file formats, e.g. \n.Rdata\n\n\n\n\n\n\n\n\nERC exporting\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nshipper\n\n\n/api/v1/shipment\n\n\nPython\n\n\nship ERCs, including packaging, and their metadata to third party repositories and archives\n\n\n\n\n\n\n\n\nAuthentication\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbouncer\n\n\n/api/v1/auth\n, \n/api/v1/user/\n\n\nJavaScript (Node.js)\n\n\nauthentication service and user management (whoami, level changing)\n\n\n\n\n\n\n\n\n5.3.2 Whitebox database\n\n\nTwo databases are used.\n\n\nMongoDB\n document database\n with enabled \nreplica-set oplog\n for eventing.\n\n\nCollections:\n\n\n\n\nusers\n\n\nsessions\n\n\ncompendia\n\n\njobs\n\n\nshipments\n\n\n\n\nThe MongoDB API is used by connecting \u00b5services via suitable client packages, which are available for all required languages.\n\n\nElasticsearch\n search index\n, kept in sync with the main document database by the \u00b5service \nfinder\n.\nThe ids are mapped to support update and delete operations.\n\n\nIndex: \no2r\n\n\nTypes:\n\n\n\n\ncompendia\n\n\njobs\n\n\n\n\nThe search index is accessed by the UI through the search endpoint provided by \nfinder\n.\n\n\n5.3.3 Whitebox tools\n\n\n\n\n\n\n\n\nproject\n\n\nlanguage\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nmeta\n\n\nPython\n\n\nscripts for extraction, translation and validation of metadata\n\n\n\n\n\n\ncontainerit\n\n\nR\n\n\ngeneration of Dockerfiles based on R sessions and scripts\n\n\n\n\n\n\n\n\n5.3.4 Whitebox ephemeral file storage\n\n\nA host directory is mounted into every container to the same location: \n/tmp/o2r:/tmp/o2r\n\n\n\n\nCredits\n\n\nThis specification and guides are developed by the members of the project Opening Reproducible Research (\nOffene Reproduzierbare Forschung\n) funded by the German Research Foundation (Deutsche Forschungsgemeinschaft (DFG) - Projektnummer \n274927273\n) under grant numbers PE 1632/10-1, KR 3930/3-1, and TR 864/6-1).\n\n\n\n\n\n\nOpening Reproducible Research (o2r, \nhttp://o2r.info/about\n) is a DFG-funded research project by Institute for Geoinformatics (\nifgi\n) and University and Regional Library (\nULB\n), University of M\u00fcnster, Germany. Building on recent advances in mainstream IT, o2r envisions a new architecture for storing, executing and interacting with the original analysis environment alongside the corresponding research data and manuscript. This architecture evolves around so called Executable Research Compendia (ERC) as the container for both research, review, and archival.\n\n\n\n\n\n\nLicense\n\n\n\n\nThe o2r architecture specification is licensed under \nCreative Commons CC0 1.0 Universal License\n, see file \nLICENSE\n.\nTo the extent possible under law, the people who associated CC0 with this work have waived all copyright and related or neighboring rights to this work.\nThis work is published from: Germany.\n\n\n\n\nAbout arc42\n\n\narc42, the Template for documentation of software and system architecture.\n\n\nBy Dr. Gernot Starke, Dr. Peter Hruschka and contributors.\n\n\nTemplate Revision: 7.0 EN (based on asciidoc), January 2017\n\n\n\u00a9 We acknowledge that this document uses material from the arc 42 architecture template, \nhttp://www.arc42.de\n. Created by Dr. Peter Hruschka & Dr. Gernot Starke\n\n\nBuild @@VERSION@@ @ @@TIMESTAMP@@",
            "title": "Architecture"
        },
        {
            "location": "/#software-architecture-for-opening-reproducible-research",
            "text": "",
            "title": "Software architecture for Opening Reproducible Research"
        },
        {
            "location": "/#1-introduction-and-goals",
            "text": "",
            "title": "1. Introduction and Goals"
        },
        {
            "location": "/#11-requirements-overview",
            "text": "The system must provide a reliable way to create and examine (e.g. discover, inspect) reproducibility packages of computational research to support reproducible publications.\nThis architecture describes the relationship of the reproducibility service with other services from the context of scientific collaboration and publishing and how they can be combined to a new system for scholarly publications.  At its core is the concept of the  Executable Research Compendium  (ERC, see  specification  and  article ), and a supporting reproducibility service, which is defined by a  web  API  specification  and its  reference implementation  is published as open source software.",
            "title": "1.1 Requirements Overview"
        },
        {
            "location": "/#12-quality-goals",
            "text": "Transparency  The system must be transparent to allow scrutiny required by a rigorous scientific process.  Separation of concern  The system must integrate with existing services and focus on the core functionality.\nIt must not replicate existing functionality such as storage or persistent identification.  Flexibility & modularity  In regard to the research project setting, the system components must be well separated, so that functions can be developed independently, e.g. using different programming languages.\nThis allows different developers to contribute efficiently.",
            "title": "1.2 Quality Goals"
        },
        {
            "location": "/#13-stakeholders",
            "text": "Role/Name  Goal/point of contact  Required interaction      Author (scientist)  publish ERC as part of a scientific publication process  -    Reviewer (scientist)  examine ERC during a review process  -    Co-author (scientist)  contribute to ERC during research (e.g. cloud based)  -    Reader (scientist)  view and interact with ERC on a journal website  -    Publisher  increase quality of publications in journals with ERC  -    Curator/preservationist  ensure research is complete and archivable using ERC  -    Operator  provide infrastructure to researchers at my university to collaborate and conduct high-quality research using ERC  -    Developer  use and extend the tools around ERC  -     Some of the stakeholders are accompanied by  user scenarios  in prose.",
            "title": "1.3 Stakeholders"
        },
        {
            "location": "/#2-architecture-constraints",
            "text": "The few constraints on this project are reflected in the final solution. This section shows them and if applicable, their motivation. (based on  biking2 )",
            "title": "2. Architecture constraints"
        },
        {
            "location": "/#21-technical-constraints",
            "text": "Constraint  Background and/or motivation      TECH.1  Only open licenses  All third party software or used data must be available under a suitable code license, i.e. either  OSI-approved  or  ODC license .    TECH.2  OS independent development and deployment  Server applications must run in well defined  Docker  containers to allow installation on any host system and to not limit developers to a specific language or environment.    TECH.3  Do not store secure information  The team members experience and available resources do not allow for handling information with security concerns, so no critical data, such as user passwords but also data with privacy concerns, must be stored in the system.",
            "title": "2.1 Technical constraints"
        },
        {
            "location": "/#22-organizational-constraints",
            "text": "Constraint  Background and/or motivation      ORG.1  Team and schedule  http://o2r.info/about    ORG.2  Do not interfere with existing well-established peer-review process  This software is  not  going to change how scientific publishing works, nor should it. While intentioned to support public peer-reviews, open science etc., the software should be agnostic of these aspects.    ORG.3  Only open licenses  All created software must be available under an  OSI-approved  license, documentation and specification under a  CC license .    ORG.4  Version control/management  Code must be versioned using  git  and published on  GitHub .    ORG.5  Transfer from group domain to persistent domain  The ERC bundles artifacts coming from a private or group domain for a transfer to a public and persistent domain (cf.  Curation Domain Model ), which imposes requirements on the availability and enrichment of metadata",
            "title": "2.2 Organizational constraints"
        },
        {
            "location": "/#23-conventions",
            "text": "Constraint  Background and/or motivation      CONV.1  Provide architecture documentation  Based on  arc42  (template version 7.0).    CONV.2  Reasonably follow coding conventions  Typical project layout and coding conventions of the respective used language should be followed as far as possible. However, we explicitly accept the research project context and do  not  provide full tests suites or documentation beyond what is needed by project team members.    CONV.3  Documentation is English  International research project, must be understandable by anyone interested.    CONV.4  Use subjectivization for server component names  Server-side components are named using personalized verbs or professions:  muncher ,  loader ,  transporter . All git repositories for software use an  o2r-  prefix, in case of server-side components e.g.  o2r-shipper .    CONV.5  Configuration using environment variables  Server-side components must be configurable using all caps environment variables prefixed with the component name, e.g.  SHIPPER_THE_SETTING , for required settings. Other settings should be put in a settings file suitable for the used language, e.g.  config.js  or  config.yml .",
            "title": "2.3 Conventions"
        },
        {
            "location": "/#3-system-scope-and-context",
            "text": "",
            "title": "3. System scope and context"
        },
        {
            "location": "/#31-business-context",
            "text": "Communication partner  Exchanged data  Technology/protocol      Reproducibility service , e.g.  o2r  publication platforms utilize creation and examination services for ERC, reproducibility service uses repositories to retrieve software artifacts, store runtime environment images, and save complete ERC  HTTP  APIs    Publication platform , e.g. online journal website and review system  users access ERC status and metadata via search results and paper landing pages; review process integrates ERC details and supports manipulation;  system's API using  HTTP  with  JSON  payload    ID provider  retrieve unique user IDs, user metadata, and authentication tokens; user must log in with the provider  HTTP    Execution infrastructure  ERC can be executed using a shared/distributed infrastructure  HTTP    Data repository  the reproducibility service fetches (a) content for ERC creation, or (b) complete ERC, from different sources and stores crated ERC persistently at suitable repositories  HTTP ,  FTP ,  WebDAV ,  git    Registry  the reproducibility service can deliver metadata on published ERC to registries/catalogues/search portals, but also retrieve/harvest contextual metadata during ERC creation; users discover ERC via registries  (proprietary)  HTTP  APIs, persistent identifiers ( DOI ),  OAI-PMH    Software repository  software repository provide software artifacts during ERC creation and store executable runtime environments  HTTP  APIs    Archives and digital preservation systems  when information is transferred from the private (research group, single researcher) or group domain (collaborations) to a public and persistent domain (archives, repositories), then extended data and metadata management is needed but also different access and re-use is enabled; these concerns are only relevant in so far as  data repositories  must be supported, but further aspects such as access rights in archives are only mediately relevant for the reproducibility service  metadata in  JSON  and  XML  provided via  HTTP  or as files",
            "title": "3.1 Business context"
        },
        {
            "location": "/#32-technical-context",
            "text": "All components use  HTTP  over cable networks connections for communication of all exchanged data (metadata documents, ERC, Linux containers, etc.).",
            "title": "3.2 Technical context"
        },
        {
            "location": "/#4-solution-strategy",
            "text": "This section provides a short overview of architecture decisions and for some the reasoning behind them.",
            "title": "4. Solution strategy"
        },
        {
            "location": "/#web-api",
            "text": "The developed solution is set in an existing system of services, and first and foremost must integrate well with these systems, focussing on the specific missing features of building and running ERCs.\nThese features are provided via a  well-defined RESTful API .",
            "title": "Web API"
        },
        {
            "location": "/#microservices",
            "text": "To allow a dynamic development and support the large variety of skills, all server-side features are developed in independent  microservices .\nThese microservices handle only specific functional parts of the API and allow independent development and deployment cycles.\nCore components are developed using server-side JavaScript based on Node.js while other components are implemented Python.  We accept that this diversification  increases complexity  of both development and testing environments and the deployment of said services.  Required documentation is minimal. The typical structure should follow common practices.",
            "title": "Microservices"
        },
        {
            "location": "/#storage-and-intra-service-communication",
            "text": "In accordance with the system scope, there is no reliable storage solution implemented.\nThe microservices simply share a common pointer to a local file system path.\nStorage of ERC is only implemented to make the solution independent during development and for the needs of core functionality (temporal storage), but it is not a feature the solution will eventually provide.  The unifying component of the architecture is the  database .\nIt is known to all microservices.  Some microservices communicate via an eventing mechanism for real-time updates, such as the search database and the component providing live updates to the user via WebSockets-\nThe eventing is based on the operation log of the database (which is normally used to synchronise database nodes).\nThis is a clear  misuse of an internal feature , but a lot simpler than maintaining a full-blown eventing solution.",
            "title": "Storage and intra-service communication"
        },
        {
            "location": "/#demonstration-user-data-authentication",
            "text": "To be able to demonstrate the system, a  browser-based client application  is developed.\nIt uses the RESTful API to control the system. OAuth 2.0  is used for authentication and minimal information, which is already public, is stored for each user.\nThis information is shared between all services that require authentication via the database.  The client application manages the control flow  of all user interactions.",
            "title": "Demonstration, user data &amp; authentication"
        },
        {
            "location": "/#tools",
            "text": "If standalone tools are developed, they should provide a command-line interface (CLI) that allows integration into microservices when needed.\nThanks to the container architecture and the controlled, we don't need to worry about documentation for or distribution/packaging of these tools.\nIt must only be ensured they are correctly installed using the microservice's Dockerfile.\nThe only required documentation is for the installation into a container and usage of the CLI.",
            "title": "Tools"
        },
        {
            "location": "/#5-building-block-view",
            "text": "",
            "title": "5. Building block view"
        },
        {
            "location": "/#51-refinement-level-1",
            "text": "",
            "title": "5.1 Refinement Level 1"
        },
        {
            "location": "/#511-blackbox-publication-platforms",
            "text": "Publications platforms are the online interaction points of users with scientific works.\nUsers create publications, e.g. submitting to a scientific journal, publishing on a pre-print server or archive, or collaborating in online repositories.\nUsers examine publications, e.g. browsing, searching, reading, downloading, or reviewing.",
            "title": "5.1.1 Blackbox Publication Platforms"
        },
        {
            "location": "/#512-blackbox-id-provider",
            "text": "Identification information of distributed systems is crucial, and for security reasons as well as for limiting manual reproduction of metadata, a central service can provide all of   unique  identification of users  and  metadata on users ,  authentication  of users, and  metadata on a user's  works , e.g. publications or ERC.   Persistent identifiers for artifacts in the reproducibility service itself are  not required , as these are provided by data storage and registries.\nHowever, services such as  ePIC  could allow to retrieve persistent IDs.",
            "title": "5.1.2 Blackbox ID Provider"
        },
        {
            "location": "/#513-blackbox-execution-infrastructure",
            "text": "The execution infrastructure provides CPU time and temporary result storage space for execution of ERC, both \"as is\" and with manipulation, i.e. changed parameters.",
            "title": "5.1.3 Blackbox Execution Infrastructure"
        },
        {
            "location": "/#514-blackbox-data-repositories",
            "text": "Data repositories are all services storing data but not software.\nMore specifically, they may store software \"as data\", but not with software-specific features such as code versioning or installation binaries for different computer architectures.\nData repositories may be self-hosted or public/free, domain-specific or generic.\nThey typically provide persistent identifiers or handles, e.g. a  DOI  or  URN .\nThey are used both for loading created ERC and for storing the ERC created by the reproducibility service.",
            "title": "5.1.4 Blackbox Data Repositories"
        },
        {
            "location": "/#515-blackbox-registries",
            "text": "Registries are metadata indexes or catalogues.  They are recipients of metadata exports by the reproducibility service to share information about ERC, e.g. add a new ERC to an author's profile.\nThis requires the reproducibility services to translate the internal metadata model into the recipients data model and encoding.   They are sources of metadata during ERC creation when the information in the fetched content is used to query registries for additional information which can be offered to the user.",
            "title": "5.1.5 Blackbox Registries"
        },
        {
            "location": "/#516-blackbox-software-repositories",
            "text": "Software repositories are a source and a sink for software at different abstraction levels.\nThey are a source for software artifacts or packages, such as system packages in install a library or language-specific extension packages.\nThey are a sink for executable images of software, which comprise a number of software artifacts, for a specific ERC instance.",
            "title": "5.1.6 Blackbox Software Repositories"
        },
        {
            "location": "/#52-refinement-level-2",
            "text": "",
            "title": "5.2 Refinement Level 2"
        },
        {
            "location": "/#521-whitebox-publication-platforms",
            "text": "Publication platforms can be roughly divided into two groups.\nThey can be either specific journals hosted independently, such as  JStatSoft  or  JOSS , or a larger platform provided by a publisher to multiple journals, such as  ScienceDirect ,  MDPI ,  SpringerLink , or  PLOS .\nTo some extend, pre-print servers, for example  OSF  or  arXiv.org , can also fall into the latter category.  Integration with the reproducibility service can happen via plug-ins to generic software, e.g.  OJS , or by bespoke extensions.\nIntegrations are based on the service's public API.",
            "title": "5.2.1 Whitebox Publication Platforms"
        },
        {
            "location": "/#522-whitebox-id-provider",
            "text": "The reproducibility service uses  ORCID  to authenticate users and retrieve user and works metadata.\nInternally, the user's public  ORCID  is the main identifier.",
            "title": "5.2.2 Whitebox ID Provider"
        },
        {
            "location": "/#523-whitebox-execution-infrastructure",
            "text": "Such an infrastructure could be either self-hosted, e.g.  Docker Swarm -based, or use a cloud service provide, such as  Amazon EC2 ,  Docker Cloud , or even use continuous integration services such as  Travis CI  or  Gitlab CI .",
            "title": "5.2.3 Whitebox Execution Infrastructure"
        },
        {
            "location": "/#524-whitebox-data-repositories",
            "text": "Collaboration platforms , e.g.  ownCloud/Sciebo ,  GitHub ,  ShareLatex ,  OSF , allow users to create, store, and share their research (code, text, data, et cetera).\nThe reproducibility service fetches contents for building an ERC from them based on public links, e.g. a public GitHub repository or shared Sciebo folder.\nIt is possible that ERC creation is linked to a project/repository on a collaboration platform and updates trigger an ERC (re-)creation or execution.  Protocols:  WebDAV ,  ownCloud ,  HTTP  (including  webhooks ),  git  Domain data repositories , e.g.  PANGAEA  or  GFZ Data Services , can be accessed by the reproducibility service during creation and execution of ERC to download data.\nAllowing access to data repositories reduces data duplication but requires control over/trust in the respective repository.  Protocol:  HTTP  APIs  Generic  Repositories , e.g.  Zenodo ,  Mendeley Data ,  Figshare ,  OSF , provide (a) access to complete ERC stored in repositories for inspection and execution by the reproducibility service, and (b) storage of created ERC. repositories.  The reproducibility service  does not persistently store anything .  Protocols: (authenticated)  HTTP  APIs  Archives , e.g. using an installation of  Archivematica , might provide long-term preservation of ERC. Preservation lies in the responsibility of the repository, which might save the hosted content to an archive, or an archive harvests a repository.  Protocol:  HTTP  carrying bitstreams and metadata",
            "title": "5.2.4 Whitebox Data Repositories"
        },
        {
            "location": "/#525-whitebox-registries",
            "text": "Research data registries and websites, for example ( CRIS ,  DataCite ,  Google Scholar ,  Scopus ,  Altmetric , to name just a few, collect metadata on publications and provide services with this data.\nServices comprise discovery but also derivation of citation data and creating networks of researchers and publications.  The listed examples include open platforms, commercial solutions, and institution-specific platforms.\nSome of the registries offer a public, well-defined API to retrieve structured metadata and to create new records.  Protocol:  HTTP  APIs",
            "title": "5.2.5 Whitebox Registries"
        },
        {
            "location": "/#526-whitebox-software-repositories",
            "text": "",
            "title": "5.2.6 Whitebox Software Repositories"
        },
        {
            "location": "/#5261-blackbox-package-repositories",
            "text": "Package repositories are used during ERC creation to download and install software artifacts for specific operating systems, e.g.  Debian APT  or  Ubuntu Launchpad , for specific programming languages or environments, e.g.  CRAN , or from source, e.g.  GitHub .",
            "title": "5.2.6.1 Blackbox Package repositories"
        },
        {
            "location": "/#5262-blackbox-container-registries",
            "text": "Container registries such as  Docker Hub ,  Quay , self-hosted  Docker Registry 2.0  or  Amazon ERC , store executable images of runtime environments.\nThey can be used to distribute the runtime environments across the execution infrastructure and provide an intermediate ephemeral storage for the reproducibility service.",
            "title": "5.2.6.2 Blackbox Container registries"
        },
        {
            "location": "/#527-whitebox-reproducibility-service",
            "text": "",
            "title": "5.2.7 Whitebox Reproducibility Service"
        },
        {
            "location": "/#5271-blackbox-webserver",
            "text": "A webserver handles all incoming calls to the API and distributes them to the respective microservice.\nA working  nginx  configuration is available  in the test setup .",
            "title": "5.2.7.1 Blackbox Webserver"
        },
        {
            "location": "/#5272-blackbox-ui",
            "text": "The UI is a web application based on  Angular JS , see  o2r-platform .\nIt connects to an execution microservice (\u00b5service) for real-time WebSocket-based notifications.",
            "title": "5.2.7.2 Blackbox UI"
        },
        {
            "location": "/#5273-blackbox-microservices",
            "text": "The reproducibility service uses a  microservice architecture  to separate functionality defined by the  web API specification  into manageable units.  This allows scalability (selected \u00b5services can be deployed as much as needed) and technology independence for each use case and developer.\nThe \u00b5services all access one main database and a shared file storage.",
            "title": "5.2.7.3 Blackbox microservices"
        },
        {
            "location": "/#5274-blackbox-tools",
            "text": "Some functionality is developed as standalone tools and used as such in the \u00b5services instead of re-implementing features.\nThese tools are integrated via their command line interface (CLI).",
            "title": "5.2.7.4 Blackbox Tools"
        },
        {
            "location": "/#5275-blackbox-database",
            "text": "The main database is the unifying element of the microservice architecture.\nAll information shared between \u00b5services or transactions between microservices are made via the database, including session state handling (= authentication).  A search database/index is used for full-text search and advanced search queries.  The database's operation log, normally used for synchronization between database nodes, is also used for    event-driven communication between microservices, and  synchronization between main document database and search index.   This \"eventing hack\" is expected to be replaced by a proper eventing layer for productive deployments.",
            "title": "5.2.7.5 Blackbox Database"
        },
        {
            "location": "/#5276-blackbox-ephemeral-file-storage",
            "text": "After loading from external sources and during creation of ERC, the files are stored in a file storage shared between the \u00b5services.\nThe file structure is known to each microservice and read/write operations happen as needed.",
            "title": "5.2.7.6 Blackbox Ephemeral file storage"
        },
        {
            "location": "/#53-refinement-level-3",
            "text": "",
            "title": "5.3 Refinement Level 3"
        },
        {
            "location": "/#531-whitebox-services",
            "text": "Each microservice is encapsulated as a  Docker  container running at its own port on an internal network and only serving its respective API path.\nFor testing or developing the  o2r-platform  GitHub project contains  docker-compose  configurations to run all microservices, see the repository's directory  /test  and check the projects  README.md  for instructions.",
            "title": "5.3.1 Whitebox \u00b5services"
        },
        {
            "location": "/#erc-creation-and-examination",
            "text": "Project  API path  Language  Description      muncher  /api/v1/compendium  and  /api/v1/job  JavaScript (Node.js)  core component for CRUD of compendia and jobs (ERC execution)    loader  /api/v1/compendium  ( HTTP POST  only)  JavaScript (Node.js)  load workspaces from repositories and collaboration platforms    finder  /api/v1/search  JavaScript (Node.js)  discovery and search, synchronizes the database with a search database (Elasticsearch) and exposes read-only search endpoints    transporter  ~ /data/  and  ~* \\.(zip|tar|tar.gz)  JavaScript (Node.js)  downloads of compendia in zip or (gzipped) tar formats    informer  ~* \\.io  JavaScript (Node.js)  socket.io -based WebSockets for live updates to the UI based on database event log, e.g. job progress    substituter  /api/v1/substitution  JavaScript (Node.js)  create new ERCs based on existing ones by substituting files    manipulater  under development  --  provide back-end containers for interactive ERCs    inspecter  under development  --  allow inspection of non-text-based file formats, e.g.  .Rdata",
            "title": "ERC creation and examination"
        },
        {
            "location": "/#erc-exporting",
            "text": "Project  API path  Language  Description      shipper  /api/v1/shipment  Python  ship ERCs, including packaging, and their metadata to third party repositories and archives",
            "title": "ERC exporting"
        },
        {
            "location": "/#authentication",
            "text": "Project  API path  Language  Description      bouncer  /api/v1/auth ,  /api/v1/user/  JavaScript (Node.js)  authentication service and user management (whoami, level changing)",
            "title": "Authentication"
        },
        {
            "location": "/#532-whitebox-database",
            "text": "Two databases are used.  MongoDB  document database  with enabled  replica-set oplog  for eventing.  Collections:   users  sessions  compendia  jobs  shipments   The MongoDB API is used by connecting \u00b5services via suitable client packages, which are available for all required languages.  Elasticsearch  search index , kept in sync with the main document database by the \u00b5service  finder .\nThe ids are mapped to support update and delete operations.  Index:  o2r  Types:   compendia  jobs   The search index is accessed by the UI through the search endpoint provided by  finder .",
            "title": "5.3.2 Whitebox database"
        },
        {
            "location": "/#533-whitebox-tools",
            "text": "project  language  description      meta  Python  scripts for extraction, translation and validation of metadata    containerit  R  generation of Dockerfiles based on R sessions and scripts",
            "title": "5.3.3 Whitebox tools"
        },
        {
            "location": "/#534-whitebox-ephemeral-file-storage",
            "text": "A host directory is mounted into every container to the same location:  /tmp/o2r:/tmp/o2r",
            "title": "5.3.4 Whitebox ephemeral file storage"
        },
        {
            "location": "/#credits",
            "text": "This specification and guides are developed by the members of the project Opening Reproducible Research ( Offene Reproduzierbare Forschung ) funded by the German Research Foundation (Deutsche Forschungsgemeinschaft (DFG) - Projektnummer  274927273 ) under grant numbers PE 1632/10-1, KR 3930/3-1, and TR 864/6-1).    Opening Reproducible Research (o2r,  http://o2r.info/about ) is a DFG-funded research project by Institute for Geoinformatics ( ifgi ) and University and Regional Library ( ULB ), University of M\u00fcnster, Germany. Building on recent advances in mainstream IT, o2r envisions a new architecture for storing, executing and interacting with the original analysis environment alongside the corresponding research data and manuscript. This architecture evolves around so called Executable Research Compendia (ERC) as the container for both research, review, and archival.",
            "title": "Credits"
        },
        {
            "location": "/#license",
            "text": "The o2r architecture specification is licensed under  Creative Commons CC0 1.0 Universal License , see file  LICENSE .\nTo the extent possible under law, the people who associated CC0 with this work have waived all copyright and related or neighboring rights to this work.\nThis work is published from: Germany.",
            "title": "License"
        },
        {
            "location": "/#about-arc42",
            "text": "arc42, the Template for documentation of software and system architecture.  By Dr. Gernot Starke, Dr. Peter Hruschka and contributors.  Template Revision: 7.0 EN (based on asciidoc), January 2017  \u00a9 We acknowledge that this document uses material from the arc 42 architecture template,  http://www.arc42.de . Created by Dr. Peter Hruschka & Dr. Gernot Starke  Build @@VERSION@@ @ @@TIMESTAMP@@ < /div",
            "title": "About arc42"
        },
        {
            "location": "/user-scenarios/",
            "text": "User scenarios\n\n\nAndrea the author and reader\n\n\nAndrea turned 29 this year. She is always up for a joke and a pot of coffee but is also quite impatient. Especially if she has to wait for others or if she hasn\u2019t had any progress for a while. However, currently Andrea does his Ph.D in the field of geosciences. Two years ago she decided to go for a cumulative dissertation, meaning that she publishes scientific papers throughout his graduation and summarizes them at the end. She already published his first paper a few months ago which is good, actually. One of the reviewers was interested in the data and the source-code in order to reproduce the results.\n\n\nAfter a few hours of searching (remember that she is not one of the most patient guys), she finally finds some files that include the dataset and also the source-code in R (a statistics program). Just a short try if it still working... weird, the results are different. Just a short look into the paper... The configuration is different than the one described in the method section. Well, just few manipulations and - still not working. \nAlthough she submitted the paper just a few months ago, she can\u2019t remember the configuration that lead to the results in the paper. Fortunately, submitting data and code was not mandatory. But Andrea knows that she made a mistake.\n\n\nMore and more journals expect their authors to submit data and source-code that underlie the research findings. For this reason, she wants to change her working behavior and to keep data and code files better under control. She remembers her last research work which was quite unstructured, maybe already messy. Code and data was distributed over several folders and even computers. She had so search for them for quite a while. Moreover, some components do not work anymore.\n\n\nThis time, she wants to do it better and searches for a great tool that assists his working flow. She just heard about a new website supporting reproducible research. It allows to upload all necessary files and to create a so called \"container\" which is \"executable\" - whatever that means. It even verifies the results in the paper making it possible to detect errors immediately. Of course it also contains common features like sharing the publication with other authors. On top of that, Andrea can also benefit from other publications. As the website automatically generates a number of meta information, new search capabilities arise. It is not only possible to search for other publications by using keywords, but also by using spatial and temporal properties and constraints. It is even possible to constrain the search to hypotheses and research questions that contain a certain vocabulary thus simplifying search for related work. Andrea is quite impressed about that. She easily finds related papers that suit to his own work. She gets a good overview about existing research questions making it easier to identify a research gap he can focus on. Andrea doesn\u2019t even have to implement all the code lines for his statistical analysis from scratch, but can build upon existing. While reading some of the related papers in the browser, he realizes a couple of user interface widgets besides the incorporated figures. He doesn\u2019t know them from traditional, static papers which are typically published as .pdf-files. Andrea recognizes that the widgets allow to interact with the diagrams to which the widgets belong. They allow to change, for example, thresholds, input variables and constants. She is thus able to check the assumptions and conclusions underlying the paper. She is a bit overwhelmed by the number of new features that might assist his current research such as exchanging the dataset or the source-code underlying the paper. Andrea is quite happy about his new tool. It provides support for structured work, finding related publications, algorithms and datasets, identifying a research gap, and even tools for interacting with traditional, static papers. So, let\u2019s go for the second paper.\n\n\nArthur the administrator\n\n\nArthur works as a system administrator in a large university library in Germany. He's quite happy with his job. After working as freelance software developer for over 20 years, he now enjoys the challenge to make all the different servers and applications under his care work like a charm 24/7 while having a stable paycheck and reasonable working hours. He is particularly proud that, since he took over the job, he successfully migrated all services to a private cloud infrastructure and enabled https-only traffic on all, event the internal, APIs and websites. Since then, there has been minimal overtime for him and close to 0 minutes downtime for the services... and a raise!\n\n\nArthur is interested in this new reproducibility service that the head of the library is interested in, but he is sceptical about all new systems. There are going to be bugs, unforseen problems, and a lot of testing \"\"in production\"\", which he does not like. But he does now that scientists have been in touch with the library before about archiving data and software, so if this is a high priority for his customers, as he sees them, there is no ways around it.\n\n\nAt second look though, he realizes the project seems to have all the basics straight for a stable and scalable deployment: All components are published under open source licenses, and the project maintainers provide different ready-to-use Docker images. Arthur worries about security, so there is no better way to make sure things work well than source code access. The project is written in a language he has not used before himself, but he can actually build the project himself from source with the provided instructions. He also sees that, instead of reinventing the wheel, the developers seem to be competent enough to build upon established libraries.\n\n\nThe Dockerfiles are great to play around with, but also easy to integrate in his own server management solution. He also likes that all the HTTP APIs as well as setup and configuration seem to be very well documented. This should make it easy to integrate the new solution with some custom tools he developed, but also with some legacy infrastructure he has not yet been able to get rid off. He does worry a bit about the scheduling solution, since he is not very keen on Docker containers being started automatically on his servers. Good thing that the project contributors seem to operate a public chat, and professional support is also available at reasonable prices.\n\n\nAfter some testing, he feels good to tell his colleagues: looks good to me, let's try this out!\n\n\nOlivia the operator\n\n\n\"Olivia is the chancellor of a mid-size state owned university in the US. She is proud to have been elected to this position a few years ago, and works very hard each day to improve both the university's reputation and the working and learning conditions of her employees and students. She had to make some unpleasant first hand experiences with aspects of todays academic life that sadly became almost normal, such as budget cuts, violence on campus, and plagiarism scandals. During all of these upsets, she is happy that the never wavered on the importance of personal integrity and credibility of each and every one of the scientists and researchers working on her campus.\n\n\nTo gain some ground in the competition with other universities, Olivia puts her best assistant on the job of finding the newest trends in academia. Soon enough she presents to her the idea of making all research conducted at the university reproducible. Olivia is first surprised by the fact, being an arts major herself, as she thought that is already the case. She starts reading the material provided to hear and realizes that science, and especially something called computational science, is very much different from the practical work she has encountered during her years as a researcher. It also becomes clear to her that it won't work to just put out a statement forcing every lab to spend enormous efforts on changing established research practices, or to re-do what has been done 5, 10 or 20 years ago. The huge variety of labs and workflows that exists and all the different kinds of people... getting out the stick simply won't work. But maybe the carrot will?\n\n\nShe discovers a novel website that promises to solve all the problems of reproducibility. The people behind it seem competent enough to her, but again she asks her assistant to consult with experts from the university library and computer science departments to see what they think. A lengthy discussion starts, and there seems to be no consensus after months of meetings and evaluations. The assistant doesn't know what to report back to Olivia. Eventually, Olivia is tired of waiting and joins a few of the meetings of the expert group as an observer. She realizes that nothing comes for free... she encourages the expert group to create a list of requirements on establishing a reproducibility website for the university. She quickly gets that, as the lecturers and staff in the group realize that this means they won't just get more work to do, they might get the proper time and money to do it!\n\n\nOlivia makes the new website a matter for the boss. She successfully acquires the funds to start and maintain both the technical services and to hire support staff to maintain it. Beyond that, the supports staff is even equipped to provided consultancy services to all researchers at the university. These services quickly become popular across all disciplines working with data and code, and after just a few months, more and more fully reproducible papers appear on the public reproducibility website. Olivia is very glad to see that the changes she introduced did not have an impact on the scientific output of the university - the monthly statistics tell her that much. Is the quality or quantity of the output going to increase? It's too soon to tell, but Olivia is sure it will. Just last week, the head of the programme reported to her that now ten papers are available on the website for which researchers from different university departments collaborated, who never collaborated before - they discovered the overlap through the new system! More than 20 undergraduate courses teaching scientific methods incorporated material from the website into their course schedule, and 50% of the graduate theses from the computer science department are now using the university reproducibility tools. Those are good enough signs for Olivia. She decides to pitch an idea to the university board: let's include reproducibility of publications as an evaluation factor for the budget allocations next year. You got to use the stick from time to time to make people appreciate the carrot.\n\n\nCarl the curator\n\n\nCarl works as a digital assets curator at a university library in Germany. He has been working as a librarian for about ten years and experienced the digital transformation of the field, which is why he specialized in the area of digital curation and archiving. He is qualified to manage and organize several collections of digital objects at a given time and recently selected objects for an exhibition of gold standard open access publications in the software category of his institutions catalog front page.\n\n\nCarl\u2019s expertise encompasses the management of accessibility levels as well as the preservation of file integrity and meta data curation. Since he discovered a growing interest in the preservation of software, he realized that reproducibility of research findings, including code and data increases the value and visibility of his university\u2019s portfolio. As a result, Carl is working closely with the library\u2019s team for Research Data Management, in order to facilitate integration of reproducible computational environments into the digital objects\u2019 life cycle that underlies their current policies.\n\n\nAs he strongly believes that publicly funded research data are public goods, Carl values his profession as a vital point of intersection between researchers, librarians und the general public. Therefore, when planning a selection of digital assets or curating the library\u2019s catalogs, Carl enjoys the interoperability that is provided by international metadata standards and linked open data vocabularies.\n\n\nPolly the publisher\n\n\nPolly is the head of a large publishing firm for scientific journals. She grew up being part of a publisher family, the third of four kids. While her older brothers wrestled with the family legacy, she has always been close to her late grandfather, who started the publishing business as a young man. So it came as no surprise that she studied arts and library science and after a few well planned career steps around the globe, she joined the family business as assistant of her father and became CEO after a few years, a decision she rarely regrets.\n\n\nThough there is one thing that makes her job challenging every day: technology's high development speed. For a large publishing business, it is hard to keep up with new and modern technology. She has to serve both old (in more than one way) customers and employers, who have had a long relationship and a work environment and processes that have developed and settled in over many years. On the other hand, she sees new ideas by entrepreneurs and startups almost every week, some crazy and some rightfully called revolutionary, who experiment with new ways to publish science without the baggage of a reputation and hundreds of journals and an order of magnitude more employees.\n\n\nSo what should Polly do? Scramble up some money to acquire a few startups and replace the existing review and authoring solution? Fire all staff members who are too slow adopting the new technologies? Close journals with an excellent reputation because editors and reviewers are not tech-savvy? \n\n\nObviously, none of these were an option. Change had to come gradually and inclusively, not in a disruptive fashion. Polly turned to her CTO Charlotte. She joined the company recently and played out to be a very good hire, as she was able to revive the in-house development team with a positive attitude and a few key hires. Charlotte is aware of the challenges and agrees to compile an action plan from her perspective. A few weeks later, she presents the options to Polly and the other board members. She suggests to adopt an open service for interactive publications, which is an integrated solution for hosting and archiving data as well as code that is often part of publications these days. It is open source, but of course that does not come for free. Charlotte suggests a combined approach of experiments by her own staff and external consulting by the original developers of the software. And she quickly mitigates all concerns raised by the other CxOs: the website is customizable, so it will not look like the competitors versions, it is extensible, so the few \"cool features\" that have been developed over the last years will be easy to integrate, and it is compatible with the existing data repository (so no need to replace that beast of a software). This new website would be an option presented to all editors to adopt for their journals. Education of the company's staff would precede this offer to make sure the intended message is spread: don't be left behind, challenge reviewers and authors to improve the quality of the journals and subsequently raise the bar for high quality open science.\n\n\nRichard the reviewer\n\n\nRichard is a successful researcher.\nAfter getting tenure a few years back, he embraces the chance to support students and collaborate with other scientists instead of hunting for the next easy publication to get his name on.\nA big part of his time is taken up by his membership in the editorial boards of two journals and his engagement with several more journals as a reviewer.\n\n\nRichard is \"senior\" in some ways, and he as well as his colleagues know his value lies in experience, not in hunting the latest hot new things.\nTherefore Richard never came around to catch up practically with the latest technologies, and while he has a good understanding of computer science and used to be a very capable programmer, this new stuff the kids are doing is beyond his means.\n\n\nAs the next paper review request lands in his inbox, he skims the abstract and soon thinks \"I will never be able to thoroughly evaluate this work, the code must be too complex to run on my machine\".\nBut the content is so interesting! What a shame.\nHe almost replies with a negative answer and then sees a new link at the bottom of the notification.\nThe publisher must have added a new feature.\nThe link's title is \"Click here to examine and manipulate code and data\".\n\n\nRichard clicks the link.\nHe is taken to a website looking partially similar to the old review system he is used to.\nOne the one side there is the well-known article view where he can read, add highlights and make comments.\nBut on the other side, there is a new menu he enthusiastically explores.\nIt allows him to edit parameters and re-run analysis of the paper!\nWithout even downloading any data or code.\nHe immediately sees the benefits: What a relief for his work, and what a chance to dig deeper into the article and conduct a thorough review.\n\n\nAfter some brief inspections of the article figures and manipulation of some parameters, Richard feels confident he can actually do the review properly.\nHe let's the editor know about his decision and wants to dive right back into the article, but then stops himself.\nFirst, he writes an email to his fellow editors about this new review system for evaluating code and data - they need it for their journal, too.\n\n\nRachel the reader\n\n\nRachel is a second year graduate student in geoinformatics.\nShe's eager to learn and has left all struggles with the technical side of research, and has become a trusted programmer in her group and is seen as an expert in more than one programming language.\n\n\nWhen she starts one of her final courses in advanced geoinformatics, the lecturer sends out a long list of reading material.\n\nHow is she supposed to get through all of it?\n\nNever faltering, she starts reading all the documents...\n\n\nAfter the third article, she is annoyed and underwhelmed by the fancy descriptions and high-level diagrams.\nAlthough they all make sense, she feels like there is more to see and understand than is presented in the article.\n\n\nShe shares her thoughts with her teacher Teresa during the next seminar.\nTeresa can relate to Rachel's frustration and quickly points her to items 8 and 9 on the reading list.\n\"These are different\", she says.\n\n\nRachel gets back to reading.\nThe next articles start out the same as the others, but she soon realizes something is different.\nThe website takes a bit longer to load, and the graphics do not seem like they are compressed images at all.\nShe needs some time to explore the relatively complex navigation, but then is excited to discover that she can read and even download all the code and data that was used to generate the figures.\nEven more, she can interact with the present methods and play around with the algorithms.\nFinally she can immediately test her own understanding, challenge her criticism, and resolve misunderstandings.\n\n\nShe plays around with the articles on the website for a little while and spends a lot longer on trying to understand the bits and pieces.\nEventually she sees a close relation of one aspect of the analysis with the research project she though about doing for her thesis.\nRachel is enthusiastic and directly downloads the whole article with its code to her own laptop to try the code out with her own dataset.",
            "title": "User scenarios"
        },
        {
            "location": "/user-scenarios/#user-scenarios",
            "text": "",
            "title": "User scenarios"
        },
        {
            "location": "/user-scenarios/#andrea-the-author-and-reader",
            "text": "Andrea turned 29 this year. She is always up for a joke and a pot of coffee but is also quite impatient. Especially if she has to wait for others or if she hasn\u2019t had any progress for a while. However, currently Andrea does his Ph.D in the field of geosciences. Two years ago she decided to go for a cumulative dissertation, meaning that she publishes scientific papers throughout his graduation and summarizes them at the end. She already published his first paper a few months ago which is good, actually. One of the reviewers was interested in the data and the source-code in order to reproduce the results.  After a few hours of searching (remember that she is not one of the most patient guys), she finally finds some files that include the dataset and also the source-code in R (a statistics program). Just a short try if it still working... weird, the results are different. Just a short look into the paper... The configuration is different than the one described in the method section. Well, just few manipulations and - still not working. \nAlthough she submitted the paper just a few months ago, she can\u2019t remember the configuration that lead to the results in the paper. Fortunately, submitting data and code was not mandatory. But Andrea knows that she made a mistake.  More and more journals expect their authors to submit data and source-code that underlie the research findings. For this reason, she wants to change her working behavior and to keep data and code files better under control. She remembers her last research work which was quite unstructured, maybe already messy. Code and data was distributed over several folders and even computers. She had so search for them for quite a while. Moreover, some components do not work anymore.  This time, she wants to do it better and searches for a great tool that assists his working flow. She just heard about a new website supporting reproducible research. It allows to upload all necessary files and to create a so called \"container\" which is \"executable\" - whatever that means. It even verifies the results in the paper making it possible to detect errors immediately. Of course it also contains common features like sharing the publication with other authors. On top of that, Andrea can also benefit from other publications. As the website automatically generates a number of meta information, new search capabilities arise. It is not only possible to search for other publications by using keywords, but also by using spatial and temporal properties and constraints. It is even possible to constrain the search to hypotheses and research questions that contain a certain vocabulary thus simplifying search for related work. Andrea is quite impressed about that. She easily finds related papers that suit to his own work. She gets a good overview about existing research questions making it easier to identify a research gap he can focus on. Andrea doesn\u2019t even have to implement all the code lines for his statistical analysis from scratch, but can build upon existing. While reading some of the related papers in the browser, he realizes a couple of user interface widgets besides the incorporated figures. He doesn\u2019t know them from traditional, static papers which are typically published as .pdf-files. Andrea recognizes that the widgets allow to interact with the diagrams to which the widgets belong. They allow to change, for example, thresholds, input variables and constants. She is thus able to check the assumptions and conclusions underlying the paper. She is a bit overwhelmed by the number of new features that might assist his current research such as exchanging the dataset or the source-code underlying the paper. Andrea is quite happy about his new tool. It provides support for structured work, finding related publications, algorithms and datasets, identifying a research gap, and even tools for interacting with traditional, static papers. So, let\u2019s go for the second paper.",
            "title": "Andrea the author and reader"
        },
        {
            "location": "/user-scenarios/#arthur-the-administrator",
            "text": "Arthur works as a system administrator in a large university library in Germany. He's quite happy with his job. After working as freelance software developer for over 20 years, he now enjoys the challenge to make all the different servers and applications under his care work like a charm 24/7 while having a stable paycheck and reasonable working hours. He is particularly proud that, since he took over the job, he successfully migrated all services to a private cloud infrastructure and enabled https-only traffic on all, event the internal, APIs and websites. Since then, there has been minimal overtime for him and close to 0 minutes downtime for the services... and a raise!  Arthur is interested in this new reproducibility service that the head of the library is interested in, but he is sceptical about all new systems. There are going to be bugs, unforseen problems, and a lot of testing \"\"in production\"\", which he does not like. But he does now that scientists have been in touch with the library before about archiving data and software, so if this is a high priority for his customers, as he sees them, there is no ways around it.  At second look though, he realizes the project seems to have all the basics straight for a stable and scalable deployment: All components are published under open source licenses, and the project maintainers provide different ready-to-use Docker images. Arthur worries about security, so there is no better way to make sure things work well than source code access. The project is written in a language he has not used before himself, but he can actually build the project himself from source with the provided instructions. He also sees that, instead of reinventing the wheel, the developers seem to be competent enough to build upon established libraries.  The Dockerfiles are great to play around with, but also easy to integrate in his own server management solution. He also likes that all the HTTP APIs as well as setup and configuration seem to be very well documented. This should make it easy to integrate the new solution with some custom tools he developed, but also with some legacy infrastructure he has not yet been able to get rid off. He does worry a bit about the scheduling solution, since he is not very keen on Docker containers being started automatically on his servers. Good thing that the project contributors seem to operate a public chat, and professional support is also available at reasonable prices.  After some testing, he feels good to tell his colleagues: looks good to me, let's try this out!",
            "title": "Arthur the administrator"
        },
        {
            "location": "/user-scenarios/#olivia-the-operator",
            "text": "\"Olivia is the chancellor of a mid-size state owned university in the US. She is proud to have been elected to this position a few years ago, and works very hard each day to improve both the university's reputation and the working and learning conditions of her employees and students. She had to make some unpleasant first hand experiences with aspects of todays academic life that sadly became almost normal, such as budget cuts, violence on campus, and plagiarism scandals. During all of these upsets, she is happy that the never wavered on the importance of personal integrity and credibility of each and every one of the scientists and researchers working on her campus.  To gain some ground in the competition with other universities, Olivia puts her best assistant on the job of finding the newest trends in academia. Soon enough she presents to her the idea of making all research conducted at the university reproducible. Olivia is first surprised by the fact, being an arts major herself, as she thought that is already the case. She starts reading the material provided to hear and realizes that science, and especially something called computational science, is very much different from the practical work she has encountered during her years as a researcher. It also becomes clear to her that it won't work to just put out a statement forcing every lab to spend enormous efforts on changing established research practices, or to re-do what has been done 5, 10 or 20 years ago. The huge variety of labs and workflows that exists and all the different kinds of people... getting out the stick simply won't work. But maybe the carrot will?  She discovers a novel website that promises to solve all the problems of reproducibility. The people behind it seem competent enough to her, but again she asks her assistant to consult with experts from the university library and computer science departments to see what they think. A lengthy discussion starts, and there seems to be no consensus after months of meetings and evaluations. The assistant doesn't know what to report back to Olivia. Eventually, Olivia is tired of waiting and joins a few of the meetings of the expert group as an observer. She realizes that nothing comes for free... she encourages the expert group to create a list of requirements on establishing a reproducibility website for the university. She quickly gets that, as the lecturers and staff in the group realize that this means they won't just get more work to do, they might get the proper time and money to do it!  Olivia makes the new website a matter for the boss. She successfully acquires the funds to start and maintain both the technical services and to hire support staff to maintain it. Beyond that, the supports staff is even equipped to provided consultancy services to all researchers at the university. These services quickly become popular across all disciplines working with data and code, and after just a few months, more and more fully reproducible papers appear on the public reproducibility website. Olivia is very glad to see that the changes she introduced did not have an impact on the scientific output of the university - the monthly statistics tell her that much. Is the quality or quantity of the output going to increase? It's too soon to tell, but Olivia is sure it will. Just last week, the head of the programme reported to her that now ten papers are available on the website for which researchers from different university departments collaborated, who never collaborated before - they discovered the overlap through the new system! More than 20 undergraduate courses teaching scientific methods incorporated material from the website into their course schedule, and 50% of the graduate theses from the computer science department are now using the university reproducibility tools. Those are good enough signs for Olivia. She decides to pitch an idea to the university board: let's include reproducibility of publications as an evaluation factor for the budget allocations next year. You got to use the stick from time to time to make people appreciate the carrot.",
            "title": "Olivia the operator"
        },
        {
            "location": "/user-scenarios/#carl-the-curator",
            "text": "Carl works as a digital assets curator at a university library in Germany. He has been working as a librarian for about ten years and experienced the digital transformation of the field, which is why he specialized in the area of digital curation and archiving. He is qualified to manage and organize several collections of digital objects at a given time and recently selected objects for an exhibition of gold standard open access publications in the software category of his institutions catalog front page.  Carl\u2019s expertise encompasses the management of accessibility levels as well as the preservation of file integrity and meta data curation. Since he discovered a growing interest in the preservation of software, he realized that reproducibility of research findings, including code and data increases the value and visibility of his university\u2019s portfolio. As a result, Carl is working closely with the library\u2019s team for Research Data Management, in order to facilitate integration of reproducible computational environments into the digital objects\u2019 life cycle that underlies their current policies.  As he strongly believes that publicly funded research data are public goods, Carl values his profession as a vital point of intersection between researchers, librarians und the general public. Therefore, when planning a selection of digital assets or curating the library\u2019s catalogs, Carl enjoys the interoperability that is provided by international metadata standards and linked open data vocabularies.",
            "title": "Carl the curator"
        },
        {
            "location": "/user-scenarios/#polly-the-publisher",
            "text": "Polly is the head of a large publishing firm for scientific journals. She grew up being part of a publisher family, the third of four kids. While her older brothers wrestled with the family legacy, she has always been close to her late grandfather, who started the publishing business as a young man. So it came as no surprise that she studied arts and library science and after a few well planned career steps around the globe, she joined the family business as assistant of her father and became CEO after a few years, a decision she rarely regrets.  Though there is one thing that makes her job challenging every day: technology's high development speed. For a large publishing business, it is hard to keep up with new and modern technology. She has to serve both old (in more than one way) customers and employers, who have had a long relationship and a work environment and processes that have developed and settled in over many years. On the other hand, she sees new ideas by entrepreneurs and startups almost every week, some crazy and some rightfully called revolutionary, who experiment with new ways to publish science without the baggage of a reputation and hundreds of journals and an order of magnitude more employees.  So what should Polly do? Scramble up some money to acquire a few startups and replace the existing review and authoring solution? Fire all staff members who are too slow adopting the new technologies? Close journals with an excellent reputation because editors and reviewers are not tech-savvy?   Obviously, none of these were an option. Change had to come gradually and inclusively, not in a disruptive fashion. Polly turned to her CTO Charlotte. She joined the company recently and played out to be a very good hire, as she was able to revive the in-house development team with a positive attitude and a few key hires. Charlotte is aware of the challenges and agrees to compile an action plan from her perspective. A few weeks later, she presents the options to Polly and the other board members. She suggests to adopt an open service for interactive publications, which is an integrated solution for hosting and archiving data as well as code that is often part of publications these days. It is open source, but of course that does not come for free. Charlotte suggests a combined approach of experiments by her own staff and external consulting by the original developers of the software. And she quickly mitigates all concerns raised by the other CxOs: the website is customizable, so it will not look like the competitors versions, it is extensible, so the few \"cool features\" that have been developed over the last years will be easy to integrate, and it is compatible with the existing data repository (so no need to replace that beast of a software). This new website would be an option presented to all editors to adopt for their journals. Education of the company's staff would precede this offer to make sure the intended message is spread: don't be left behind, challenge reviewers and authors to improve the quality of the journals and subsequently raise the bar for high quality open science.",
            "title": "Polly the publisher"
        },
        {
            "location": "/user-scenarios/#richard-the-reviewer",
            "text": "Richard is a successful researcher.\nAfter getting tenure a few years back, he embraces the chance to support students and collaborate with other scientists instead of hunting for the next easy publication to get his name on.\nA big part of his time is taken up by his membership in the editorial boards of two journals and his engagement with several more journals as a reviewer.  Richard is \"senior\" in some ways, and he as well as his colleagues know his value lies in experience, not in hunting the latest hot new things.\nTherefore Richard never came around to catch up practically with the latest technologies, and while he has a good understanding of computer science and used to be a very capable programmer, this new stuff the kids are doing is beyond his means.  As the next paper review request lands in his inbox, he skims the abstract and soon thinks \"I will never be able to thoroughly evaluate this work, the code must be too complex to run on my machine\".\nBut the content is so interesting! What a shame.\nHe almost replies with a negative answer and then sees a new link at the bottom of the notification.\nThe publisher must have added a new feature.\nThe link's title is \"Click here to examine and manipulate code and data\".  Richard clicks the link.\nHe is taken to a website looking partially similar to the old review system he is used to.\nOne the one side there is the well-known article view where he can read, add highlights and make comments.\nBut on the other side, there is a new menu he enthusiastically explores.\nIt allows him to edit parameters and re-run analysis of the paper!\nWithout even downloading any data or code.\nHe immediately sees the benefits: What a relief for his work, and what a chance to dig deeper into the article and conduct a thorough review.  After some brief inspections of the article figures and manipulation of some parameters, Richard feels confident he can actually do the review properly.\nHe let's the editor know about his decision and wants to dive right back into the article, but then stops himself.\nFirst, he writes an email to his fellow editors about this new review system for evaluating code and data - they need it for their journal, too.",
            "title": "Richard the reviewer"
        },
        {
            "location": "/user-scenarios/#rachel-the-reader",
            "text": "Rachel is a second year graduate student in geoinformatics.\nShe's eager to learn and has left all struggles with the technical side of research, and has become a trusted programmer in her group and is seen as an expert in more than one programming language.  When she starts one of her final courses in advanced geoinformatics, the lecturer sends out a long list of reading material. How is she supposed to get through all of it? \nNever faltering, she starts reading all the documents...  After the third article, she is annoyed and underwhelmed by the fancy descriptions and high-level diagrams.\nAlthough they all make sense, she feels like there is more to see and understand than is presented in the article.  She shares her thoughts with her teacher Teresa during the next seminar.\nTeresa can relate to Rachel's frustration and quickly points her to items 8 and 9 on the reading list.\n\"These are different\", she says.  Rachel gets back to reading.\nThe next articles start out the same as the others, but she soon realizes something is different.\nThe website takes a bit longer to load, and the graphics do not seem like they are compressed images at all.\nShe needs some time to explore the relatively complex navigation, but then is excited to discover that she can read and even download all the code and data that was used to generate the figures.\nEven more, she can interact with the present methods and play around with the algorithms.\nFinally she can immediately test her own understanding, challenge her criticism, and resolve misunderstandings.  She plays around with the articles on the website for a little while and spends a lot longer on trying to understand the bits and pieces.\nEventually she sees a close relation of one aspect of the analysis with the research project she though about doing for her thesis.\nRachel is enthusiastic and directly downloads the whole article with its code to her own laptop to try the code out with her own dataset.",
            "title": "Rachel the reader"
        },
        {
            "location": "/glossary/",
            "text": "Glossary\n\n\nERC\n\n\nExecutable Research Compendium, see this \nscientific article\n for concepts and the \nspecification\n for technical documentation.\n\n\nExecutable Research Compendium\n\n\nSee \nERC",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#glossary",
            "text": "",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#erc",
            "text": "Executable Research Compendium, see this  scientific article  for concepts and the  specification  for technical documentation.",
            "title": "ERC"
        },
        {
            "location": "/glossary/#executable-research-compendium",
            "text": "See  ERC",
            "title": "Executable Research Compendium"
        }
    ]
}