{
    "docs": [
        {
            "location": "/",
            "text": "Software architecture for Opening Reproducible Research\n\n\n1. Introduction and Goals\n\n\n1.1 Requirements Overview\n\n\nThe system must provide a reliable way to create, execute, and manipulate reproducibility packages of computational resesearch.\nAt its core is the \nExecutable Research Compendium (ERC)\n, see \nERC specification\n and the publication \n\"Opening the Publication Process with Executable Research Compendia\"\n. A draft of a web API implementing the requirements can be found \nhere\n.\n\n\n1.2 Quality Goals\n\n\n\n\nTransparency\n\n\nThe system must be transparent to allow scrutiny required by a rigorous scientific process.\n\n\nSeparation of concern\n\n\nThe system must integrate with existing platforms and focus on the core functionality. It must not replicate existing functionality such as storage or persistent identification.\n\n\nFlexibility & modularity\n\n\nIn regard to the research project setting, the system components must be well seperated, so that functions can be developed independently, e.g. using different programming languages. This allows different developers to contribute effeciently.\n\n\n\n\n1.3 Stakeholders\n\n\n\n\n\n\n\n\nRole/Name\n\n\nGoal/point of contact\n\n\nRequired interaction\n\n\n\n\n\n\n\n\n\n\nAuthor (scientist)\n\n\npublish ERC as part of a scientific publication process\n\n\n-\n\n\n\n\n\n\nReviewer (scientist)\n\n\ninspect ERC during a review process from a review platform\n\n\n-\n\n\n\n\n\n\nCo-author (scientist)\n\n\ncontribute to ERC during research (e.g. cloud based)\n\n\n-\n\n\n\n\n\n\nReader (scientist)\n\n\nview and interact with ERC on a journal website\n\n\n-\n\n\n\n\n\n\nPublisher\n\n\nincrease quality of publications in journals with ERC\n\n\n-\n\n\n\n\n\n\nCurator/preservationist\n\n\nensure research is complete and archivable using ERC\n\n\n-\n\n\n\n\n\n\nOperator\n\n\nprovide infrastructure to researchers at my university to collaborate and conduct high-quality research using ERC\n\n\n-\n\n\n\n\n\n\nDeveloper\n\n\nuse and extend the tools around ERC\n\n\n-\n\n\n\n\n\n\n\n\nSome of the stakeholders are accompanied by \nuser scenarios\n in prose.\n\n\n2. Architecture constraints\n\n\nThe few constraints on this project are reflected in the final solution. This section shows them and if applicable, their motivation. (based on \nbiking2\n)\n\n\n2.1 Technical constraints\n\n\n\n\n\n\n\n\n\u00a0\n\n\nConstraint\n\n\nBackground and/or motivation\n\n\n\n\n\n\n\n\n\n\nTECH.1\n\n\nOnly open licenses\n\n\nAll third party software or used data must be available under a suitable code license, i.e. either \nOSI-approved\n or \nODC license\n.\n\n\n\n\n\n\nTECH.2\n\n\nOS independent development and deployment\n\n\nServer applications must run in well defined \nDocker\n containers to allow installation on any host system and not limit developers to a specific language or environment.\n\n\n\n\n\n\nTECH.3\n\n\nLower security risks, do not store secure information\n\n\nThe team members experience and available resources do not allow for handling information with security concerns, so no critial data, such as passwords, must be stored in the system.\n\n\n\n\n\n\n\n\n2.2 Organizational constraints\n\n\n\n\n\n\n\n\n\u00a0\n\n\nConstraint\n\n\nBackground and/or motivation\n\n\n\n\n\n\n\n\n\n\nORG.1\n\n\nTeam and schedule\n\n\nhttp://o2r.info/about\n\n\n\n\n\n\nORG.2\n\n\nDo not interfere with existing well-established peer-review process\n\n\nThis software is \nnot\n going to change how scientific publishing works, nor should it. While intentioned to support public peer-reviews, open science etc., the software should be agnostic of these aspects.\n\n\n\n\n\n\nORG.3\n\n\nOnly open licenses\n\n\nAll created software must be available under an \nOSI-approved\n license, documentation and specifiction under a \nCC license\n.\n\n\n\n\n\n\nORG.3\n\n\nVersion control/management\n\n\nCode must be versioned using \ngit\n and published on \nGitHub\n.\n\n\n\n\n\n\n\n\n2.3 Conventions\n\n\n\n\n\n\n\n\n\u00a0\n\n\nConstraint\n\n\nBackground and/or motivation\n\n\n\n\n\n\n\n\n\n\nCONV.1\n\n\nProvide architecture documentation\n\n\nBased on \narc42\n (template version 7.0).\n\n\n\n\n\n\nCONV.2\n\n\nReasonably follow coding conventions\n\n\nTypical project layout and coding conventions of the respective used language should be followed as far as possible. However, we explicitly accept the research project context and do \nnot\n provide full tests suites or documentation beyond what is needed by project team members.\n\n\n\n\n\n\nCONV.3\n\n\nDocumentation is English\n\n\nInternational research project, must be understandable by anyone interested.\n\n\n\n\n\n\nCONV.4\n\n\nUse subjectivization for server component names\n\n\nServer-side components are named using personalized verbs or professions: \nmuncher\n, \nloader\n, \ntransportar\n. All git repositories for software use an \no2r-\n prefix, in case of server-side components e.g. \no2r-shipper\n.\n\n\n\n\n\n\nCONV.5\n\n\nConfiguration using environment variables\n\n\nServer-side components must be configurable using all caps environment variables prefixed with the component name, e.g. \nSHIPPER_THE_SETTING\n, for required settings. Other settings should be put in a settings file suitable for the used language, e.g. \nconfig.js\n or \nconfig.yml\n.\n\n\n\n\n\n\n\n\n3. System scope and context\n\n\n3.1 Business context\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunication partner\n\n\nExchanged data\n\n\nTechnology/protocol\n\n\n\n\n\n\n\n\n\n\nReproducibility service\n, e.g. \no2r\n\n\npublication platforms utilize creation and execution services of ERC, reproducibility service uses repositories to retrieve software artifacts, store runtime environment images, and save complete ERC\n\n\nHTTP\n APIs\n\n\n\n\n\n\nPublication platform\n\n\nusers access ERC status and metadata via search results and paper landing pages; review process integrates ERC details and control;\n\n\nsystem's API using \nHTTP\n with \nJSON\n payload\n\n\n\n\n\n\nID provider\n\n\nretrieve unique user IDs, user metadata, and authentication tokens; user must log in with the provider\n\n\nHTTP\n\n\n\n\n\n\nExecution infrastructure\n\n\nERC can be executed using a shared/distributed infrastructure\n\n\nHTTP\n\n\n\n\n\n\nData repository\n\n\nthe reproducibility service fetches (a) content for ERC creation, or (b) complete ERC, from different sources and stores crated ERC persistently at suitable repositories\n\n\nHTTP\n, \nFTP\n, \nWebDAV\n, \ngit\n\n\n\n\n\n\nRegistry\n\n\nthe reproducibility service can deliver metadata on published ERC to registries/catalogues/search portals, but also retrieve/harvest contextual metadata during ERC creation; users discover ERC on these platforms\n\n\n(proprietary) \nHTTP\n APIs, persistent identifiers (\nDOI\n), \nOAI-PMH\n\n\n\n\n\n\nSoftware repository\n\n\nsoftware repository provide software artifacs during ERC creation and store executable runtime environments\n\n\nHTTP\n APIs\n\n\n\n\n\n\n\n\n\n\n\n3.2 Technical context\n\n\nAll components use \nHTTP\n over cable networks connections for communication of all exchanged data (metadata documents, ERC bundles, Linux containers, etc.)\n\n\n4. Solution strategy\n\n\nThis section provides a short overview of architecture decisions and for some the reasoning behind them.\n\n\nWeb API\n\n\nThe developed solution is set in an existing system of services, and first and foremost must integrate well with these systems, focussing on the specific missing features of building and running ERCs.\nThese features are provided via a \nwell-defined RESTful API\n.\n\n\nMicroservices\n\n\nTo allow a dynamic development and support the large variety of skills, all server-side features are developed in independend \nmicroservices\n.\nThese microservices handle only specific functional parts of the API and allow independent development and deployment cycles.\nCore components are developed using server-side JavaScript based on Node.js while other components are implemented Python.\n\n\nWe accept that this diversification \nincreases complexity\n of both development and testing environments and the deployment of said services.\n\n\nRequired documentation is minimal. The typical structure should follow common practices.\n\n\nStorage and intra-service communication\n\n\nIn accordance with the system scope, there is no reliable storage solution implemented.\nThe microservices simply share a common pointer to a local file system path.\nStorage of ERC is only implemented to make the solution independent during development and for the needs of core functionality (temporal storage), but it is not a feature the solution will eventually provide.\n\n\nThe unifying component of the architecture is the \ndatabase\n.\nIt is known to all microservices.\n\n\nSome microservices communicate via an eventing mechanism for real-time updates, such as the search database and the component providing live updates to the user via WebSockets-\nThe eventing is based on the operation log of the database (which is normally used to synchronise database nodes).\nThis is a clear \nmisuse of an internal feature\n, but a lot simpler than maintaining a full-blown eventing solution.\n\n\nDemonstration, user data & authentication\n\n\nTo be able to demonstrate the system, a \nbrowser-based client application\n is developed.\nIt uses the RESTful API to control the system.\n\nOAuth 2.0\n is used for authentication and minimal information, which is already public, is stored for each user.\nThis information is shared between all services that require authentication via the database.\n\n\nThe client application manages the control flow\n of all user interactions.\n\n\nTools\n\n\nIf standalone tools are developed, they should provide a command-line interface (CLI) that allows integration into microservices when needed.\nThanks to the container architecture and the controlled, we don't need to worry about documentattion for or distribution/packaging of these tools.\nIt must only be ensured they are correctly installed using the microservice's Dockerfile.\nThe only required documentation is for the installation into a container and usage of the CLI.\n\n\n5. Building block view\n\n\n5.1 Refinement Level 1\n\n\n5.1.1 Blackbox Publication Platforms\n\n\nPublications platforms are the primary online interaction point of users with scientific works.\nUsers consume publications, e.g. browsing, searching, and reading.\nUsers create publications, e.g. submitting to a scientific journal.\n\n\n5.1.2 Blackbox ID Provider\n\n\nIdentification information of distributed systems is crucial, and for security reasons as well as for limiting manual reproduction of metadata, a central service can provide all of\n\n\n\n\nunique \nidentification of users\n and \nmetadata on users\n,\n\n\nauthentication\n of users, and\n\n\nmetadata on a user's \nworks\n, e.g. publications or ERC.\n\n\n\n\n\n\n\n5.1.3 Blackbox Execution Infrastructure\n\n\nThe execution infrastructure provides CPU time and temporary result storage space for execution of ERC, both \"as is\" and with manipulation, i.e. changed parameters.\n\n\n5.1.4 Blackbox Data Repositories\n\n\nData repositories are all services and platforms that store data but not software. They may be self-hosted or public free platforms.\nThey are used both for loading content that is used to build an ERC and for storing the ERC created by the reproducibility service.\n\n\n5.1.5 Blackbox Registries\n\n\nRegistries are metadata indexes or catalogues.\n\n\nThey are recipients of metadata exports by the reproducibility service to share information about ERC, e.g. add a new ERC to an author's profile.\nThis requires the reproducibility services to translate the internal metadata model into the recipients data model and encoding. \n\n\nThey are sources of metadata during ERC creation when the information in the fetched content is used to query registries for additional information which can be offered to the user.\n\n\n5.1.6 Blackbox Software Repositories\n\n\nSoftware repositories are a source and a sink for software at different abstraction levels.\nThey are a source for software artifacts or packages, such as system packages in install a library or language-specific extension packages.\nThey are a sink for executable images of software, which comprise a number of software artifacts, for a specific ERC instance.\n\n\n5.2 Refinement Level 2\n\n\n5.2.1 Whitebox Publication Platforms\n\n\nPublication platforms can be roughly divided into two groups.\nThey can be specific journals hosted independently, such as \nJStatSoft\n, or be a larger platform provided by a publisher to multiple journals, such as \nScienceDirect\n, \nMDPI\n, \nSpringerLink\n, or \nPLOS\n.\n\n\nIntegration with the reproducibility service can happen via plug-ins to open platforms, e.g. \nOJS\n, or by bespoke extensions based on the service's public API.\n\n\n5.2.2 Whitebox ID Provider\n\n\nThe reproducibility service uses \nORCID\n to authenticate users and retrieve user and works metadata.\nInternally, the user's public \nORCID\n is the main identifier.\n\n\n5.2.3 Whitebox Execution Infrastructure\n\n\nSuch an infrastructure could be either self-hosted, e.g. \nDocker Swarm\n-based, or use a cloud service provide, such as \nAmazon EC2\n, \nDocker Cloud\n, or even use contrinuous integration platforms such as \nTravis CI\n or \nGitlab CI\n.\n\n\n5.2.4 Whitebox Data Repositories\n\n\n\n\nCollaboration platforms\n, e.g. ownCloud/Sciebo, GitHub, ShareLatex, \nOSF\n, allow users to create, store, and share their research.\nThe reproducibility service fetches contents for building an ERC from these platforms based on public links, e.g. GitHub repository or Sciebo shared folder.\nIt is possible that ERC creation is linked persistently to such collaboration platforms and updates to a shared storage cause ERC creation, execution etc.\n\n\nProtocols: \nWebDAV\n, \nownCloud\n, \nHTTP\n, \ngit\n\n\nDomain data repositories\n, e.g. \nPANGAEA\n or \nGFZ Data Services\n, can be accessed by the reproducibility service during creation and execution of ERC to download data.\nAllowing access to data repositories reduces data duplication but requires control over/trust in the respective repository.\n\n\nProtocol: \nHTTP\n APIs\n\n\nGeneric \nRepositories\n, e.g. \nZenodo\n, \nMendeley Data\n, \nFigshare\n, \nOSF\n, provide (a) access to complete ERC stored in repositories for inspection and execution by the reproducibility service, and (b) storage of created ERC. repositories.\n\n\nThe reproducibility service \ndoes not persistently store anything\n.\n\n\nProtocols: (authenticated) \nHTTP\n APIs\n\n\nArchives\n, e.g. using an installation of \nArchivematica\n, might provide long-term preservation of ERC. Preservation lies in the responsibility of the repository, which might save the hosted content to an archive, or an archive harvests a repository.\n\n\nProtocol: \nHTTP\n carrying bitstreams and metadata\n\n\n5.2.5 Whitebox Registries\n\n\nTBD (\nCRIS\n, \nDataCite\n, \nGoogle Scholar\n, \nScopus\n, ...)\n\n\n5.2.6 Whitebox Software Repositories\n\n\n\n\n5.2.6.1 Blackbox Package repositories\n\n\nPackage repositories are used during ERC creation to download and install software artifacts for specific operating systems, e.g. \nDebian APT\n or \nUbuntu Launchpad\n, for specific programming languages or environments, e.g. \nCRAN\n, or from source, e.g. \nGitHub\n.\n\n\n5.2.6.2 Blackbox Container registries\n\n\nContainer registries such as \nDocker Hub\n, \nQuay\n, self-hosted \nDocker Registry 2.0\n or \nAmazon ERC\n, store executable images of runtime environments.\nThey can be used to distribute the runtime environments across the execution infrastructure and provide an intermediate ephemeral storage for the reproducibility service.\n\n\n5.2.7 Whitebox Reproducibility Service\n\n\n\n\n5.2.7.1 Blackbox Webserver\n\n\nA webserver handles all incoming calls to the API and distributes them to the respective microservice.\nA working \nnginx\n configuration is available \nin the test setup\n.\n\n\n5.2.7.2 Blackbox UI\n\n\nThe UI is a web application based on \nAngular JS\n, see \no2r-platform\n.\nIt connects to an execution microservice (\u00b5service) for real-time WebSocket-based notifications.\n\n\n5.2.7.3 Blackbox microservices\n\n\nThe reproducibility service uses a \nmicroservice architecture\n to seperate functionality defined by the \nweb API specification\n into manageable units.\n\n\nThis allows scalability (selected \u00b5services can be deployed as much as needed) and technology independence for each use case and developer.\nThe \u00b5services all access one main database and a shared file storage.\n\n\n5.2.7.4 Blackbox Tools\n\n\nSome functionality is developed as standalone tools and used as such in the \u00b5services instead of re-implementing features.\nThese tools are integrated via their command line interface (CLI).\n\n\n5.2.7.5 Blackbox Database\n\n\nThe main database is the unifying element of the microservice architecture.\nAll information shared between \u00b5services or transactions between microservices are made via the database, including session state handling (= authentication).\n\n\nA search database/index is used for full-text search and advanced search queries.\n\n\nThe database's operation log, normally used for synchronization between database nodes, is also used for \n\n\n\n\nevent-driven communication between microservices, and\n\n\nsynchronization between main document database and search index.\n\n\n\n\nThis \"eventing hack\" is expected to be replaced by a proper eventing layer for productive deployments.\n\n\n\n5.2.7.6 Blackbox Ephemeral file storage\n\n\nAfter loading from external sources and during creation of ERC, the files are stored in a file storage shared between the \u00b5services.\nThe file structure is known to each microservice and read/write operations happen as needed.\n\n\n5.3 Refinement Level 3\n\n\n5.3.1 Whitebox \u00b5services\n\n\nEach microservice is encapsulated as a \nDocker\n container running at its own port on an internal network and only serving its respective API path.\nFor testing or developing the \no2r-platform\n GitHub project contains \ndocker-compose\n configurations to run all microservices, see the repository's directory \n/test\n and check the projects \nREADME.md\n for instructions.\n\n\nERC loading, building, and access\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmuncher\n\n\n/api/v1/compendium\n\n\nJavaScript (Node.js)\n\n\ncore component for container execution and CRUD for compendia and jobs\n\n\n\n\n\n\nfinder\n\n\n/api/v1/search\n\n\nJavaScript (Node.js)\n\n\ndiscovery and search, synchronizes the database with a search database (Elasticsearch) and exposes read-only search endpoints\n\n\n\n\n\n\ncontentbutler\n\n\n~ /data/\n\n\nJavaScript (Node.js)\n\n\naccess to content of compendia, reads file-base storage\n\n\n\n\n\n\ntransportar\n\n\n~* \\.(zip|tar|tar.gz)\n\n\nJavaScript (Node.js)\n\n\ndownloads of compendia in zip or (gzipped) tar formats\n\n\n\n\n\n\nloader\n\n\nunder development\n\n\nJavaScript\n\n\nload workspaces from repositories and cloud platforms\n\n\n\n\n\n\nmanipulator\n\n\nunder development\n\n\nJavaScript\n\n\nprovide backend containers for interactive ERCs\n\n\n\n\n\n\n\n\nERC exporting\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nshipper\n\n\n/api/v1/shipment\n\n\nPython\n\n\nsave compendia to repositories and archives\n\n\n\n\n\n\n\n\nERC execution\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmuncher\n\n\n/api/v1/job\n\n\nJavaScript (Node.js)\n\n\ncore component for container execution and CRUD for compendia and jobs\n\n\n\n\n\n\ninformer\n\n\n~* \\.io\n\n\nJavaScript (Node.js)\n\n\nsocket.io\n-based WebSockets for live updates to the UI based on database event log\n\n\n\n\n\n\n\n\nAuthentication\n\n\n\n\n\n\n\n\nProject\n\n\nAPI path\n\n\nLanguage\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbouncer\n\n\n/api/v1/auth\n, \n/api/v1/user/\n\n\nJavaScript (Node.js)\n\n\nauthentication service and user information (whoami)\n\n\n\n\n\n\n\n\n5.3.2 Whitebox database\n\n\nTwo databases are used.\n\n\nMongoDB\n document database\n with enabled \nreplica-set oplog\n for eventing.\n\n\nCollections:\n\n\n\n\nusers\n\n\nsessions\n\n\ncompendia\n\n\njobs\n\n\nshipments\n\n\n\n\nThe MongoDB API is used by connecing \u00b5services via suitable client packages, which are available for all required languages.\n\n\nElasticsearch\n search index\n, kept in sync with the main document database by the \u00b5service \nfinder\n.\nThe ids are mapped to support update and delete operations.\n\n\nIndex: \no2r\n\n\nTypes:\n\n\n\n\ncompendia\n\n\njobs\n\n\n\n\nThe search index is read by the UI via a read-only proxy to the regular Elasticsearch API.\n\n\n5.3.3 Whitebox tools\n\n\n\n\n\n\n\n\nproject\n\n\nlanguage\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nmeta\n\n\nPython\n\n\nscripts for extraction, translation and validation of metadata\n\n\n\n\n\n\ncontaineRit\n\n\nR\n\n\ngeneration of Dockerfiles based on R sessions and scripts\n\n\n\n\n\n\n\n\n5.3.4 Whitebox ephemeral file storage\n\n\nA host directory is mounted into every container to the same location: \n/tmp/o2r:/tmp/o2r\n\n\n\n\n\n\n\nCredits\n\n\nThis specification and guides are developed by the members of the DFG-funded project Opening Reproducible Research.\n\n\n\n\n\n\nOpening Reproducible Research (o2r) is a DFG-funded research project by Institute for Geoinformatics (\nifgi\n) and University and Regional Library (\nULB\n), University of M\u00fcnster, Germany. Building on recent advances in mainstream IT, o2r envisions a new architecture for storing, executing and interacting with the original analysis environment alongside the corresponding research data and manuscript. This architecture evolves around so called Executable Research Compendia (ERC) as the container for both research, review, and archival.\n\n\n\n\n\n\nLicense\n\n\n\n\nThe o2r architecture specification is licensed under \nCreative Commons CC0 1.0 Universal License\n, see file \nLICENSE\n.\nTo the extent possible under law, the people who associated CC0 with this work have waived all copyright and related or neighboring rights to this work.\nThis work is published from: Germany.\n\n\n\n\nAbout arc42\n\n\narc42, the Template for documentation of software and system architecture.\n\n\nBy Dr. Gernot Starke, Dr. Peter Hruschka and contributors.\n\n\nTemplate Revision: 7.0 EN (based on asciidoc), January 2017\n\n\n\u00a9 We acknowledge that this document uses material from the arc 42 architecture template, \nhttp://www.arc42.de\n. Created by Dr. Peter Hruschka & Dr. Gernot Starke\n\n\nBuild @@VERSION@@ @ @@TIMESTAMP@@",
            "title": "Architecture"
        },
        {
            "location": "/#software-architecture-for-opening-reproducible-research",
            "text": "",
            "title": "Software architecture for Opening Reproducible Research"
        },
        {
            "location": "/#1-introduction-and-goals",
            "text": "",
            "title": "1. Introduction and Goals"
        },
        {
            "location": "/#11-requirements-overview",
            "text": "The system must provide a reliable way to create, execute, and manipulate reproducibility packages of computational resesearch.\nAt its core is the  Executable Research Compendium (ERC) , see  ERC specification  and the publication  \"Opening the Publication Process with Executable Research Compendia\" . A draft of a web API implementing the requirements can be found  here .",
            "title": "1.1 Requirements Overview"
        },
        {
            "location": "/#12-quality-goals",
            "text": "Transparency  The system must be transparent to allow scrutiny required by a rigorous scientific process.  Separation of concern  The system must integrate with existing platforms and focus on the core functionality. It must not replicate existing functionality such as storage or persistent identification.  Flexibility & modularity  In regard to the research project setting, the system components must be well seperated, so that functions can be developed independently, e.g. using different programming languages. This allows different developers to contribute effeciently.",
            "title": "1.2 Quality Goals"
        },
        {
            "location": "/#13-stakeholders",
            "text": "Role/Name  Goal/point of contact  Required interaction      Author (scientist)  publish ERC as part of a scientific publication process  -    Reviewer (scientist)  inspect ERC during a review process from a review platform  -    Co-author (scientist)  contribute to ERC during research (e.g. cloud based)  -    Reader (scientist)  view and interact with ERC on a journal website  -    Publisher  increase quality of publications in journals with ERC  -    Curator/preservationist  ensure research is complete and archivable using ERC  -    Operator  provide infrastructure to researchers at my university to collaborate and conduct high-quality research using ERC  -    Developer  use and extend the tools around ERC  -     Some of the stakeholders are accompanied by  user scenarios  in prose.",
            "title": "1.3 Stakeholders"
        },
        {
            "location": "/#2-architecture-constraints",
            "text": "The few constraints on this project are reflected in the final solution. This section shows them and if applicable, their motivation. (based on  biking2 )",
            "title": "2. Architecture constraints"
        },
        {
            "location": "/#21-technical-constraints",
            "text": "Constraint  Background and/or motivation      TECH.1  Only open licenses  All third party software or used data must be available under a suitable code license, i.e. either  OSI-approved  or  ODC license .    TECH.2  OS independent development and deployment  Server applications must run in well defined  Docker  containers to allow installation on any host system and not limit developers to a specific language or environment.    TECH.3  Lower security risks, do not store secure information  The team members experience and available resources do not allow for handling information with security concerns, so no critial data, such as passwords, must be stored in the system.",
            "title": "2.1 Technical constraints"
        },
        {
            "location": "/#22-organizational-constraints",
            "text": "Constraint  Background and/or motivation      ORG.1  Team and schedule  http://o2r.info/about    ORG.2  Do not interfere with existing well-established peer-review process  This software is  not  going to change how scientific publishing works, nor should it. While intentioned to support public peer-reviews, open science etc., the software should be agnostic of these aspects.    ORG.3  Only open licenses  All created software must be available under an  OSI-approved  license, documentation and specifiction under a  CC license .    ORG.3  Version control/management  Code must be versioned using  git  and published on  GitHub .",
            "title": "2.2 Organizational constraints"
        },
        {
            "location": "/#23-conventions",
            "text": "Constraint  Background and/or motivation      CONV.1  Provide architecture documentation  Based on  arc42  (template version 7.0).    CONV.2  Reasonably follow coding conventions  Typical project layout and coding conventions of the respective used language should be followed as far as possible. However, we explicitly accept the research project context and do  not  provide full tests suites or documentation beyond what is needed by project team members.    CONV.3  Documentation is English  International research project, must be understandable by anyone interested.    CONV.4  Use subjectivization for server component names  Server-side components are named using personalized verbs or professions:  muncher ,  loader ,  transportar . All git repositories for software use an  o2r-  prefix, in case of server-side components e.g.  o2r-shipper .    CONV.5  Configuration using environment variables  Server-side components must be configurable using all caps environment variables prefixed with the component name, e.g.  SHIPPER_THE_SETTING , for required settings. Other settings should be put in a settings file suitable for the used language, e.g.  config.js  or  config.yml .",
            "title": "2.3 Conventions"
        },
        {
            "location": "/#3-system-scope-and-context",
            "text": "",
            "title": "3. System scope and context"
        },
        {
            "location": "/#31-business-context",
            "text": "Communication partner  Exchanged data  Technology/protocol      Reproducibility service , e.g.  o2r  publication platforms utilize creation and execution services of ERC, reproducibility service uses repositories to retrieve software artifacts, store runtime environment images, and save complete ERC  HTTP  APIs    Publication platform  users access ERC status and metadata via search results and paper landing pages; review process integrates ERC details and control;  system's API using  HTTP  with  JSON  payload    ID provider  retrieve unique user IDs, user metadata, and authentication tokens; user must log in with the provider  HTTP    Execution infrastructure  ERC can be executed using a shared/distributed infrastructure  HTTP    Data repository  the reproducibility service fetches (a) content for ERC creation, or (b) complete ERC, from different sources and stores crated ERC persistently at suitable repositories  HTTP ,  FTP ,  WebDAV ,  git    Registry  the reproducibility service can deliver metadata on published ERC to registries/catalogues/search portals, but also retrieve/harvest contextual metadata during ERC creation; users discover ERC on these platforms  (proprietary)  HTTP  APIs, persistent identifiers ( DOI ),  OAI-PMH    Software repository  software repository provide software artifacs during ERC creation and store executable runtime environments  HTTP  APIs",
            "title": "3.1 Business context"
        },
        {
            "location": "/#32-technical-context",
            "text": "All components use  HTTP  over cable networks connections for communication of all exchanged data (metadata documents, ERC bundles, Linux containers, etc.)",
            "title": "3.2 Technical context"
        },
        {
            "location": "/#4-solution-strategy",
            "text": "This section provides a short overview of architecture decisions and for some the reasoning behind them.",
            "title": "4. Solution strategy"
        },
        {
            "location": "/#web-api",
            "text": "The developed solution is set in an existing system of services, and first and foremost must integrate well with these systems, focussing on the specific missing features of building and running ERCs.\nThese features are provided via a  well-defined RESTful API .",
            "title": "Web API"
        },
        {
            "location": "/#microservices",
            "text": "To allow a dynamic development and support the large variety of skills, all server-side features are developed in independend  microservices .\nThese microservices handle only specific functional parts of the API and allow independent development and deployment cycles.\nCore components are developed using server-side JavaScript based on Node.js while other components are implemented Python.  We accept that this diversification  increases complexity  of both development and testing environments and the deployment of said services.  Required documentation is minimal. The typical structure should follow common practices.",
            "title": "Microservices"
        },
        {
            "location": "/#storage-and-intra-service-communication",
            "text": "In accordance with the system scope, there is no reliable storage solution implemented.\nThe microservices simply share a common pointer to a local file system path.\nStorage of ERC is only implemented to make the solution independent during development and for the needs of core functionality (temporal storage), but it is not a feature the solution will eventually provide.  The unifying component of the architecture is the  database .\nIt is known to all microservices.  Some microservices communicate via an eventing mechanism for real-time updates, such as the search database and the component providing live updates to the user via WebSockets-\nThe eventing is based on the operation log of the database (which is normally used to synchronise database nodes).\nThis is a clear  misuse of an internal feature , but a lot simpler than maintaining a full-blown eventing solution.",
            "title": "Storage and intra-service communication"
        },
        {
            "location": "/#demonstration-user-data-authentication",
            "text": "To be able to demonstrate the system, a  browser-based client application  is developed.\nIt uses the RESTful API to control the system. OAuth 2.0  is used for authentication and minimal information, which is already public, is stored for each user.\nThis information is shared between all services that require authentication via the database.  The client application manages the control flow  of all user interactions.",
            "title": "Demonstration, user data &amp; authentication"
        },
        {
            "location": "/#tools",
            "text": "If standalone tools are developed, they should provide a command-line interface (CLI) that allows integration into microservices when needed.\nThanks to the container architecture and the controlled, we don't need to worry about documentattion for or distribution/packaging of these tools.\nIt must only be ensured they are correctly installed using the microservice's Dockerfile.\nThe only required documentation is for the installation into a container and usage of the CLI.",
            "title": "Tools"
        },
        {
            "location": "/#5-building-block-view",
            "text": "",
            "title": "5. Building block view"
        },
        {
            "location": "/#51-refinement-level-1",
            "text": "",
            "title": "5.1 Refinement Level 1"
        },
        {
            "location": "/#511-blackbox-publication-platforms",
            "text": "Publications platforms are the primary online interaction point of users with scientific works.\nUsers consume publications, e.g. browsing, searching, and reading.\nUsers create publications, e.g. submitting to a scientific journal.",
            "title": "5.1.1 Blackbox Publication Platforms"
        },
        {
            "location": "/#512-blackbox-id-provider",
            "text": "Identification information of distributed systems is crucial, and for security reasons as well as for limiting manual reproduction of metadata, a central service can provide all of   unique  identification of users  and  metadata on users ,  authentication  of users, and  metadata on a user's  works , e.g. publications or ERC.",
            "title": "5.1.2 Blackbox ID Provider"
        },
        {
            "location": "/#513-blackbox-execution-infrastructure",
            "text": "The execution infrastructure provides CPU time and temporary result storage space for execution of ERC, both \"as is\" and with manipulation, i.e. changed parameters.",
            "title": "5.1.3 Blackbox Execution Infrastructure"
        },
        {
            "location": "/#514-blackbox-data-repositories",
            "text": "Data repositories are all services and platforms that store data but not software. They may be self-hosted or public free platforms.\nThey are used both for loading content that is used to build an ERC and for storing the ERC created by the reproducibility service.",
            "title": "5.1.4 Blackbox Data Repositories"
        },
        {
            "location": "/#515-blackbox-registries",
            "text": "Registries are metadata indexes or catalogues.  They are recipients of metadata exports by the reproducibility service to share information about ERC, e.g. add a new ERC to an author's profile.\nThis requires the reproducibility services to translate the internal metadata model into the recipients data model and encoding.   They are sources of metadata during ERC creation when the information in the fetched content is used to query registries for additional information which can be offered to the user.",
            "title": "5.1.5 Blackbox Registries"
        },
        {
            "location": "/#516-blackbox-software-repositories",
            "text": "Software repositories are a source and a sink for software at different abstraction levels.\nThey are a source for software artifacts or packages, such as system packages in install a library or language-specific extension packages.\nThey are a sink for executable images of software, which comprise a number of software artifacts, for a specific ERC instance.",
            "title": "5.1.6 Blackbox Software Repositories"
        },
        {
            "location": "/#52-refinement-level-2",
            "text": "",
            "title": "5.2 Refinement Level 2"
        },
        {
            "location": "/#521-whitebox-publication-platforms",
            "text": "Publication platforms can be roughly divided into two groups.\nThey can be specific journals hosted independently, such as  JStatSoft , or be a larger platform provided by a publisher to multiple journals, such as  ScienceDirect ,  MDPI ,  SpringerLink , or  PLOS .  Integration with the reproducibility service can happen via plug-ins to open platforms, e.g.  OJS , or by bespoke extensions based on the service's public API.",
            "title": "5.2.1 Whitebox Publication Platforms"
        },
        {
            "location": "/#522-whitebox-id-provider",
            "text": "The reproducibility service uses  ORCID  to authenticate users and retrieve user and works metadata.\nInternally, the user's public  ORCID  is the main identifier.",
            "title": "5.2.2 Whitebox ID Provider"
        },
        {
            "location": "/#523-whitebox-execution-infrastructure",
            "text": "Such an infrastructure could be either self-hosted, e.g.  Docker Swarm -based, or use a cloud service provide, such as  Amazon EC2 ,  Docker Cloud , or even use contrinuous integration platforms such as  Travis CI  or  Gitlab CI .",
            "title": "5.2.3 Whitebox Execution Infrastructure"
        },
        {
            "location": "/#524-whitebox-data-repositories",
            "text": "Collaboration platforms , e.g. ownCloud/Sciebo, GitHub, ShareLatex,  OSF , allow users to create, store, and share their research.\nThe reproducibility service fetches contents for building an ERC from these platforms based on public links, e.g. GitHub repository or Sciebo shared folder.\nIt is possible that ERC creation is linked persistently to such collaboration platforms and updates to a shared storage cause ERC creation, execution etc.  Protocols:  WebDAV ,  ownCloud ,  HTTP ,  git  Domain data repositories , e.g.  PANGAEA  or  GFZ Data Services , can be accessed by the reproducibility service during creation and execution of ERC to download data.\nAllowing access to data repositories reduces data duplication but requires control over/trust in the respective repository.  Protocol:  HTTP  APIs  Generic  Repositories , e.g.  Zenodo ,  Mendeley Data ,  Figshare ,  OSF , provide (a) access to complete ERC stored in repositories for inspection and execution by the reproducibility service, and (b) storage of created ERC. repositories.  The reproducibility service  does not persistently store anything .  Protocols: (authenticated)  HTTP  APIs  Archives , e.g. using an installation of  Archivematica , might provide long-term preservation of ERC. Preservation lies in the responsibility of the repository, which might save the hosted content to an archive, or an archive harvests a repository.  Protocol:  HTTP  carrying bitstreams and metadata",
            "title": "5.2.4 Whitebox Data Repositories"
        },
        {
            "location": "/#525-whitebox-registries",
            "text": "TBD ( CRIS ,  DataCite ,  Google Scholar ,  Scopus , ...)",
            "title": "5.2.5 Whitebox Registries"
        },
        {
            "location": "/#526-whitebox-software-repositories",
            "text": "",
            "title": "5.2.6 Whitebox Software Repositories"
        },
        {
            "location": "/#5261-blackbox-package-repositories",
            "text": "Package repositories are used during ERC creation to download and install software artifacts for specific operating systems, e.g.  Debian APT  or  Ubuntu Launchpad , for specific programming languages or environments, e.g.  CRAN , or from source, e.g.  GitHub .",
            "title": "5.2.6.1 Blackbox Package repositories"
        },
        {
            "location": "/#5262-blackbox-container-registries",
            "text": "Container registries such as  Docker Hub ,  Quay , self-hosted  Docker Registry 2.0  or  Amazon ERC , store executable images of runtime environments.\nThey can be used to distribute the runtime environments across the execution infrastructure and provide an intermediate ephemeral storage for the reproducibility service.",
            "title": "5.2.6.2 Blackbox Container registries"
        },
        {
            "location": "/#527-whitebox-reproducibility-service",
            "text": "",
            "title": "5.2.7 Whitebox Reproducibility Service"
        },
        {
            "location": "/#5271-blackbox-webserver",
            "text": "A webserver handles all incoming calls to the API and distributes them to the respective microservice.\nA working  nginx  configuration is available  in the test setup .",
            "title": "5.2.7.1 Blackbox Webserver"
        },
        {
            "location": "/#5272-blackbox-ui",
            "text": "The UI is a web application based on  Angular JS , see  o2r-platform .\nIt connects to an execution microservice (\u00b5service) for real-time WebSocket-based notifications.",
            "title": "5.2.7.2 Blackbox UI"
        },
        {
            "location": "/#5273-blackbox-microservices",
            "text": "The reproducibility service uses a  microservice architecture  to seperate functionality defined by the  web API specification  into manageable units.  This allows scalability (selected \u00b5services can be deployed as much as needed) and technology independence for each use case and developer.\nThe \u00b5services all access one main database and a shared file storage.",
            "title": "5.2.7.3 Blackbox microservices"
        },
        {
            "location": "/#5274-blackbox-tools",
            "text": "Some functionality is developed as standalone tools and used as such in the \u00b5services instead of re-implementing features.\nThese tools are integrated via their command line interface (CLI).",
            "title": "5.2.7.4 Blackbox Tools"
        },
        {
            "location": "/#5275-blackbox-database",
            "text": "The main database is the unifying element of the microservice architecture.\nAll information shared between \u00b5services or transactions between microservices are made via the database, including session state handling (= authentication).  A search database/index is used for full-text search and advanced search queries.  The database's operation log, normally used for synchronization between database nodes, is also used for    event-driven communication between microservices, and  synchronization between main document database and search index.   This \"eventing hack\" is expected to be replaced by a proper eventing layer for productive deployments.",
            "title": "5.2.7.5 Blackbox Database"
        },
        {
            "location": "/#5276-blackbox-ephemeral-file-storage",
            "text": "After loading from external sources and during creation of ERC, the files are stored in a file storage shared between the \u00b5services.\nThe file structure is known to each microservice and read/write operations happen as needed.",
            "title": "5.2.7.6 Blackbox Ephemeral file storage"
        },
        {
            "location": "/#53-refinement-level-3",
            "text": "",
            "title": "5.3 Refinement Level 3"
        },
        {
            "location": "/#531-whitebox-services",
            "text": "Each microservice is encapsulated as a  Docker  container running at its own port on an internal network and only serving its respective API path.\nFor testing or developing the  o2r-platform  GitHub project contains  docker-compose  configurations to run all microservices, see the repository's directory  /test  and check the projects  README.md  for instructions.",
            "title": "5.3.1 Whitebox \u00b5services"
        },
        {
            "location": "/#erc-loading-building-and-access",
            "text": "Project  API path  Language  Description      muncher  /api/v1/compendium  JavaScript (Node.js)  core component for container execution and CRUD for compendia and jobs    finder  /api/v1/search  JavaScript (Node.js)  discovery and search, synchronizes the database with a search database (Elasticsearch) and exposes read-only search endpoints    contentbutler  ~ /data/  JavaScript (Node.js)  access to content of compendia, reads file-base storage    transportar  ~* \\.(zip|tar|tar.gz)  JavaScript (Node.js)  downloads of compendia in zip or (gzipped) tar formats    loader  under development  JavaScript  load workspaces from repositories and cloud platforms    manipulator  under development  JavaScript  provide backend containers for interactive ERCs",
            "title": "ERC loading, building, and access"
        },
        {
            "location": "/#erc-exporting",
            "text": "Project  API path  Language  Description      shipper  /api/v1/shipment  Python  save compendia to repositories and archives",
            "title": "ERC exporting"
        },
        {
            "location": "/#erc-execution",
            "text": "Project  API path  Language  Description      muncher  /api/v1/job  JavaScript (Node.js)  core component for container execution and CRUD for compendia and jobs    informer  ~* \\.io  JavaScript (Node.js)  socket.io -based WebSockets for live updates to the UI based on database event log",
            "title": "ERC execution"
        },
        {
            "location": "/#authentication",
            "text": "Project  API path  Language  Description      bouncer  /api/v1/auth ,  /api/v1/user/  JavaScript (Node.js)  authentication service and user information (whoami)",
            "title": "Authentication"
        },
        {
            "location": "/#532-whitebox-database",
            "text": "Two databases are used.  MongoDB  document database  with enabled  replica-set oplog  for eventing.  Collections:   users  sessions  compendia  jobs  shipments   The MongoDB API is used by connecing \u00b5services via suitable client packages, which are available for all required languages.  Elasticsearch  search index , kept in sync with the main document database by the \u00b5service  finder .\nThe ids are mapped to support update and delete operations.  Index:  o2r  Types:   compendia  jobs   The search index is read by the UI via a read-only proxy to the regular Elasticsearch API.",
            "title": "5.3.2 Whitebox database"
        },
        {
            "location": "/#533-whitebox-tools",
            "text": "project  language  description      meta  Python  scripts for extraction, translation and validation of metadata    containeRit  R  generation of Dockerfiles based on R sessions and scripts",
            "title": "5.3.3 Whitebox tools"
        },
        {
            "location": "/#534-whitebox-ephemeral-file-storage",
            "text": "A host directory is mounted into every container to the same location:  /tmp/o2r:/tmp/o2r",
            "title": "5.3.4 Whitebox ephemeral file storage"
        },
        {
            "location": "/#credits",
            "text": "This specification and guides are developed by the members of the DFG-funded project Opening Reproducible Research.    Opening Reproducible Research (o2r) is a DFG-funded research project by Institute for Geoinformatics ( ifgi ) and University and Regional Library ( ULB ), University of M\u00fcnster, Germany. Building on recent advances in mainstream IT, o2r envisions a new architecture for storing, executing and interacting with the original analysis environment alongside the corresponding research data and manuscript. This architecture evolves around so called Executable Research Compendia (ERC) as the container for both research, review, and archival.",
            "title": "Credits"
        },
        {
            "location": "/#license",
            "text": "The o2r architecture specification is licensed under  Creative Commons CC0 1.0 Universal License , see file  LICENSE .\nTo the extent possible under law, the people who associated CC0 with this work have waived all copyright and related or neighboring rights to this work.\nThis work is published from: Germany.",
            "title": "License"
        },
        {
            "location": "/#about-arc42",
            "text": "arc42, the Template for documentation of software and system architecture.  By Dr. Gernot Starke, Dr. Peter Hruschka and contributors.  Template Revision: 7.0 EN (based on asciidoc), January 2017  \u00a9 We acknowledge that this document uses material from the arc 42 architecture template,  http://www.arc42.de . Created by Dr. Peter Hruschka & Dr. Gernot Starke  Build @@VERSION@@ @ @@TIMESTAMP@@ < /div",
            "title": "About arc42"
        },
        {
            "location": "/user-scenarios/",
            "text": "User scenarios\n\n\nAndrea the author and reader\n\n\n\n\nAndrea turned 29 this year. She is always up for a joke and a pot of coffee but is also quite impatient. Especially if she has to wait for others or if she hasn\u2019t had any progress for a while. However, currently Andrea does his Ph.D in the field of geosciences. Two years ago she decided to go for a cumulative dissertation, meaning that she publishes scientific papers throughout his graduation and summarizes them at the end. She already published his first paper a few months ago which is good, actually. One of the reviewers was interested in the data and the source-code in order to reproduce the results.\n\n\nAfter a few hours of searching (remember that she is not one of the most patient guys), she finally finds some files that include the dataset and also the source-code in R (a statistics program). Just a short try if it still working\u2026Weird, the results are different. Just a short look into the paper ... The configuration is different than the one described in the method section. Well, just few manipulations and \u2026 still not working. \nAlthough she submitted the paper just a few months ago, she can\u2019t remember the configuration that lead to the results in the paper. Fortunately, submitting data and code was not mandatory. But Andrea knows that she made a mistake.\n\n\nMore and more journals expect their authors to submit data and source-code that underlie the research findings. For this reason, she wants to change his working behavior and to keep his files under control. She remembers his last research work which was quite unstructured, maybe already messy. His code and data was distributed over several folders and even computers. She had so search for them for quite a while. Moreover, some components do not work anymore. This time, she wants to do it better and searches for a great tool that assists his working flow. He just heard about a new platform supporting reproducible research. The platform allows to upload all necessary files and to create a, so called, container which is executable. It even verifies the results in the paper making it possible to detect errors immediately. Of course it also contains common features like sharing the publication with other authors. On top of that, Andrea can also benefit from other publications. As the platform automatically generates a number of meta information, new search capabilities arise. It is not only possible to search for other publications by using keywords, but also by using spatial and temporal properties and constraints. It is even possible to constrain the search to hypotheses and research questions that contain a certain vocabulary thus simplifying search for related work. Andrea is quite impressed about that. She easily finds related papers that suit to his own work. She gets a good overview about existing research questions making it easier to identify a research gap he can focus on. Andrea doesn\u2019t even have to implement all the code lines for his statistical analysis from scratch, but can build upon existing. While reading some of the related papers in the browser, he realizes a couple of user interface widgets besides the incorporated figures. He doesn\u2019t know them from traditional, static papers which are typically published as .pdf-files. Andrea recognizes that the widgets allow to interact with the diagrams to which the widgets belong. They allow to change, for example, thresholds, input variables and constants. She is thus able to check the assumptions and conclusions underlying the paper. She is a bit overwhelmed by the number of new features that might assist his current research such as exchanging the dataset or the source-code underlying the paper. Andrea is quite happy about his new tool. It provides support for structured work, finding related publications, algorithms and datasets, identifying a research gap, and even tools for interacting with traditional, static papers. So, let\u2019s go for the second paper.\n\n\n\n\nArthur the administrator\n\n\n\n\nArthur works as a system administrator in a large university library in Germany. He's quite happy with his job. After working as freelance software developer for over 20 years, he now enjoys the challenge to make all the different servers and applications under his care work like a charm 24/7 while having a stable paycheck and reasonable working hours. He is particularly proud that, since he took over the job, he successfully migrated all services to a private cloud infrastructure and enabled https-only traffic on all, event the internal, APIs and websites. Since then, there has been minimal overtime for him and close to 0 minutes downtime for the services... and a raise!\n\n\nArthur is interested in this new reproducibility platform that the head of the library is interested in, but he is sceptical about all new systems. There are going to be bugs, unforseen problems, and a lot of testing \"\"in production\"\", which he does not like. But he does now that scientists have been in touch with the library before about archiving data and software, so if this is a high priority for his customers, as he sees them, there is no ways around it.\n\n\nAt second look though, he realizes the project seems to have all the basics straight for a stable and scalable deployment: All components are published under open source licenses, and the project maintainers provide different ready-to-use Docker images. Arthur worries about security, so there is no better way to make sure things work well than source code access. The project is written in a language he has not used before himself, but he can actually build the project himself from source with the provided instructions. He also sees that, instead of reinventing the wheel, the developers seem to be competent enough to build upon established libraries.\n\n\nThe Dockerfiles are great to play around with, but also easy to integrate in his own server management solution. He also likes that all the HTTP APIs as well as setup and configuration seem to be very well documented. This should make it easy to integrate the new solution with some custom tools he developed, but also with some legacy infrastructure he has not yet been able to get rid off. He does worry a bit about the scheduling solution, since he is not very keen on Docker containers beeing started automatically on his servers. Good thing that the project contributors seem to operate a public chat, and professional support is also available at reasonable prices.\n\n\nAfter some testing, he feels good to tell his colleagues: looks good to me, let's try this out!\n\n\n\n\nOlivia the operator\n\n\n\n\n\"Olivia is the chancellor of a mid-size state owned university in the US. She is proud to have been elected to this position a few years ago, and works very hard each day to improve both the university's reputation and the working and learning conditions of her employees and students. She had to make some unpleasent first hand experiences with aspects of todays academic life that sadly became almost normal, such as budget cuts, violence on campus, and plagarism scandals. During all of these upsets, she is happy that the never waivered on the importance of personal integrity and credibility of each and every one of the scientists and researchers working on her campus.\n\n\nTo gain some ground in the competition with other universities, Olivia puts her best assisstant on the job of finding the newest trends in academia. Soon enough she presents to her the idea of making all research conducted at the university reproducible. Olivia is first surprised by the fact, being an arts major herself, as she thought that is already the case. She starts reading the material provided to hear and realizes that science, and especially somehting called computational science, is very much different from the practical work she has encountered during her years as a researcher. It also becomes clear to her that it won't work to just put out a statement forcing every lab to spend enormous efforts on changing established research practices, or to re-do what has been done 5, 10 or 20 years ago. The huge variety of labs and workflows that exists and all the different kinds of people... getting out the stick simply won't work. But maybe the carrot will?\n\n\nShe discovers a novel platform that promises to solve all the problems of reproducibility. The people behind it seem competent enough to her, but again she asks her assisstant to consult with experts from the university library and computer science departsments to see what they think. A lengthy discussion starts, and there seems to be no concensus after months of meetings and evaluations. The assisstant doesn't know what to report back to Olivia. Eventually, Olivia is tired of waiting and joins a few of the meetings of the expert group as an observer. She realizes that nothing comes for free... she encourages the expert group to create a list of requirements on establishing a reproducibility platform for the university. She quickly gets that one, as the lecturers and staff in the group realize that this means they won't just get more work to do, they might get the proper time and money to do it!\n\n\nOlivia makes the platform a matter for the boss. She sucessfully acquires the funds to start and maintain both the technical platform and to hire support staff to maintain it. Beyond that, the supports staff is even equipped to provided consultancy services to all researchers at the university. These services quickly become popular across all disciplines working with data and code, and after just a few months, more and more fully reproducible papers appear on the public reproducibility platform. Olivia is very glad to see that the changes she introduced did not have an impact on the scientific output of the university - the monthly statistics tell her that much. Is the quality or quantity of the output going to increase? It's too soon to tell, but Olivia is sure it will. Just last week, the head of the programme reported to her that now ten papers are available on the platform for which researchers from different university departments collaborated, who never collaborated before - they discovered the overlap through the publication platform! More than 20 undergraduate courses teaching scientific methods incorporated the platform into their course schedule, and 50% of the graduate theses from the computer science department are now using the university reproducibility tools. Those are good enough signs for Olivia. She decides to pitch an idea to the university board: let's include reproducibility of publications as an evaluation factor for the budget allocations next year. You got to use the stick from time to time to make people aprecciate the carrot.\n\n\n\n\nCarl the curator\n\n\n\n\nCarl works as a digital assets curator at a university library in Germany. He has been working as a librarian for about ten years and experienced the digital transformation of the field, which is why he specialized in the area of digital curation and archiving. He is qualified to manage and organize several collections of digital objects at a given time and recently selected objects for an exhibition of gold standard open access publications in the software category of his institutions catalog frontpage.\nCarl\u2019s expertise encompasses the management of accessibility levels as well as the preservation of file integrity and meta data curation. Since he discovered a growing interest in the preservation of software, he realized that reproducibility of research findings, including code and data increases the value and visibility of his university\u2019s portfolio. As a result, Carl is working closely with the library\u2019s team for Research Data Management, in order to facilitate integration of reproducible computational environments into the digital objects\u2019 life cycle that underlies their current policies.\nAs he strongly believes that publicly funded research data are public goods, Carl values his profession as a vital point of intersection between researchers, librarians und the general public. Therefore, when planning a selection of digital assets or curating the library\u2019s catalogs, Carl enjoys the interoperability that is provided by international metadata standards and linked open data vocabularies.\n\n\n\n\nPolly the publisher\n\n\n\n\nPolly is the head of a large publishing firm for scientific journals. She grew up being part of a publisher family, the third of four kids. While her older brothers wrestled with the family legacy, she has always been close to her late grandfather, who started the publishing business as a young man. So it came as no surprise that she studied arts and library science and after a few well planned career steps around the globe, she joined the family business as assitant of her father and became CEO after a few years, a decision she rarely regrets.\n\n\nThough there is one thing that makes her job challenging every day: technology's high development speed. For a large publishing business, it is hard to keep up with new and modern technology. She has to serve both old (in more than one way) customers and employers, who have had a long relationship and a work environment and processes that have developed and settled in over many years. On the other hand, she sees new ideas by entrepeneurs and startups almost every week, some crazy and some rightfully called revolutional, who experiment with new ways to publish science without the baggage of a reputation and hundrets of journals and an order of magnitude more employees.\n\n\nSo what should Polly do? Scrample up some money to acquire a few startups and replace the existing review and authoring platform? Fire all staff members who are too slow adopting the new technologies? Close journals with an excellent reputation because editors and reviewers are not tech-savvy? \n\n\nObviously, none of these were an option. Change had to come gradually and incluse, not in a disruptive fashion. Polly turned to her CTO Charlotte. She joined the company recently and played out to be a very good hire, as she was able to revive the in-house development team with a positive attitude and a few key hires. Charlotte is aware of the challenges and agress to compile an action plan from her perspective. A few weeks later, she presents the options to Polly and the other board members. She suggests to adopt and open platform for interactive publications, which is an integrated solution for hosting and archiving data as well as code that is often part of publications these days. It is open source, but of course that does not come for free. Charlotte suggests a combined approach of experiments by her own staff and external consulting by the original developers of the platform. And she quickly mitigates all concerns raised by the other CxOs: the platform is customizable, so it will not look like the competitors versions, it is extensible, so the few \"\"cool features\"\" that have been developed over the last years will be easy to integrate, and it is compatible with the existing data hosting platform (so no need to replace that beast of a software). This new platform would be an option presented to all editors to adopt for their journals. Education of the company's staff would precede this offer to make sure the inteneded message is spread: don't be left behind, challenge reviewers and authors to improve the quality of the journals and subsequently raise the bar for high quality open science.\n\n\n\n\nRichard the reviewer\n\n\nTBD\n\n\nRachel the reader\n\n\nTBD",
            "title": "User scenarios"
        },
        {
            "location": "/user-scenarios/#user-scenarios",
            "text": "",
            "title": "User scenarios"
        },
        {
            "location": "/user-scenarios/#andrea-the-author-and-reader",
            "text": "Andrea turned 29 this year. She is always up for a joke and a pot of coffee but is also quite impatient. Especially if she has to wait for others or if she hasn\u2019t had any progress for a while. However, currently Andrea does his Ph.D in the field of geosciences. Two years ago she decided to go for a cumulative dissertation, meaning that she publishes scientific papers throughout his graduation and summarizes them at the end. She already published his first paper a few months ago which is good, actually. One of the reviewers was interested in the data and the source-code in order to reproduce the results.  After a few hours of searching (remember that she is not one of the most patient guys), she finally finds some files that include the dataset and also the source-code in R (a statistics program). Just a short try if it still working\u2026Weird, the results are different. Just a short look into the paper ... The configuration is different than the one described in the method section. Well, just few manipulations and \u2026 still not working. \nAlthough she submitted the paper just a few months ago, she can\u2019t remember the configuration that lead to the results in the paper. Fortunately, submitting data and code was not mandatory. But Andrea knows that she made a mistake.  More and more journals expect their authors to submit data and source-code that underlie the research findings. For this reason, she wants to change his working behavior and to keep his files under control. She remembers his last research work which was quite unstructured, maybe already messy. His code and data was distributed over several folders and even computers. She had so search for them for quite a while. Moreover, some components do not work anymore. This time, she wants to do it better and searches for a great tool that assists his working flow. He just heard about a new platform supporting reproducible research. The platform allows to upload all necessary files and to create a, so called, container which is executable. It even verifies the results in the paper making it possible to detect errors immediately. Of course it also contains common features like sharing the publication with other authors. On top of that, Andrea can also benefit from other publications. As the platform automatically generates a number of meta information, new search capabilities arise. It is not only possible to search for other publications by using keywords, but also by using spatial and temporal properties and constraints. It is even possible to constrain the search to hypotheses and research questions that contain a certain vocabulary thus simplifying search for related work. Andrea is quite impressed about that. She easily finds related papers that suit to his own work. She gets a good overview about existing research questions making it easier to identify a research gap he can focus on. Andrea doesn\u2019t even have to implement all the code lines for his statistical analysis from scratch, but can build upon existing. While reading some of the related papers in the browser, he realizes a couple of user interface widgets besides the incorporated figures. He doesn\u2019t know them from traditional, static papers which are typically published as .pdf-files. Andrea recognizes that the widgets allow to interact with the diagrams to which the widgets belong. They allow to change, for example, thresholds, input variables and constants. She is thus able to check the assumptions and conclusions underlying the paper. She is a bit overwhelmed by the number of new features that might assist his current research such as exchanging the dataset or the source-code underlying the paper. Andrea is quite happy about his new tool. It provides support for structured work, finding related publications, algorithms and datasets, identifying a research gap, and even tools for interacting with traditional, static papers. So, let\u2019s go for the second paper.",
            "title": "Andrea the author and reader"
        },
        {
            "location": "/user-scenarios/#arthur-the-administrator",
            "text": "Arthur works as a system administrator in a large university library in Germany. He's quite happy with his job. After working as freelance software developer for over 20 years, he now enjoys the challenge to make all the different servers and applications under his care work like a charm 24/7 while having a stable paycheck and reasonable working hours. He is particularly proud that, since he took over the job, he successfully migrated all services to a private cloud infrastructure and enabled https-only traffic on all, event the internal, APIs and websites. Since then, there has been minimal overtime for him and close to 0 minutes downtime for the services... and a raise!  Arthur is interested in this new reproducibility platform that the head of the library is interested in, but he is sceptical about all new systems. There are going to be bugs, unforseen problems, and a lot of testing \"\"in production\"\", which he does not like. But he does now that scientists have been in touch with the library before about archiving data and software, so if this is a high priority for his customers, as he sees them, there is no ways around it.  At second look though, he realizes the project seems to have all the basics straight for a stable and scalable deployment: All components are published under open source licenses, and the project maintainers provide different ready-to-use Docker images. Arthur worries about security, so there is no better way to make sure things work well than source code access. The project is written in a language he has not used before himself, but he can actually build the project himself from source with the provided instructions. He also sees that, instead of reinventing the wheel, the developers seem to be competent enough to build upon established libraries.  The Dockerfiles are great to play around with, but also easy to integrate in his own server management solution. He also likes that all the HTTP APIs as well as setup and configuration seem to be very well documented. This should make it easy to integrate the new solution with some custom tools he developed, but also with some legacy infrastructure he has not yet been able to get rid off. He does worry a bit about the scheduling solution, since he is not very keen on Docker containers beeing started automatically on his servers. Good thing that the project contributors seem to operate a public chat, and professional support is also available at reasonable prices.  After some testing, he feels good to tell his colleagues: looks good to me, let's try this out!",
            "title": "Arthur the administrator"
        },
        {
            "location": "/user-scenarios/#olivia-the-operator",
            "text": "\"Olivia is the chancellor of a mid-size state owned university in the US. She is proud to have been elected to this position a few years ago, and works very hard each day to improve both the university's reputation and the working and learning conditions of her employees and students. She had to make some unpleasent first hand experiences with aspects of todays academic life that sadly became almost normal, such as budget cuts, violence on campus, and plagarism scandals. During all of these upsets, she is happy that the never waivered on the importance of personal integrity and credibility of each and every one of the scientists and researchers working on her campus.  To gain some ground in the competition with other universities, Olivia puts her best assisstant on the job of finding the newest trends in academia. Soon enough she presents to her the idea of making all research conducted at the university reproducible. Olivia is first surprised by the fact, being an arts major herself, as she thought that is already the case. She starts reading the material provided to hear and realizes that science, and especially somehting called computational science, is very much different from the practical work she has encountered during her years as a researcher. It also becomes clear to her that it won't work to just put out a statement forcing every lab to spend enormous efforts on changing established research practices, or to re-do what has been done 5, 10 or 20 years ago. The huge variety of labs and workflows that exists and all the different kinds of people... getting out the stick simply won't work. But maybe the carrot will?  She discovers a novel platform that promises to solve all the problems of reproducibility. The people behind it seem competent enough to her, but again she asks her assisstant to consult with experts from the university library and computer science departsments to see what they think. A lengthy discussion starts, and there seems to be no concensus after months of meetings and evaluations. The assisstant doesn't know what to report back to Olivia. Eventually, Olivia is tired of waiting and joins a few of the meetings of the expert group as an observer. She realizes that nothing comes for free... she encourages the expert group to create a list of requirements on establishing a reproducibility platform for the university. She quickly gets that one, as the lecturers and staff in the group realize that this means they won't just get more work to do, they might get the proper time and money to do it!  Olivia makes the platform a matter for the boss. She sucessfully acquires the funds to start and maintain both the technical platform and to hire support staff to maintain it. Beyond that, the supports staff is even equipped to provided consultancy services to all researchers at the university. These services quickly become popular across all disciplines working with data and code, and after just a few months, more and more fully reproducible papers appear on the public reproducibility platform. Olivia is very glad to see that the changes she introduced did not have an impact on the scientific output of the university - the monthly statistics tell her that much. Is the quality or quantity of the output going to increase? It's too soon to tell, but Olivia is sure it will. Just last week, the head of the programme reported to her that now ten papers are available on the platform for which researchers from different university departments collaborated, who never collaborated before - they discovered the overlap through the publication platform! More than 20 undergraduate courses teaching scientific methods incorporated the platform into their course schedule, and 50% of the graduate theses from the computer science department are now using the university reproducibility tools. Those are good enough signs for Olivia. She decides to pitch an idea to the university board: let's include reproducibility of publications as an evaluation factor for the budget allocations next year. You got to use the stick from time to time to make people aprecciate the carrot.",
            "title": "Olivia the operator"
        },
        {
            "location": "/user-scenarios/#carl-the-curator",
            "text": "Carl works as a digital assets curator at a university library in Germany. He has been working as a librarian for about ten years and experienced the digital transformation of the field, which is why he specialized in the area of digital curation and archiving. He is qualified to manage and organize several collections of digital objects at a given time and recently selected objects for an exhibition of gold standard open access publications in the software category of his institutions catalog frontpage.\nCarl\u2019s expertise encompasses the management of accessibility levels as well as the preservation of file integrity and meta data curation. Since he discovered a growing interest in the preservation of software, he realized that reproducibility of research findings, including code and data increases the value and visibility of his university\u2019s portfolio. As a result, Carl is working closely with the library\u2019s team for Research Data Management, in order to facilitate integration of reproducible computational environments into the digital objects\u2019 life cycle that underlies their current policies.\nAs he strongly believes that publicly funded research data are public goods, Carl values his profession as a vital point of intersection between researchers, librarians und the general public. Therefore, when planning a selection of digital assets or curating the library\u2019s catalogs, Carl enjoys the interoperability that is provided by international metadata standards and linked open data vocabularies.",
            "title": "Carl the curator"
        },
        {
            "location": "/user-scenarios/#polly-the-publisher",
            "text": "Polly is the head of a large publishing firm for scientific journals. She grew up being part of a publisher family, the third of four kids. While her older brothers wrestled with the family legacy, she has always been close to her late grandfather, who started the publishing business as a young man. So it came as no surprise that she studied arts and library science and after a few well planned career steps around the globe, she joined the family business as assitant of her father and became CEO after a few years, a decision she rarely regrets.  Though there is one thing that makes her job challenging every day: technology's high development speed. For a large publishing business, it is hard to keep up with new and modern technology. She has to serve both old (in more than one way) customers and employers, who have had a long relationship and a work environment and processes that have developed and settled in over many years. On the other hand, she sees new ideas by entrepeneurs and startups almost every week, some crazy and some rightfully called revolutional, who experiment with new ways to publish science without the baggage of a reputation and hundrets of journals and an order of magnitude more employees.  So what should Polly do? Scrample up some money to acquire a few startups and replace the existing review and authoring platform? Fire all staff members who are too slow adopting the new technologies? Close journals with an excellent reputation because editors and reviewers are not tech-savvy?   Obviously, none of these were an option. Change had to come gradually and incluse, not in a disruptive fashion. Polly turned to her CTO Charlotte. She joined the company recently and played out to be a very good hire, as she was able to revive the in-house development team with a positive attitude and a few key hires. Charlotte is aware of the challenges and agress to compile an action plan from her perspective. A few weeks later, she presents the options to Polly and the other board members. She suggests to adopt and open platform for interactive publications, which is an integrated solution for hosting and archiving data as well as code that is often part of publications these days. It is open source, but of course that does not come for free. Charlotte suggests a combined approach of experiments by her own staff and external consulting by the original developers of the platform. And she quickly mitigates all concerns raised by the other CxOs: the platform is customizable, so it will not look like the competitors versions, it is extensible, so the few \"\"cool features\"\" that have been developed over the last years will be easy to integrate, and it is compatible with the existing data hosting platform (so no need to replace that beast of a software). This new platform would be an option presented to all editors to adopt for their journals. Education of the company's staff would precede this offer to make sure the inteneded message is spread: don't be left behind, challenge reviewers and authors to improve the quality of the journals and subsequently raise the bar for high quality open science.",
            "title": "Polly the publisher"
        },
        {
            "location": "/user-scenarios/#richard-the-reviewer",
            "text": "TBD",
            "title": "Richard the reviewer"
        },
        {
            "location": "/user-scenarios/#rachel-the-reader",
            "text": "TBD",
            "title": "Rachel the reader"
        },
        {
            "location": "/preservation-workflows/",
            "text": "Preservation workflows\n\n\ndescribe infrastructure for adoption by libraries, publishers, and third-party organizations\n\n\ndevelop workflows that can be implemented by libraries - how to use the architecture \"from the library perspective\"",
            "title": "Preservation workflows"
        },
        {
            "location": "/preservation-workflows/#preservation-workflows",
            "text": "describe infrastructure for adoption by libraries, publishers, and third-party organizations  develop workflows that can be implemented by libraries - how to use the architecture \"from the library perspective\"",
            "title": "Preservation workflows"
        },
        {
            "location": "/glossary/",
            "text": "Glossary\n\n\nERC\n\n\nExecutable Research Compendium, see this \nscientific article\n for concepts and the \nspecification\n for technical documentation.\n\n\nExecutable Research Compendium\n\n\nSee \nERC",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#glossary",
            "text": "",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#erc",
            "text": "Executable Research Compendium, see this  scientific article  for concepts and the  specification  for technical documentation.",
            "title": "ERC"
        },
        {
            "location": "/glossary/#executable-research-compendium",
            "text": "See  ERC",
            "title": "Executable Research Compendium"
        }
    ]
}